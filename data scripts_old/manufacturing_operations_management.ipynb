{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb3134e",
   "metadata": {},
   "source": [
    "Level 3: Manufacturing Operations Management Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef034f",
   "metadata": {},
   "source": [
    "Work Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc1c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Products data file data/products.csv not found.\n",
      "Work orders will be generated with synthetic product IDs.\n",
      "Note: Customer orders data file data/customer_orders.csv not found.\n",
      "Work orders will be generated with synthetic customer order IDs.\n",
      "Note: Production schedules data file data/production_schedules.csv not found.\n",
      "Work orders will be generated with synthetic production schedule IDs.\n",
      "Generating synthetic product IDs...\n",
      "Generating synthetic customer order IDs...\n",
      "Generating synthetic production schedule IDs...\n",
      "Successfully generated 200 work order records.\n",
      "Data saved to data/work_orders.csv\n",
      "\n",
      "Work Orders Statistics:\n",
      "Total work orders: 200\n",
      "\n",
      "Work Order Type Distribution:\n",
      "  Production: 116 (58.0%)\n",
      "  Maintenance: 43 (21.5%)\n",
      "  Engineering: 12 (6.0%)\n",
      "  Rework: 10 (5.0%)\n",
      "  Quality Check: 10 (5.0%)\n",
      "  Calibration: 5 (2.5%)\n",
      "  Cleaning: 4 (2.0%)\n",
      "\n",
      "Status Distribution:\n",
      "  Completed: 101 (50.5%)\n",
      "  Planned: 59 (29.5%)\n",
      "  Cancelled: 22 (11.0%)\n",
      "  In Progress: 9 (4.5%)\n",
      "  Released: 7 (3.5%)\n",
      "  On Hold: 2 (1.0%)\n",
      "\n",
      "Priority Distribution:\n",
      "  Priority 1: 15 (7.5%)\n",
      "  Priority 2: 40 (20.0%)\n",
      "  Priority 3: 73 (36.5%)\n",
      "  Priority 4: 52 (26.0%)\n",
      "  Priority 5: 20 (10.0%)\n",
      "\n",
      "Work orders linked to customer orders: 85 (42.5%)\n",
      "Work orders linked to production schedules: 102 (51.0%)\n",
      "\n",
      "Production work orders completion rate: 49.1%\n",
      "Average yield rate: 96.7%\n",
      "\n",
      "Date Statistics:\n",
      "  Average planned duration: 8.8 days\n",
      "  Average actual duration: 8.6 days\n",
      "  On-time completion rate: 57.4%\n",
      "\n",
      "Time Distribution of Work Orders:\n",
      "  July 2025: 43 work orders\n",
      "  June 2025: 50 work orders\n",
      "  May 2025: 38 work orders\n",
      "  July 2025: 43 work orders\n",
      "  August 2025: 33 work orders\n",
      "  September 2025: 0 work orders\n",
      "\n",
      "Sample work orders data (first 5 records):\n",
      "  work_order_id work_order_type     product_id  planned_quantity  \\\n",
      "0   WO-9EB43C78      Production  PROD-98FE3228            4070.3   \n",
      "1   WO-CEE98A3C          Rework  PROD-57421F7F              53.5   \n",
      "2   WO-F4DD5746      Production  PROD-F136676C             579.6   \n",
      "3   WO-1ADDB741      Production  PROD-35FD08EF            5281.4   \n",
      "4   WO-A21AC1CE      Production  PROD-CC655F5A            5116.6   \n",
      "\n",
      "   actual_quantity quantity_unit planned_start_date actual_start_date  \\\n",
      "0           4168.3            m²         2025-04-16        2025-04-18   \n",
      "1             55.5             m         2025-04-16        2025-04-15   \n",
      "2            522.8            m³         2025-04-17        2025-04-17   \n",
      "3              0.0            m³         2025-04-18               NaT   \n",
      "4           4796.0         units         2025-04-20        2025-04-21   \n",
      "\n",
      "  planned_end_date actual_end_date     status  priority customer_order_id  \\\n",
      "0       2025-04-22      2025-04-21  Completed         4       CO-86ABB755   \n",
      "1       2025-04-26      2025-04-24  Completed         2                     \n",
      "2       2025-04-28      2025-05-08  Completed         2       CO-EFBCFBC2   \n",
      "3       2025-04-23             NaT  Cancelled         3       CO-010FA73F   \n",
      "4       2025-04-26      2025-04-24  Completed         4       CO-71E39924   \n",
      "\n",
      "  production_schedule_id   facility_id  planned_duration  \n",
      "0            PS-82E4640C  FAC-2B0BE767                 6  \n",
      "1                         FAC-E91E49C1                10  \n",
      "2            PS-82E4640C  FAC-80A0687B                11  \n",
      "3            PS-8D777211  FAC-2B0BE767                 5  \n",
      "4            PS-7904741F  FAC-80A0687B                 6  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114075/2864994044.py:427: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  completed['planned_quantity'] = pd.to_numeric(completed['planned_quantity'], errors='coerce')\n",
      "/tmp/ipykernel_114075/2864994044.py:428: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  completed['actual_quantity'] = pd.to_numeric(completed['actual_quantity'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Please run the equipment data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_products_data(products_file=\"data/products.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated products data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - products_file: CSV file containing products data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the products data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(products_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Products data file {products_file} not found.\")\n",
    "        print(\"Work orders will be generated with synthetic product IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_facilities_data(facilities_file=\"data/facilities.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated facilities data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - facilities_file: CSV file containing facilities data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the facilities data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(facilities_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Facilities data file {facilities_file} not found.\")\n",
    "        print(\"Work orders will not have facility assignments.\")\n",
    "        return None\n",
    "\n",
    "def load_customer_orders_data(customer_orders_file=\"data/customer_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated customer orders data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - customer_orders_file: CSV file containing customer orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the customer orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(customer_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Customer orders data file {customer_orders_file} not found.\")\n",
    "        print(\"Work orders will be generated with synthetic customer order IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_production_schedules_data(production_schedules_file=\"data/production_schedules.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated production schedules data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - production_schedules_file: CSV file containing production schedules data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the production schedules data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(production_schedules_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Production schedules data file {production_schedules_file} not found.\")\n",
    "        print(\"Work orders will be generated with synthetic production schedule IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_work_orders(equipment_df, products_df=None, facilities_df=None, \n",
    "                        customer_orders_df=None, production_schedules_df=None, \n",
    "                        num_work_orders=200, start_time=None, end_time=None,\n",
    "                        output_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the WorkOrders table from ISA-95 Level 3.\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_df: DataFrame containing equipment data\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - facilities_df: DataFrame containing facilities data (optional)\n",
    "    - customer_orders_df: DataFrame containing customer orders data (optional)\n",
    "    - production_schedules_df: DataFrame containing production schedules data (optional)\n",
    "    - num_work_orders: Number of work order records to generate\n",
    "    - start_time: Start time for work order date range (defaults to 90 days ago)\n",
    "    - end_time: End time for work order date range (defaults to 30 days in the future)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated work orders data\n",
    "    \"\"\"\n",
    "    if equipment_df is None or len(equipment_df) == 0:\n",
    "        print(\"Error: No equipment data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=90)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now() + timedelta(days=30)\n",
    "    \n",
    "    # Generate product IDs if not provided\n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"Generating synthetic product IDs...\")\n",
    "        product_ids = [f\"PROD-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    else:\n",
    "        product_ids = products_df['product_id'].unique().tolist()\n",
    "    \n",
    "    # Generate facility IDs if not provided\n",
    "    if facilities_df is None or len(facilities_df) == 0:\n",
    "        print(\"Generating synthetic facility IDs...\")\n",
    "        facility_ids = [f\"FAC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(5)]\n",
    "    else:\n",
    "        facility_ids = facilities_df['facility_id'].unique().tolist()\n",
    "    \n",
    "    # Generate customer order IDs if not provided\n",
    "    if customer_orders_df is None or len(customer_orders_df) == 0:\n",
    "        print(\"Generating synthetic customer order IDs...\")\n",
    "        customer_order_ids = [f\"CO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    else:\n",
    "        customer_order_ids = customer_orders_df['order_id'].unique().tolist()\n",
    "    \n",
    "    # Generate production schedule IDs if not provided\n",
    "    if production_schedules_df is None or len(production_schedules_df) == 0:\n",
    "        print(\"Generating synthetic production schedule IDs...\")\n",
    "        production_schedule_ids = [f\"PS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(10)]\n",
    "    else:\n",
    "        production_schedule_ids = production_schedules_df['schedule_id'].unique().tolist()\n",
    "    \n",
    "    # Define work order types and their probabilities\n",
    "    work_order_types = {\n",
    "        \"Production\": 0.6,        # Most common\n",
    "        \"Maintenance\": 0.2,\n",
    "        \"Quality Check\": 0.05,\n",
    "        \"Rework\": 0.05,\n",
    "        \"Engineering\": 0.05,\n",
    "        \"Cleaning\": 0.03,\n",
    "        \"Calibration\": 0.02\n",
    "    }\n",
    "    \n",
    "    # Define work order statuses and their transition probabilities\n",
    "    # Format: {status: {next_status: probability}}\n",
    "    status_transitions = {\n",
    "        \"Planned\": {\"Planned\": 0.2, \"Released\": 0.7, \"Cancelled\": 0.1},\n",
    "        \"Released\": {\"Released\": 0.2, \"In Progress\": 0.75, \"Cancelled\": 0.05},\n",
    "        \"In Progress\": {\"In Progress\": 0.3, \"Completed\": 0.6, \"On Hold\": 0.1},\n",
    "        \"On Hold\": {\"On Hold\": 0.3, \"In Progress\": 0.6, \"Cancelled\": 0.1},\n",
    "        \"Completed\": {\"Completed\": 0.95, \"Rework\": 0.05},\n",
    "        \"Cancelled\": {\"Cancelled\": 1.0},  # Terminal state\n",
    "        \"Rework\": {\"Rework\": 0.3, \"In Progress\": 0.7}\n",
    "    }\n",
    "    \n",
    "    # Define priority levels\n",
    "    priority_levels = [1, 2, 3, 4, 5]  # 1 = highest, 5 = lowest\n",
    "    priority_weights = [0.1, 0.2, 0.4, 0.2, 0.1]  # Most orders are medium priority\n",
    "    \n",
    "    # Define possible units of measurement\n",
    "    quantity_units = [\"kg\", \"L\", \"units\", \"pallets\", \"boxes\", \"tons\", \"m\", \"m²\", \"m³\", \"batches\"]\n",
    "    \n",
    "    # Generate work order data\n",
    "    data = {\n",
    "        \"work_order_id\": [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_work_orders)],\n",
    "        \"work_order_type\": [],\n",
    "        \"product_id\": [],\n",
    "        \"planned_quantity\": [],\n",
    "        \"actual_quantity\": [],\n",
    "        \"quantity_unit\": [],\n",
    "        \"planned_start_date\": [],\n",
    "        \"actual_start_date\": [],\n",
    "        \"planned_end_date\": [],\n",
    "        \"actual_end_date\": [],\n",
    "        \"status\": [],\n",
    "        \"priority\": [],\n",
    "        \"customer_order_id\": [],\n",
    "        \"production_schedule_id\": [],\n",
    "        \"facility_id\": []\n",
    "    }\n",
    "    \n",
    "    # Generate a time distribution for work orders (weighted toward recent and near future)\n",
    "    time_points = []\n",
    "    time_range_days = (end_time - start_time).days\n",
    "    \n",
    "    for _ in range(num_work_orders):\n",
    "        # Use a beta distribution to weight toward recent and near future\n",
    "        # Beta(2, 1) will weight toward the future, Beta(1, 2) toward the past\n",
    "        if random.random() < 0.6:  # 60% of orders are more recent/future\n",
    "            beta = random.betavariate(2, 1)\n",
    "        else:\n",
    "            beta = random.betavariate(1, 2)\n",
    "        \n",
    "        days_offset = int(beta * time_range_days)\n",
    "        time_point = start_time + timedelta(days=days_offset)\n",
    "        time_points.append(time_point)\n",
    "    \n",
    "    # Sort time points to establish a chronological sequence\n",
    "    time_points.sort()\n",
    "    \n",
    "    # Generate data for each work order\n",
    "    for i in range(num_work_orders):\n",
    "        # Select work order type (weighted random)\n",
    "        work_order_type = random.choices(\n",
    "            list(work_order_types.keys()), \n",
    "            weights=list(work_order_types.values())\n",
    "        )[0]\n",
    "        data[\"work_order_type\"].append(work_order_type)\n",
    "        \n",
    "        # Assign product ID (if applicable)\n",
    "        if work_order_type in [\"Production\", \"Rework\", \"Quality Check\"]:\n",
    "            data[\"product_id\"].append(random.choice(product_ids))\n",
    "        else:\n",
    "            data[\"product_id\"].append(\"\")  # No product for maintenance, cleaning, etc.\n",
    "        \n",
    "        # Generate quantity (if applicable)\n",
    "        if work_order_type in [\"Production\", \"Rework\"]:\n",
    "            # Production quantities vary by product type, but we'll use a general range\n",
    "            planned_quantity = random.choice([10, 50, 100, 500, 1000, 5000]) * random.uniform(0.8, 1.2)\n",
    "            planned_quantity = round(planned_quantity, 1)\n",
    "            data[\"planned_quantity\"].append(planned_quantity)\n",
    "            \n",
    "            # Set unit appropriate for the quantity\n",
    "            quantity_unit = random.choice(quantity_units)\n",
    "            data[\"quantity_unit\"].append(quantity_unit)\n",
    "            \n",
    "            # Generate actual quantity based on status (to be filled in later)\n",
    "            data[\"actual_quantity\"].append(0)  # Placeholder\n",
    "        else:\n",
    "            data[\"planned_quantity\"].append(0)\n",
    "            data[\"quantity_unit\"].append(\"\")\n",
    "            data[\"actual_quantity\"].append(0)\n",
    "        \n",
    "        # Set base planned start date from the chronological sequence\n",
    "        base_date = time_points[i]\n",
    "        \n",
    "        # For work orders in the past, determine status through a Markov chain simulation\n",
    "        current_status = \"Planned\"  # All work orders start as planned\n",
    "        current_date = base_date\n",
    "        \n",
    "        # Simulate status transitions based on time progression\n",
    "        while current_date < datetime.now():\n",
    "            # Get possible next statuses and their probabilities\n",
    "            next_status_probs = status_transitions[current_status]\n",
    "            next_statuses = list(next_status_probs.keys())\n",
    "            next_probs = list(next_status_probs.values())\n",
    "            \n",
    "            # Select next status\n",
    "            next_status = random.choices(next_statuses, weights=next_probs)[0]\n",
    "            \n",
    "            # If status changed, advance the date\n",
    "            if next_status != current_status:\n",
    "                # Time in a status depends on the status itself\n",
    "                if current_status == \"Planned\":\n",
    "                    days_in_status = random.randint(1, 5)  # 1-5 days in planning\n",
    "                elif current_status == \"Released\":\n",
    "                    days_in_status = random.randint(1, 3)  # 1-3 days released before starting\n",
    "                elif current_status == \"In Progress\":\n",
    "                    days_in_status = random.randint(3, 15)  # 3-15 days in production\n",
    "                elif current_status == \"On Hold\":\n",
    "                    days_in_status = random.randint(2, 10)  # 2-10 days on hold\n",
    "                else:\n",
    "                    days_in_status = random.randint(1, 5)  # Default\n",
    "                \n",
    "                current_date += timedelta(days=days_in_status)\n",
    "            else:\n",
    "                # If status didn't change, just advance a small random amount\n",
    "                current_date += timedelta(days=random.randint(1, 3))\n",
    "            \n",
    "            current_status = next_status\n",
    "            \n",
    "            # Stop if we've reached a terminal status\n",
    "            if current_status in [\"Completed\", \"Cancelled\"]:\n",
    "                break\n",
    "        \n",
    "        # Set the final status\n",
    "        data[\"status\"].append(current_status)\n",
    "        \n",
    "        # Set duration based on work order type\n",
    "        if work_order_type == \"Production\":\n",
    "            # Production orders typically take longer\n",
    "            duration_days = random.randint(5, 20)\n",
    "        elif work_order_type == \"Maintenance\":\n",
    "            duration_days = random.randint(1, 5)\n",
    "        elif work_order_type == \"Cleaning\":\n",
    "            duration_days = random.randint(1, 2)\n",
    "        elif work_order_type == \"Calibration\":\n",
    "            duration_days = random.randint(1, 3)\n",
    "        else:\n",
    "            duration_days = random.randint(2, 10)\n",
    "        \n",
    "        # Set planned dates\n",
    "        planned_start_date = base_date\n",
    "        planned_end_date = planned_start_date + timedelta(days=duration_days)\n",
    "        \n",
    "        data[\"planned_start_date\"].append(planned_start_date.strftime(\"%Y-%m-%d\"))\n",
    "        data[\"planned_end_date\"].append(planned_end_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Set actual dates based on status\n",
    "        if current_status in [\"In Progress\", \"On Hold\", \"Completed\", \"Rework\"]:\n",
    "            # Started but may not be finished\n",
    "            # Add some variation to actual start date\n",
    "            start_variation = random.randint(-2, 2)  # +/- 2 days from planned\n",
    "            actual_start_date = planned_start_date + timedelta(days=start_variation)\n",
    "            data[\"actual_start_date\"].append(actual_start_date.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            if current_status == \"Completed\":\n",
    "                # Finished - may be early, on time, or late\n",
    "                completion_variation = random.choices(\n",
    "                    [-3, -2, -1, 0, 1, 2, 3, 5, 10],  # Days early(-) or late(+)\n",
    "                    weights=[0.05, 0.1, 0.15, 0.3, 0.15, 0.1, 0.05, 0.05, 0.05]\n",
    "                )[0]\n",
    "                actual_end_date = planned_end_date + timedelta(days=completion_variation)\n",
    "                data[\"actual_end_date\"].append(actual_end_date.strftime(\"%Y-%m-%d\"))\n",
    "                \n",
    "                # Set actual quantity for completed orders\n",
    "                if data[\"planned_quantity\"][i] > 0:\n",
    "                    # Actual quantity is usually close to planned, but may vary\n",
    "                    quantity_variation = random.uniform(0.9, 1.05)  # -10% to +5%\n",
    "                    actual_quantity = data[\"planned_quantity\"][i] * quantity_variation\n",
    "                    data[\"actual_quantity\"][i] = round(actual_quantity, 1)\n",
    "            else:\n",
    "                # Not finished yet\n",
    "                data[\"actual_end_date\"].append(\"\")\n",
    "        else:\n",
    "            # Not started yet\n",
    "            data[\"actual_start_date\"].append(\"\")\n",
    "            data[\"actual_end_date\"].append(\"\")\n",
    "        \n",
    "        # Set priority (weighted random)\n",
    "        data[\"priority\"].append(random.choices(priority_levels, weights=priority_weights)[0])\n",
    "        \n",
    "        # Connect to customer order (production orders are more likely to be connected)\n",
    "        if work_order_type == \"Production\" and random.random() < 0.8:\n",
    "            data[\"customer_order_id\"].append(random.choice(customer_order_ids))\n",
    "        else:\n",
    "            data[\"customer_order_id\"].append(\"\")\n",
    "        \n",
    "        # Connect to production schedule (production orders are more likely to be scheduled)\n",
    "        if work_order_type == \"Production\" and random.random() < 0.9:\n",
    "            data[\"production_schedule_id\"].append(random.choice(production_schedule_ids))\n",
    "        else:\n",
    "            data[\"production_schedule_id\"].append(\"\")\n",
    "        \n",
    "        # Assign to facility\n",
    "        data[\"facility_id\"].append(random.choice(facility_ids))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} work order records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(work_orders_df):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated work orders data\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_df: DataFrame containing work orders data\n",
    "    \"\"\"\n",
    "    if work_orders_df is None or len(work_orders_df) == 0:\n",
    "        print(\"No work orders data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nWork Orders Statistics:\")\n",
    "    print(f\"Total work orders: {len(work_orders_df)}\")\n",
    "    \n",
    "    print(\"\\nWork Order Type Distribution:\")\n",
    "    type_counts = work_orders_df['work_order_type'].value_counts()\n",
    "    for wo_type, count in type_counts.items():\n",
    "        print(f\"  {wo_type}: {count} ({count/len(work_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nStatus Distribution:\")\n",
    "    status_counts = work_orders_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(work_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nPriority Distribution:\")\n",
    "    priority_counts = work_orders_df['priority'].value_counts().sort_index()\n",
    "    for priority, count in priority_counts.items():\n",
    "        print(f\"  Priority {priority}: {count} ({count/len(work_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Customer order connections\n",
    "    co_count = work_orders_df['customer_order_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nWork orders linked to customer orders: {co_count} ({co_count/len(work_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Production schedule connections\n",
    "    ps_count = work_orders_df['production_schedule_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"Work orders linked to production schedules: {ps_count} ({ps_count/len(work_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Production work orders completion rate\n",
    "    prod_orders = work_orders_df[work_orders_df['work_order_type'] == 'Production']\n",
    "    if len(prod_orders) > 0:\n",
    "        completed = prod_orders[prod_orders['status'] == 'Completed']\n",
    "        print(f\"\\nProduction work orders completion rate: {len(completed)/len(prod_orders)*100:.1f}%\")\n",
    "        \n",
    "        # Quantity statistics for production orders\n",
    "        if len(completed) > 0:\n",
    "            # Convert to numeric to ensure proper calculations\n",
    "            completed['planned_quantity'] = pd.to_numeric(completed['planned_quantity'], errors='coerce')\n",
    "            completed['actual_quantity'] = pd.to_numeric(completed['actual_quantity'], errors='coerce')\n",
    "            \n",
    "            # Filter out any NaN values\n",
    "            valid_quantities = completed.dropna(subset=['planned_quantity', 'actual_quantity'])\n",
    "            \n",
    "            if len(valid_quantities) > 0:\n",
    "                # Calculate yield rate (actual vs. planned)\n",
    "                valid_quantities['yield_rate'] = valid_quantities['actual_quantity'] / valid_quantities['planned_quantity']\n",
    "                avg_yield = valid_quantities['yield_rate'].mean() * 100\n",
    "                print(f\"Average yield rate: {avg_yield:.1f}%\")\n",
    "    \n",
    "    # Date statistics\n",
    "    print(\"\\nDate Statistics:\")\n",
    "    \n",
    "    # Convert date strings to datetime objects for comparison\n",
    "    work_orders_df['planned_start_date'] = pd.to_datetime(work_orders_df['planned_start_date'])\n",
    "    work_orders_df['planned_end_date'] = pd.to_datetime(work_orders_df['planned_end_date'])\n",
    "    \n",
    "    # Filter out empty strings before conversion\n",
    "    work_orders_df['actual_start_date'] = pd.to_datetime(\n",
    "        work_orders_df['actual_start_date'].replace('', pd.NaT), errors='coerce'\n",
    "    )\n",
    "    work_orders_df['actual_end_date'] = pd.to_datetime(\n",
    "        work_orders_df['actual_end_date'].replace('', pd.NaT), errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Calculate planned duration\n",
    "    work_orders_df['planned_duration'] = (work_orders_df['planned_end_date'] - work_orders_df['planned_start_date']).dt.days\n",
    "    \n",
    "    print(f\"  Average planned duration: {work_orders_df['planned_duration'].mean():.1f} days\")\n",
    "    \n",
    "    # Calculate actual duration for completed work orders\n",
    "    completed = work_orders_df[work_orders_df['status'] == 'Completed'].dropna(subset=['actual_start_date', 'actual_end_date'])\n",
    "    if len(completed) > 0:\n",
    "        completed['actual_duration'] = (completed['actual_end_date'] - completed['actual_start_date']).dt.days\n",
    "        print(f\"  Average actual duration: {completed['actual_duration'].mean():.1f} days\")\n",
    "        \n",
    "        # Calculate on-time delivery\n",
    "        completed['on_time'] = completed['actual_end_date'] <= completed['planned_end_date']\n",
    "        on_time_pct = completed['on_time'].mean() * 100\n",
    "        print(f\"  On-time completion rate: {on_time_pct:.1f}%\")\n",
    "    \n",
    "    # Distribution of work orders over time\n",
    "    current_month = pd.Timestamp.now().replace(day=1)\n",
    "    past_months = [current_month - pd.DateOffset(months=i) for i in range(3)]\n",
    "    future_months = [current_month + pd.DateOffset(months=i) for i in range(1, 3)]\n",
    "    \n",
    "    print(\"\\nTime Distribution of Work Orders:\")\n",
    "    for month in past_months + [current_month] + future_months:\n",
    "        month_start = month\n",
    "        month_end = month + pd.DateOffset(months=1) - pd.DateOffset(days=1)\n",
    "        month_orders = work_orders_df[(work_orders_df['planned_start_date'] >= month_start) & \n",
    "                                     (work_orders_df['planned_start_date'] <= month_end)]\n",
    "        month_name = month.strftime(\"%B %Y\")\n",
    "        print(f\"  {month_name}: {len(month_orders)} work orders\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    equipment_df = load_equipment_data()\n",
    "    products_df = load_products_data()\n",
    "    facilities_df = load_facilities_data()\n",
    "    customer_orders_df = load_customer_orders_data()\n",
    "    production_schedules_df = load_production_schedules_data()\n",
    "    \n",
    "    if equipment_df is not None:\n",
    "        # Generate work orders data\n",
    "        work_orders_df = generate_work_orders(\n",
    "            equipment_df,\n",
    "            products_df,\n",
    "            facilities_df,\n",
    "            customer_orders_df,\n",
    "            production_schedules_df,\n",
    "            num_work_orders=200,  # Generate 200 work order records\n",
    "            output_file=\"data/work_orders.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if work_orders_df is not None:\n",
    "            display_statistics(work_orders_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample work orders data (first 5 records):\")\n",
    "            print(work_orders_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e76c53",
   "metadata": {},
   "source": [
    "Material Lots Material Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b977eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Materials data file data/materials.csv not found.\n",
      "Material lots will be generated with synthetic material IDs.\n",
      "Note: Suppliers data file data/suppliers.csv not found.\n",
      "Material lots will be generated with synthetic supplier IDs.\n",
      "Note: Storage locations data file data/storage_locations.csv not found.\n",
      "Material lots will be generated with synthetic storage location IDs.\n",
      "Generating synthetic material IDs...\n",
      "Generating synthetic supplier IDs...\n",
      "Generating synthetic storage location IDs...\n",
      "Successfully generated 300 material lot records.\n",
      "Data saved to data/material_lots.csv\n",
      "Successfully generated 500 material transaction records.\n",
      "Data saved to data/material_transactions.csv\n",
      "\n",
      "Material Lots Statistics:\n",
      "Total material lots: 300\n",
      "\n",
      "Status Distribution:\n",
      "  Active: 189 (63.0%)\n",
      "  Consumed: 63 (21.0%)\n",
      "  In Process: 28 (9.3%)\n",
      "  Reserved: 20 (6.7%)\n",
      "\n",
      "Quality Status Distribution:\n",
      "  Released: 207 (69.0%)\n",
      "  Under Quarantine: 34 (11.3%)\n",
      "  Pending Test: 28 (9.3%)\n",
      "  Rejected: 15 (5.0%)\n",
      "  Hold: 9 (3.0%)\n",
      "  Expired: 7 (2.3%)\n",
      "\n",
      "Quantity Units Distribution:\n",
      "  kg: 45 (15.0%)\n",
      "  L: 39 (13.0%)\n",
      "  batches: 31 (10.3%)\n",
      "  g: 31 (10.3%)\n",
      "  boxes: 20 (6.7%)\n",
      "  m³: 18 (6.0%)\n",
      "  drums: 16 (5.3%)\n",
      "  pallets: 15 (5.0%)\n",
      "  units: 11 (3.7%)\n",
      "  tons: 10 (3.3%)\n",
      "\n",
      "Lots with supplier association: 165 (55.0%)\n",
      "Lots with storage location: 209 (69.7%)\n",
      "Lots with parent lot: 40 (13.3%)\n",
      "\n",
      "Quantity Statistics:\n",
      "  Total quantity: 436034.7 (in various units)\n",
      "  Average lot size: 1453.4\n",
      "  Min lot size: 0.8\n",
      "  Max lot size: 25000.0\n",
      "\n",
      "Cost Statistics:\n",
      "  Average cost per unit: $275.94\n",
      "  Min cost per unit: $0.53\n",
      "  Max cost per unit: $4917.33\n",
      "\n",
      "Shelf Life Statistics:\n",
      "  Average shelf life: 638.3 days\n",
      "  Min shelf life: 91 days\n",
      "  Max shelf life: 3639 days\n",
      "\n",
      "Remaining Shelf Life (Active Lots):\n",
      "  Average remaining shelf life: 574.1 days\n",
      "  Min remaining shelf life: -51 days\n",
      "  Max remaining shelf life: 3407 days\n",
      "  Expired lots still active: 7 (3.7% of active lots)\n",
      "  Lots expiring within 30 days: 9 (4.8% of active lots)\n",
      "\n",
      "Material Transactions Statistics:\n",
      "Total transactions: 500\n",
      "\n",
      "Transaction Type Distribution:\n",
      "  Issue: 153 (30.6%)\n",
      "  Receipt: 125 (25.0%)\n",
      "  Transfer: 100 (20.0%)\n",
      "  Adjustment: 53 (10.6%)\n",
      "  Scrapping: 25 (5.0%)\n",
      "  Return: 22 (4.4%)\n",
      "  Consumption: 22 (4.4%)\n",
      "\n",
      "Transactions with work order: 152 (30.4%)\n",
      "Transactions with batch: 138 (27.6%)\n",
      "\n",
      "Transaction Quantity Statistics:\n",
      "  Average transaction quantity: 1119.6\n",
      "  Min transaction quantity: -614.5\n",
      "  Max transaction quantity: 25000.0\n",
      "\n",
      "Monthly Transaction Distribution:\n",
      "  2025-01: 43 transactions\n",
      "  2025-02: 81 transactions\n",
      "  2025-03: 90 transactions\n",
      "  2025-04: 91 transactions\n",
      "  2025-05: 80 transactions\n",
      "  2025-06: 84 transactions\n",
      "  2025-07: 31 transactions\n",
      "\n",
      "Sample material lots data (first 5 records):\n",
      "         lot_id   material_id  lot_quantity quantity_unit      status  \\\n",
      "0  LOT-3AED1C75  MAT-75E3D9F8        413.81            mg      Active   \n",
      "1  LOT-D80B7C91  MAT-2278077A        104.36         vials  In Process   \n",
      "2  LOT-83101683  MAT-A1FB8733        511.86       batches      Active   \n",
      "3  LOT-37878407  MAT-C85850F3      10000.00         units      Active   \n",
      "4  LOT-511ABE29  MAT-95E0F1DE        112.16       pallets    Consumed   \n",
      "\n",
      "  creation_date expiration_date   supplier_id supplier_lot_id receipt_date  \\\n",
      "0    2025-01-17      2026-01-23  SUP-5595FD7E          S63333   2025-01-19   \n",
      "1    2025-01-01      2027-05-18                                 2025-01-19   \n",
      "2    2024-12-21      2026-09-03  SUP-D9601BAE          L61986   2025-01-20   \n",
      "3    2025-01-16      2027-07-30  SUP-725ACCD8          L89055   2025-01-24   \n",
      "4    2025-01-16      2026-11-10  SUP-259598F1          S91758   2025-01-30   \n",
      "\n",
      "  storage_location_id    quality_status  cost_per_unit parent_lot_id  \\\n",
      "0        LOC-D141BEE9           Expired        3210.31                 \n",
      "1                              Released          50.69                 \n",
      "2        LOC-0C20D549  Under Quarantine        2959.82                 \n",
      "3        LOC-DFEE1105          Released           6.56                 \n",
      "4                              Released         217.86                 \n",
      "\n",
      "   shelf_life_days  remaining_days  \n",
      "0              371             191  \n",
      "1              867             671  \n",
      "2              621             414  \n",
      "3              925             744  \n",
      "4              663             482  \n",
      "\n",
      "Sample material transactions data (first 5 records):\n",
      "  transaction_id transaction_type        lot_id           timestamp  quantity  \\\n",
      "0  TRAN-514EF1BF       Adjustment  LOT-6901D9E2 2025-01-16 21:41:57      0.57   \n",
      "1  TRAN-A3FC1A1F            Issue  LOT-71732351 2025-01-17 01:04:57     78.37   \n",
      "2  TRAN-1811A611            Issue  LOT-70CB7C39 2025-01-17 07:44:57     17.78   \n",
      "3  TRAN-F4500C14            Issue  LOT-56E81486 2025-01-17 11:14:57   1636.83   \n",
      "4  TRAN-523F7D71          Receipt  LOT-A40315F2 2025-01-17 16:16:57    114.21   \n",
      "\n",
      "  from_location_id to_location_id work_order_id        batch_id operator_id  \\\n",
      "0     LOC-FB0049D4                                                OP-D8F103   \n",
      "1     LOC-833A9EF8                  WO-B2E7700D                   OP-9201E0   \n",
      "2     LOC-D3A31A72                               BATCH-8155F840   OP-813424   \n",
      "3     LOC-FB0049D4                  WO-FF1EA9DF  BATCH-91C504C5   OP-808755   \n",
      "4                    LOC-9650E1D6                                 OP-D8F103   \n",
      "\n",
      "       transaction_reason reference_document    month  \n",
      "0             Cycle Count          ADJ-86213  2025-01  \n",
      "1        Batch Production        WO-B2E7700D  2025-01  \n",
      "2        Batch Production           WO-22556  2025-01  \n",
      "3        Batch Production        WO-FF1EA9DF  2025-01  \n",
      "4  Contract Manufacturing           PO-52571  2025-01  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_materials_data(materials_file=\"data/materials.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated materials data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - materials_file: CSV file containing materials data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the materials data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(materials_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Materials data file {materials_file} not found.\")\n",
    "        print(\"Material lots will be generated with synthetic material IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_suppliers_data(suppliers_file=\"data/suppliers.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated suppliers data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - suppliers_file: CSV file containing suppliers data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the suppliers data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(suppliers_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Suppliers data file {suppliers_file} not found.\")\n",
    "        print(\"Material lots will be generated with synthetic supplier IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_storage_locations_data(storage_locations_file=\"data/storage_locations.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated storage locations data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - storage_locations_file: CSV file containing storage locations data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the storage locations data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(storage_locations_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Storage locations data file {storage_locations_file} not found.\")\n",
    "        print(\"Material lots will be generated with synthetic storage location IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_material_lots(materials_df=None, suppliers_df=None, storage_locations_df=None, \n",
    "                          num_lots=300, start_time=None, end_time=None,\n",
    "                          output_file=\"data/material_lots.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the MaterialLots table from ISA-95 Level 3.\n",
    "    \n",
    "    Parameters:\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    - suppliers_df: DataFrame containing suppliers data (optional)\n",
    "    - storage_locations_df: DataFrame containing storage locations data (optional)\n",
    "    - num_lots: Number of material lot records to generate\n",
    "    - start_time: Start time for receipt dates (defaults to 180 days ago)\n",
    "    - end_time: End time for receipt dates (defaults to now)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated material lots data\n",
    "    \"\"\"\n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=180)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Generate material IDs if not provided\n",
    "    if materials_df is None or len(materials_df) == 0:\n",
    "        print(\"Generating synthetic material IDs...\")\n",
    "        material_ids = [f\"MAT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "        \n",
    "        # Create synthetic material types\n",
    "        material_types = [\"Raw Material\", \"Packaging\", \"Intermediate\", \"Bulk\", \"Active Ingredient\", \n",
    "                         \"Excipient\", \"Finished Good\", \"Component\", \"Additive\", \"Catalyst\"]\n",
    "        \n",
    "        # Assign material types to material IDs\n",
    "        material_type_map = {}\n",
    "        for mat_id in material_ids:\n",
    "            material_type_map[mat_id] = random.choice(material_types)\n",
    "    else:\n",
    "        material_ids = materials_df['material_id'].unique().tolist()\n",
    "        \n",
    "        # Create material type map from materials_df\n",
    "        material_type_map = {}\n",
    "        for _, material in materials_df.iterrows():\n",
    "            material_type_map[material['material_id']] = material['material_type'] if 'material_type' in material.columns else \"Unknown\"\n",
    "    \n",
    "    # Generate supplier IDs if not provided\n",
    "    if suppliers_df is None or len(suppliers_df) == 0:\n",
    "        print(\"Generating synthetic supplier IDs...\")\n",
    "        supplier_ids = [f\"SUP-{uuid.uuid4().hex[:8].upper()}\" for _ in range(15)]\n",
    "    else:\n",
    "        supplier_ids = suppliers_df['supplier_id'].unique().tolist()\n",
    "    \n",
    "    # Generate storage location IDs if not provided\n",
    "    if storage_locations_df is None or len(storage_locations_df) == 0:\n",
    "        print(\"Generating synthetic storage location IDs...\")\n",
    "        storage_location_ids = [f\"LOC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    else:\n",
    "        storage_location_ids = storage_locations_df['location_id'].unique().tolist()\n",
    "    \n",
    "    # Define quality status options and their probabilities\n",
    "    quality_statuses = {\n",
    "        \"Released\": 0.7,          # Most lots are released\n",
    "        \"Under Quarantine\": 0.1,\n",
    "        \"Rejected\": 0.05,\n",
    "        \"Pending Test\": 0.1,\n",
    "        \"Hold\": 0.03,\n",
    "        \"Expired\": 0.02\n",
    "    }\n",
    "    \n",
    "    # Define possible units of measurement\n",
    "    quantity_units = {\n",
    "        \"Raw Material\": [\"kg\", \"tons\", \"L\", \"m³\", \"drums\"],\n",
    "        \"Packaging\": [\"units\", \"rolls\", \"boxes\", \"pallets\", \"sheets\"],\n",
    "        \"Intermediate\": [\"kg\", \"L\", \"batches\", \"drums\", \"totes\"],\n",
    "        \"Bulk\": [\"kg\", \"L\", \"m³\", \"tons\", \"batches\"],\n",
    "        \"Active Ingredient\": [\"kg\", \"g\", \"mg\", \"L\", \"batches\"],\n",
    "        \"Excipient\": [\"kg\", \"g\", \"L\", \"drums\", \"bags\"],\n",
    "        \"Finished Good\": [\"units\", \"boxes\", \"pallets\", \"cases\", \"bottles\"],\n",
    "        \"Component\": [\"units\", \"pieces\", \"sets\", \"packages\", \"boxes\"],\n",
    "        \"Additive\": [\"kg\", \"g\", \"L\", \"drums\", \"bags\"],\n",
    "        \"Catalyst\": [\"kg\", \"g\", \"L\", \"containers\", \"vials\"]\n",
    "    }\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"lot_id\": [f\"LOT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_lots)],\n",
    "        \"material_id\": [],\n",
    "        \"lot_quantity\": [],\n",
    "        \"quantity_unit\": [],\n",
    "        \"status\": [],\n",
    "        \"creation_date\": [],\n",
    "        \"expiration_date\": [],\n",
    "        \"supplier_id\": [],\n",
    "        \"supplier_lot_id\": [],\n",
    "        \"receipt_date\": [],\n",
    "        \"storage_location_id\": [],\n",
    "        \"quality_status\": [],\n",
    "        \"cost_per_unit\": [],\n",
    "        \"parent_lot_id\": []\n",
    "    }\n",
    "    \n",
    "    # Track lots for potential parent-child relationships\n",
    "    all_lots = data[\"lot_id\"].copy()\n",
    "    potential_parents = random.sample(all_lots, int(len(all_lots) * 0.2))  # 20% can be parents\n",
    "    \n",
    "    # Generate receipt dates distributed over the time range\n",
    "    receipt_dates = []\n",
    "    time_range_days = (end_time - start_time).days\n",
    "    \n",
    "    for _ in range(num_lots):\n",
    "        # Use a beta distribution to weight toward more recent receipts\n",
    "        if random.random() < 0.7:  # 70% of lots are more recent\n",
    "            beta = random.betavariate(2, 1)\n",
    "        else:\n",
    "            beta = random.betavariate(1, 2)\n",
    "        \n",
    "        days_offset = int(beta * time_range_days)\n",
    "        receipt_date = start_time + timedelta(days=days_offset)\n",
    "        receipt_dates.append(receipt_date)\n",
    "    \n",
    "    # Sort receipt dates (older to newer)\n",
    "    receipt_dates.sort()\n",
    "    \n",
    "    # Generate data for each material lot\n",
    "    for i in range(num_lots):\n",
    "        # Select material ID\n",
    "        material_id = random.choice(material_ids)\n",
    "        data[\"material_id\"].append(material_id)\n",
    "        \n",
    "        # Get material type (for appropriate unit selection)\n",
    "        material_type = material_type_map.get(material_id, \"Raw Material\")\n",
    "        \n",
    "        # Select quantity unit based on material type\n",
    "        if material_type in quantity_units:\n",
    "            unit = random.choice(quantity_units[material_type])\n",
    "        else:\n",
    "            unit = random.choice([\"kg\", \"units\", \"L\", \"pallets\", \"pieces\"])\n",
    "        \n",
    "        data[\"quantity_unit\"].append(unit)\n",
    "        \n",
    "        # Generate lot quantity (based on unit)\n",
    "        if unit in [\"kg\", \"L\"]:\n",
    "            # Typically ordered in hundreds or thousands\n",
    "            quantity = random.choice([100, 200, 500, 1000, 2000, 5000]) * random.uniform(0.8, 1.2)\n",
    "        elif unit in [\"g\", \"mg\", \"ml\"]:\n",
    "            # Small quantities for fine materials\n",
    "            quantity = random.choice([100, 500, 1000, 5000, 10000]) * random.uniform(0.8, 1.2)\n",
    "        elif unit in [\"tons\", \"m³\"]:\n",
    "            # Bulk materials in smaller quantities\n",
    "            quantity = random.choice([1, 2, 5, 10, 20, 50]) * random.uniform(0.8, 1.2)\n",
    "        elif unit in [\"units\", \"pieces\", \"bottles\"]:\n",
    "            # Discrete items often in multiples of packaging sizes\n",
    "            quantity = random.choice([100, 500, 1000, 5000, 10000, 25000])\n",
    "        elif unit in [\"pallets\", \"cases\", \"boxes\"]:\n",
    "            # Packaged goods in smaller counts\n",
    "            quantity = random.choice([5, 10, 20, 50, 100]) * random.uniform(0.8, 1.2)\n",
    "        else:\n",
    "            # Default quantity\n",
    "            quantity = random.choice([10, 50, 100, 500, 1000]) * random.uniform(0.8, 1.2)\n",
    "        \n",
    "        data[\"lot_quantity\"].append(round(quantity, 2))\n",
    "        \n",
    "        # Set receipt date from the generated distribution\n",
    "        receipt_date = receipt_dates[i]\n",
    "        data[\"receipt_date\"].append(receipt_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Creation date is typically shortly before receipt (manufacturing date at supplier)\n",
    "        manufacturing_lead_time = random.randint(1, 30)  # 1-30 days lead time\n",
    "        creation_date = receipt_date - timedelta(days=manufacturing_lead_time)\n",
    "        data[\"creation_date\"].append(creation_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Set expiration date based on material type\n",
    "        if material_type == \"Raw Material\":\n",
    "            shelf_life_days = random.randint(365, 1825)  # 1-5 years\n",
    "        elif material_type == \"Active Ingredient\":\n",
    "            shelf_life_days = random.randint(180, 1095)  # 6 months to 3 years\n",
    "        elif material_type in [\"Intermediate\", \"Bulk\"]:\n",
    "            shelf_life_days = random.randint(90, 365)  # 3 months to 1 year\n",
    "        elif material_type == \"Finished Good\":\n",
    "            shelf_life_days = random.randint(180, 730)  # 6 months to 2 years\n",
    "        elif material_type == \"Packaging\":\n",
    "            shelf_life_days = random.randint(730, 3650)  # 2-10 years\n",
    "        else:\n",
    "            shelf_life_days = random.randint(365, 1095)  # 1-3 years\n",
    "        \n",
    "        expiration_date = creation_date + timedelta(days=shelf_life_days)\n",
    "        data[\"expiration_date\"].append(expiration_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Determine status (based on quantity remaining)\n",
    "        if random.random() < 0.7:  # 70% are active inventory\n",
    "            data[\"status\"].append(\"Active\")\n",
    "        elif random.random() < 0.5:  # Half of the remainder are consumed\n",
    "            data[\"status\"].append(\"Consumed\")\n",
    "        else:  # The rest are reserved or in process\n",
    "            data[\"status\"].append(random.choice([\"Reserved\", \"In Process\"]))\n",
    "        \n",
    "        # Assign supplier (raw materials and packaging always have suppliers)\n",
    "        if material_type in [\"Raw Material\", \"Packaging\", \"Active Ingredient\", \"Excipient\", \"Component\"]:\n",
    "            data[\"supplier_id\"].append(random.choice(supplier_ids))\n",
    "            # Generate supplier's lot ID\n",
    "            data[\"supplier_lot_id\"].append(f\"{random.choice(['L', 'B', 'S'])}{random.randint(10000, 99999)}\")\n",
    "        else:\n",
    "            # Internal materials may not have external suppliers\n",
    "            if random.random() < 0.3:  # 30% chance of having supplier even for internal materials\n",
    "                data[\"supplier_id\"].append(random.choice(supplier_ids))\n",
    "                data[\"supplier_lot_id\"].append(f\"{random.choice(['L', 'B', 'S'])}{random.randint(10000, 99999)}\")\n",
    "            else:\n",
    "                data[\"supplier_id\"].append(\"\")\n",
    "                data[\"supplier_lot_id\"].append(\"\")\n",
    "        \n",
    "        # Assign storage location\n",
    "        if data[\"status\"][i] in [\"Active\", \"Reserved\"]:\n",
    "            data[\"storage_location_id\"].append(random.choice(storage_location_ids))\n",
    "        else:\n",
    "            # Consumed or in-process materials may not have a storage location\n",
    "            data[\"storage_location_id\"].append(\"\")\n",
    "        \n",
    "        # Set quality status (weighted random)\n",
    "        data[\"quality_status\"].append(\n",
    "            random.choices(list(quality_statuses.keys()), weights=list(quality_statuses.values()))[0]\n",
    "        )\n",
    "        \n",
    "        # Generate cost per unit (based on material type)\n",
    "        if material_type == \"Active Ingredient\":\n",
    "            # Expensive materials\n",
    "            cost = random.uniform(100, 5000)\n",
    "        elif material_type in [\"Raw Material\", \"Excipient\", \"Catalyst\"]:\n",
    "            # Moderate cost materials\n",
    "            cost = random.uniform(5, 100)\n",
    "        elif material_type in [\"Packaging\", \"Component\"]:\n",
    "            # Lower cost materials\n",
    "            cost = random.uniform(0.5, 10)\n",
    "        elif material_type == \"Finished Good\":\n",
    "            # Higher value products\n",
    "            cost = random.uniform(20, 500)\n",
    "        else:\n",
    "            # Default cost range\n",
    "            cost = random.uniform(1, 50)\n",
    "        \n",
    "        data[\"cost_per_unit\"].append(round(cost, 2))\n",
    "        \n",
    "        # Determine parent lot (if any)\n",
    "        # Intermediate, Bulk, and Finished Good materials are more likely to have parent lots\n",
    "        if (material_type in [\"Intermediate\", \"Bulk\", \"Finished Good\"] and \n",
    "            data[\"lot_id\"][i] not in potential_parents and \n",
    "            random.random() < 0.4):  # 40% chance for applicable materials\n",
    "            \n",
    "            # Find suitable parents (created before this lot)\n",
    "            earlier_lots = [all_lots[j] for j in range(i) if receipt_dates[j] < receipt_date]\n",
    "            if earlier_lots:\n",
    "                parent_id = random.choice(earlier_lots)\n",
    "                data[\"parent_lot_id\"].append(parent_id)\n",
    "            else:\n",
    "                data[\"parent_lot_id\"].append(\"\")\n",
    "        else:\n",
    "            data[\"parent_lot_id\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} material lot records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_material_transactions(material_lots_df, work_orders_df=None, num_transactions=500,\n",
    "                                  start_time=None, end_time=None, output_file=\"data/material_transactions.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the MaterialTransactions table.\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_df: DataFrame containing material lots data\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - num_transactions: Number of transaction records to generate\n",
    "    - start_time: Start time for transaction dates (defaults to 180 days ago)\n",
    "    - end_time: End time for transaction dates (defaults to now)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated material transactions data\n",
    "    \"\"\"\n",
    "    if material_lots_df is None or len(material_lots_df) == 0:\n",
    "        print(\"Error: No material lots data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=180)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Get storage location IDs from material lots\n",
    "    storage_locations = material_lots_df['storage_location_id'].unique()\n",
    "    storage_locations = [loc for loc in storage_locations if pd.notna(loc) and loc != \"\"]\n",
    "    \n",
    "    if not storage_locations:\n",
    "        print(\"Warning: No storage locations found in material lots data.\")\n",
    "        storage_locations = [f\"LOC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(5)]\n",
    "    \n",
    "    # Get work order IDs if available\n",
    "    if work_orders_df is not None and len(work_orders_df) > 0:\n",
    "        work_order_ids = work_orders_df['work_order_id'].unique().tolist()\n",
    "    else:\n",
    "        work_order_ids = []\n",
    "    \n",
    "    # Define transaction types and their probabilities\n",
    "    transaction_types = {\n",
    "        \"Receipt\": 0.25,          # Initial receipt from supplier\n",
    "        \"Issue\": 0.3,             # Issue to production\n",
    "        \"Return\": 0.05,           # Return from production\n",
    "        \"Transfer\": 0.2,          # Move between locations\n",
    "        \"Adjustment\": 0.1,        # Inventory adjustment\n",
    "        \"Consumption\": 0.05,      # Material consumed\n",
    "        \"Scrapping\": 0.05         # Disposal of material\n",
    "    }\n",
    "    \n",
    "    # Define possible reasons for each transaction type\n",
    "    transaction_reasons = {\n",
    "        \"Receipt\": [\"Initial Receipt\", \"Purchase Order\", \"Vendor Delivery\", \"Stock Replenishment\", \"Contract Manufacturing\"],\n",
    "        \"Issue\": [\"Production Order\", \"Work Order\", \"Batch Production\", \"Line Replenishment\", \"Process Requirement\"],\n",
    "        \"Return\": [\"Excess Material\", \"Quality Issue\", \"Process Change\", \"Order Cancellation\", \"Wrong Material\"],\n",
    "        \"Transfer\": [\"Storage Optimization\", \"Staging for Production\", \"Quarantine\", \"Relocation\", \"Consolidation\"],\n",
    "        \"Adjustment\": [\"Cycle Count\", \"Inventory Audit\", \"Quantity Correction\", \"System Reconciliation\", \"Physical Count\"],\n",
    "        \"Consumption\": [\"Material Used\", \"Process Consumption\", \"Batch Completion\", \"Manufacturing Process\", \"Test Samples\"],\n",
    "        \"Scrapping\": [\"Quality Rejection\", \"Expired Material\", \"Damaged Goods\", \"Contamination\", \"Obsolete Material\"]\n",
    "    }\n",
    "    \n",
    "    # Generate transaction data\n",
    "    data = {\n",
    "        \"transaction_id\": [f\"TRAN-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_transactions)],\n",
    "        \"transaction_type\": [],\n",
    "        \"lot_id\": [],\n",
    "        \"timestamp\": [],\n",
    "        \"quantity\": [],\n",
    "        \"from_location_id\": [],\n",
    "        \"to_location_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"batch_id\": [],\n",
    "        \"operator_id\": [],\n",
    "        \"transaction_reason\": [],\n",
    "        \"reference_document\": []\n",
    "    }\n",
    "    \n",
    "    # Generate operator IDs\n",
    "    operator_ids = [f\"OP-{uuid.uuid4().hex[:6].upper()}\" for _ in range(10)]\n",
    "    \n",
    "    # Generate batch IDs (synthetic)\n",
    "    batch_ids = [f\"BATCH-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    \n",
    "    # Generate document reference patterns\n",
    "    po_pattern = \"PO-{}\"\n",
    "    wo_pattern = \"WO-{}\"\n",
    "    gr_pattern = \"GR-{}\"\n",
    "    adj_pattern = \"ADJ-{}\"\n",
    "    \n",
    "    # Generate transactions\n",
    "    active_lots = material_lots_df[material_lots_df['status'].isin(['Active', 'Reserved', 'In Process'])]['lot_id'].tolist()\n",
    "    consumed_lots = material_lots_df[material_lots_df['status'] == 'Consumed']['lot_id'].tolist()\n",
    "    all_lots = material_lots_df['lot_id'].tolist()\n",
    "    \n",
    "    # Generate timestamps distributed over the time range\n",
    "    time_range_minutes = int((end_time - start_time).total_seconds() / 60)\n",
    "    timestamps = []\n",
    "    \n",
    "    for _ in range(num_transactions):\n",
    "        random_minutes = random.randint(0, time_range_minutes)\n",
    "        timestamp = start_time + timedelta(minutes=random_minutes)\n",
    "        timestamps.append(timestamp)\n",
    "    \n",
    "    # Sort timestamps (older to newer)\n",
    "    timestamps.sort()\n",
    "    \n",
    "    # Now generate transaction data\n",
    "    for i in range(num_transactions):\n",
    "        # Determine transaction type (weighted random)\n",
    "        transaction_type = random.choices(list(transaction_types.keys()), \n",
    "                                        weights=list(transaction_types.values()))[0]\n",
    "        data[\"transaction_type\"].append(transaction_type)\n",
    "        \n",
    "        # Set timestamp\n",
    "        data[\"timestamp\"].append(timestamps[i].strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Select lot based on transaction type\n",
    "        if transaction_type == \"Receipt\":\n",
    "            # Use any lot (as if it's being received)\n",
    "            lot_id = random.choice(all_lots)\n",
    "        elif transaction_type in [\"Issue\", \"Transfer\", \"Return\", \"Adjustment\"]:\n",
    "            # Use active lots\n",
    "            if active_lots:\n",
    "                lot_id = random.choice(active_lots)\n",
    "            else:\n",
    "                lot_id = random.choice(all_lots)\n",
    "        elif transaction_type in [\"Consumption\", \"Scrapping\"]:\n",
    "            # Prefer consumed lots for consistency, but can use any\n",
    "            if consumed_lots and random.random() < 0.7:\n",
    "                lot_id = random.choice(consumed_lots)\n",
    "            elif active_lots:\n",
    "                lot_id = random.choice(active_lots)\n",
    "            else:\n",
    "                lot_id = random.choice(all_lots)\n",
    "        else:\n",
    "            lot_id = random.choice(all_lots)\n",
    "        \n",
    "        data[\"lot_id\"].append(lot_id)\n",
    "        \n",
    "        # Get lot information\n",
    "        lot_info = material_lots_df[material_lots_df['lot_id'] == lot_id].iloc[0]\n",
    "        total_quantity = lot_info['lot_quantity']\n",
    "        \n",
    "        # Determine transaction quantity\n",
    "        if transaction_type == \"Receipt\":\n",
    "            # Receipt is typically the full quantity\n",
    "            quantity = total_quantity\n",
    "        elif transaction_type == \"Issue\":\n",
    "            # Issue is typically a portion or all of the quantity\n",
    "            quantity = total_quantity * random.uniform(0.1, 1.0)\n",
    "        elif transaction_type == \"Return\":\n",
    "            # Return is typically a smaller portion\n",
    "            quantity = total_quantity * random.uniform(0.05, 0.3)\n",
    "        elif transaction_type == \"Transfer\":\n",
    "            # Transfer is typically the full quantity\n",
    "            quantity = total_quantity\n",
    "        elif transaction_type == \"Adjustment\":\n",
    "            # Adjustment can be positive or negative\n",
    "            if random.random() < 0.5:\n",
    "                # Positive adjustment\n",
    "                quantity = total_quantity * random.uniform(0.01, 0.1)\n",
    "            else:\n",
    "                # Negative adjustment\n",
    "                quantity = -total_quantity * random.uniform(0.01, 0.1)\n",
    "        elif transaction_type == \"Consumption\":\n",
    "            # Consumption is typically a large portion or all\n",
    "            quantity = total_quantity * random.uniform(0.5, 1.0)\n",
    "        elif transaction_type == \"Scrapping\":\n",
    "            # Scrapping can be a portion or all\n",
    "            quantity = total_quantity * random.uniform(0.1, 1.0)\n",
    "        \n",
    "        data[\"quantity\"].append(round(quantity, 2))\n",
    "        \n",
    "        # Set location information based on transaction type\n",
    "        if transaction_type == \"Receipt\":\n",
    "            # From supplier (blank) to storage\n",
    "            data[\"from_location_id\"].append(\"\")\n",
    "            data[\"to_location_id\"].append(random.choice(storage_locations))\n",
    "        elif transaction_type == \"Issue\":\n",
    "            # From storage to production (can be blank)\n",
    "            data[\"from_location_id\"].append(lot_info['storage_location_id'] if pd.notna(lot_info['storage_location_id']) else random.choice(storage_locations))\n",
    "            data[\"to_location_id\"].append(\"\")  # Issued to production, not a storage location\n",
    "        elif transaction_type == \"Return\":\n",
    "            # From production (blank) to storage\n",
    "            data[\"from_location_id\"].append(\"\")\n",
    "            data[\"to_location_id\"].append(lot_info['storage_location_id'] if pd.notna(lot_info['storage_location_id']) else random.choice(storage_locations))\n",
    "        elif transaction_type == \"Transfer\":\n",
    "            # From one storage location to another\n",
    "            from_loc = lot_info['storage_location_id'] if pd.notna(lot_info['storage_location_id']) else random.choice(storage_locations)\n",
    "            # Ensure to_location is different from from_location\n",
    "            available_to_locs = [loc for loc in storage_locations if loc != from_loc]\n",
    "            to_loc = random.choice(available_to_locs) if available_to_locs else random.choice(storage_locations)\n",
    "            data[\"from_location_id\"].append(from_loc)\n",
    "            data[\"to_location_id\"].append(to_loc)\n",
    "        elif transaction_type == \"Adjustment\":\n",
    "            # Adjustment happens in the current location\n",
    "            data[\"from_location_id\"].append(lot_info['storage_location_id'] if pd.notna(lot_info['storage_location_id']) else random.choice(storage_locations))\n",
    "            data[\"to_location_id\"].append(\"\")\n",
    "        elif transaction_type in [\"Consumption\", \"Scrapping\"]:\n",
    "            # From storage to nowhere (consumed/scrapped)\n",
    "            data[\"from_location_id\"].append(lot_info['storage_location_id'] if pd.notna(lot_info['storage_location_id']) else random.choice(storage_locations))\n",
    "            data[\"to_location_id\"].append(\"\")\n",
    "        \n",
    "        # Associate with work order if applicable\n",
    "        if transaction_type in [\"Issue\", \"Consumption\"] and work_order_ids and random.random() < 0.8:\n",
    "            # 80% chance of having a work order for production-related transactions\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        elif transaction_type == \"Return\" and work_order_ids and random.random() < 0.6:\n",
    "            # 60% chance of having a work order for returns\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        else:\n",
    "            data[\"work_order_id\"].append(\"\")\n",
    "        \n",
    "        # Associate with batch if applicable\n",
    "        if transaction_type in [\"Issue\", \"Consumption\", \"Return\"] and random.random() < 0.7:\n",
    "            # 70% chance of having a batch for production-related transactions\n",
    "            data[\"batch_id\"].append(random.choice(batch_ids))\n",
    "        else:\n",
    "            data[\"batch_id\"].append(\"\")\n",
    "        \n",
    "        # Set operator\n",
    "        data[\"operator_id\"].append(random.choice(operator_ids))\n",
    "        \n",
    "        # Set transaction reason\n",
    "        if transaction_type in transaction_reasons:\n",
    "            reason = random.choice(transaction_reasons[transaction_type])\n",
    "        else:\n",
    "            reason = \"Standard Transaction\"\n",
    "        \n",
    "        data[\"transaction_reason\"].append(reason)\n",
    "        \n",
    "        # Generate reference document\n",
    "        if transaction_type == \"Receipt\":\n",
    "            ref_doc = po_pattern.format(random.randint(10000, 99999))\n",
    "        elif transaction_type in [\"Issue\", \"Consumption\"]:\n",
    "            if data[\"work_order_id\"][i]:\n",
    "                ref_doc = wo_pattern.format(data[\"work_order_id\"][i].split('-')[-1])\n",
    "            else:\n",
    "                ref_doc = wo_pattern.format(random.randint(10000, 99999))\n",
    "        elif transaction_type == \"Adjustment\":\n",
    "            ref_doc = adj_pattern.format(random.randint(10000, 99999))\n",
    "        elif transaction_type == \"Transfer\":\n",
    "            ref_doc = gr_pattern.format(random.randint(10000, 99999))\n",
    "        else:\n",
    "            ref_doc = f\"DOC-{random.randint(10000, 99999)}\"\n",
    "        \n",
    "        data[\"reference_document\"].append(ref_doc)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} material transaction records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(material_lots_df, material_transactions_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated material lots and transactions data\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_df: DataFrame containing material lots data\n",
    "    - material_transactions_df: DataFrame containing material transactions data (optional)\n",
    "    \"\"\"\n",
    "    if material_lots_df is None or len(material_lots_df) == 0:\n",
    "        print(\"No material lots data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nMaterial Lots Statistics:\")\n",
    "    print(f\"Total material lots: {len(material_lots_df)}\")\n",
    "    \n",
    "    print(\"\\nStatus Distribution:\")\n",
    "    status_counts = material_lots_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nQuality Status Distribution:\")\n",
    "    quality_counts = material_lots_df['quality_status'].value_counts()\n",
    "    for status, count in quality_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nQuantity Units Distribution:\")\n",
    "    unit_counts = material_lots_df['quantity_unit'].value_counts().head(10)\n",
    "    for unit, count in unit_counts.items():\n",
    "        print(f\"  {unit}: {count} ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Supplier connections\n",
    "    supplier_count = material_lots_df['supplier_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nLots with supplier association: {supplier_count} ({supplier_count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Storage location connections\n",
    "    location_count = material_lots_df['storage_location_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"Lots with storage location: {location_count} ({location_count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Parent lot relationships\n",
    "    parent_count = material_lots_df['parent_lot_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"Lots with parent lot: {parent_count} ({parent_count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Quantity and cost statistics\n",
    "    print(\"\\nQuantity Statistics:\")\n",
    "    print(f\"  Total quantity: {material_lots_df['lot_quantity'].sum():.1f} (in various units)\")\n",
    "    print(f\"  Average lot size: {material_lots_df['lot_quantity'].mean():.1f}\")\n",
    "    print(f\"  Min lot size: {material_lots_df['lot_quantity'].min():.1f}\")\n",
    "    print(f\"  Max lot size: {material_lots_df['lot_quantity'].max():.1f}\")\n",
    "    \n",
    "    print(\"\\nCost Statistics:\")\n",
    "    print(f\"  Average cost per unit: ${material_lots_df['cost_per_unit'].mean():.2f}\")\n",
    "    print(f\"  Min cost per unit: ${material_lots_df['cost_per_unit'].min():.2f}\")\n",
    "    print(f\"  Max cost per unit: ${material_lots_df['cost_per_unit'].max():.2f}\")\n",
    "    \n",
    "    # Age analysis\n",
    "    material_lots_df['creation_date'] = pd.to_datetime(material_lots_df['creation_date'])\n",
    "    material_lots_df['expiration_date'] = pd.to_datetime(material_lots_df['expiration_date'])\n",
    "    \n",
    "    material_lots_df['shelf_life_days'] = (material_lots_df['expiration_date'] - material_lots_df['creation_date']).dt.days\n",
    "    \n",
    "    print(\"\\nShelf Life Statistics:\")\n",
    "    print(f\"  Average shelf life: {material_lots_df['shelf_life_days'].mean():.1f} days\")\n",
    "    print(f\"  Min shelf life: {material_lots_df['shelf_life_days'].min()} days\")\n",
    "    print(f\"  Max shelf life: {material_lots_df['shelf_life_days'].max()} days\")\n",
    "    \n",
    "    # Calculate remaining shelf life\n",
    "    now = pd.Timestamp.now()\n",
    "    material_lots_df['remaining_days'] = (material_lots_df['expiration_date'] - now).dt.days\n",
    "    \n",
    "    # Only consider active lots\n",
    "    active_lots = material_lots_df[material_lots_df['status'] == 'Active']\n",
    "    if len(active_lots) > 0:\n",
    "        print(\"\\nRemaining Shelf Life (Active Lots):\")\n",
    "        print(f\"  Average remaining shelf life: {active_lots['remaining_days'].mean():.1f} days\")\n",
    "        print(f\"  Min remaining shelf life: {active_lots['remaining_days'].min()} days\")\n",
    "        print(f\"  Max remaining shelf life: {active_lots['remaining_days'].max()} days\")\n",
    "        \n",
    "        # Expired lots\n",
    "        expired_count = len(active_lots[active_lots['remaining_days'] <= 0])\n",
    "        print(f\"  Expired lots still active: {expired_count} ({expired_count/len(active_lots)*100:.1f}% of active lots)\")\n",
    "        \n",
    "        # Near expiry (less than 30 days)\n",
    "        near_expiry = len(active_lots[(active_lots['remaining_days'] > 0) & (active_lots['remaining_days'] <= 30)])\n",
    "        print(f\"  Lots expiring within 30 days: {near_expiry} ({near_expiry/len(active_lots)*100:.1f}% of active lots)\")\n",
    "    \n",
    "    # Material transactions statistics (if available)\n",
    "    if material_transactions_df is not None and len(material_transactions_df) > 0:\n",
    "        print(\"\\nMaterial Transactions Statistics:\")\n",
    "        print(f\"Total transactions: {len(material_transactions_df)}\")\n",
    "        \n",
    "        print(\"\\nTransaction Type Distribution:\")\n",
    "        trans_type_counts = material_transactions_df['transaction_type'].value_counts()\n",
    "        for trans_type, count in trans_type_counts.items():\n",
    "            print(f\"  {trans_type}: {count} ({count/len(material_transactions_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Work order connections\n",
    "        wo_count = material_transactions_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "        print(f\"\\nTransactions with work order: {wo_count} ({wo_count/len(material_transactions_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Batch connections\n",
    "        batch_count = material_transactions_df['batch_id'].apply(lambda x: x != \"\").sum()\n",
    "        print(f\"Transactions with batch: {batch_count} ({batch_count/len(material_transactions_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Quantity statistics\n",
    "        print(\"\\nTransaction Quantity Statistics:\")\n",
    "        print(f\"  Average transaction quantity: {material_transactions_df['quantity'].mean():.1f}\")\n",
    "        print(f\"  Min transaction quantity: {material_transactions_df['quantity'].min():.1f}\")\n",
    "        print(f\"  Max transaction quantity: {material_transactions_df['quantity'].max():.1f}\")\n",
    "        \n",
    "        # Time distribution\n",
    "        material_transactions_df['timestamp'] = pd.to_datetime(material_transactions_df['timestamp'])\n",
    "        \n",
    "        # Group by month\n",
    "        material_transactions_df['month'] = material_transactions_df['timestamp'].dt.to_period('M')\n",
    "        monthly_counts = material_transactions_df['month'].value_counts().sort_index()\n",
    "        \n",
    "        print(\"\\nMonthly Transaction Distribution:\")\n",
    "        for month, count in monthly_counts.items():\n",
    "            print(f\"  {month}: {count} transactions\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    materials_df = load_materials_data()\n",
    "    suppliers_df = load_suppliers_data()\n",
    "    storage_locations_df = load_storage_locations_data()\n",
    "    \n",
    "    # Try to load work orders for transactions\n",
    "    try:\n",
    "        work_orders_df = pd.read_csv(\"data/work_orders.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Note: Work orders data file not found.\")\n",
    "        work_orders_df = None\n",
    "    \n",
    "    # Generate material lots data\n",
    "    material_lots_df = generate_material_lots(\n",
    "        materials_df,\n",
    "        suppliers_df,\n",
    "        storage_locations_df,\n",
    "        num_lots=300,  # Generate 300 material lot records\n",
    "        output_file=\"data/material_lots.csv\"\n",
    "    )\n",
    "    \n",
    "    # Generate material transactions if lots were created successfully\n",
    "    if material_lots_df is not None:\n",
    "        material_transactions_df = generate_material_transactions(\n",
    "            material_lots_df,\n",
    "            work_orders_df,\n",
    "            num_transactions=500,  # Generate 500 transaction records\n",
    "            output_file=\"data/material_transactions.csv\"\n",
    "        )\n",
    "    else:\n",
    "        material_transactions_df = None\n",
    "    \n",
    "    # Display statistics\n",
    "    if material_lots_df is not None:\n",
    "        display_statistics(material_lots_df, material_transactions_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample material lots data (first 5 records):\")\n",
    "        print(material_lots_df.head(5))\n",
    "        \n",
    "        if material_transactions_df is not None:\n",
    "            print(\"\\nSample material transactions data (first 5 records):\")\n",
    "            print(material_transactions_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186df3d",
   "metadata": {},
   "source": [
    "Material Consumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb5b4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Material consumption will be generated with synthetic operator IDs.\n",
      "Generating synthetic operator IDs...\n",
      "Successfully generated 400 material consumption records.\n",
      "Data saved to data/material_consumption.csv\n",
      "\n",
      "Material Consumption Statistics:\n",
      "Total consumption records: 400\n",
      "\n",
      "Consumption records with batch: 360 (90.0%)\n",
      "Consumption records with work order: 310 (77.5%)\n",
      "Consumption records with batch step: 270 (67.5%)\n",
      "\n",
      "Consumption Quantity Statistics:\n",
      "  Total quantity consumed: 282133.2 (in various units)\n",
      "  Average consumption quantity: 705.3\n",
      "  Min consumption quantity: 0.2\n",
      "  Max consumption quantity: 19105.0\n",
      "\n",
      "Planned vs Actual Consumption:\n",
      "  Total planned consumption: 285244.0\n",
      "  Total actual consumption: 282133.2\n",
      "  Total variance: -3110.8\n",
      "  Average absolute variance: 4.3%\n",
      "\n",
      "Consumption Variance Distribution:\n",
      "  Over-consumption: 201 records (50.2%)\n",
      "  Under-consumption: 193 records (48.2%)\n",
      "  On-target: 6 records (1.5%)\n",
      "\n",
      "Monthly Consumption Distribution:\n",
      "  2025-01: 36 consumption records\n",
      "  2025-02: 58 consumption records\n",
      "  2025-03: 60 consumption records\n",
      "  2025-04: 71 consumption records\n",
      "  2025-05: 75 consumption records\n",
      "  2025-06: 66 consumption records\n",
      "  2025-07: 34 consumption records\n",
      "\n",
      "Unit of Measurement Distribution:\n",
      "  kg: 56 records (14.0%)\n",
      "  L: 53 records (13.2%)\n",
      "  batches: 47 records (11.8%)\n",
      "  g: 47 records (11.8%)\n",
      "  drums: 24 records (6.0%)\n",
      "  m³: 23 records (5.8%)\n",
      "  pallets: 18 records (4.5%)\n",
      "  boxes: 16 records (4.0%)\n",
      "  units: 16 records (4.0%)\n",
      "  sets: 14 records (3.5%)\n",
      "  tons: 12 records (3.0%)\n",
      "  bags: 11 records (2.8%)\n",
      "  vials: 11 records (2.8%)\n",
      "  containers: 11 records (2.8%)\n",
      "  cases: 9 records (2.2%)\n",
      "  mg: 8 records (2.0%)\n",
      "  totes: 6 records (1.5%)\n",
      "  sheets: 5 records (1.2%)\n",
      "  packages: 5 records (1.2%)\n",
      "  rolls: 4 records (1.0%)\n",
      "  bottles: 2 records (0.5%)\n",
      "  pieces: 2 records (0.5%)\n",
      "\n",
      "Material Lot Usage Frequency:\n",
      "  Average usage per lot: 1.9 times\n",
      "  Max usage per lot: 5 times\n",
      "  Single-use lots: 86 lots\n",
      "  Multi-use lots: 124 lots\n",
      "\n",
      "Top Equipment Usage:\n",
      "  EQ-5780B275: 9 consumption records\n",
      "  EQ-7989CA69: 6 consumption records\n",
      "  EQ-71EBC886: 6 consumption records\n",
      "  EQ-8953C760: 6 consumption records\n",
      "  EQ-7958ED31: 6 consumption records\n",
      "  EQ-720A372C: 5 consumption records\n",
      "  EQ-1B50660F: 5 consumption records\n",
      "  EQ-ABEEE950: 5 consumption records\n",
      "  EQ-0D5700B0: 5 consumption records\n",
      "  EQ-40873C7B: 5 consumption records\n",
      "\n",
      "Sample material consumption data (first 5 records):\n",
      "  consumption_id        lot_id        batch_id work_order_id  \\\n",
      "0  CONS-4334809C  LOT-8F34A0C5  BATCH-544B3F37   WO-ECF6B07E   \n",
      "1  CONS-3B68AD6B  LOT-FAC37BC1  BATCH-AD5DB274   WO-9C3600AD   \n",
      "2  CONS-36E63D71  LOT-27E3484C  BATCH-D73F1360   WO-24D6C2AB   \n",
      "3  CONS-228AD8FC  LOT-400DD61D  BATCH-5B21DC4D   WO-9C3600AD   \n",
      "4  CONS-D6F368EC  LOT-E1391F91  BATCH-FCDB3D14   WO-D6F7ED8C   \n",
      "\n",
      "            timestamp  quantity     unit equipment_id        step_id  \\\n",
      "0 2025-01-18 14:00:57      6.70    cases  EQ-93527ACF  STEP-9CD745F9   \n",
      "1 2025-01-18 16:19:57    149.26        L  EQ-9A644928  STEP-B3CD85CC   \n",
      "2 2025-01-18 17:44:57      4.00  pallets  EQ-277C7BA4  STEP-AC4B1C16   \n",
      "3 2025-01-19 05:15:57    385.86       kg  EQ-DEB311BD  STEP-88342D84   \n",
      "4 2025-01-19 06:22:57   3020.90        L  EQ-AC717F77  STEP-A9E58388   \n",
      "\n",
      "  operator_id  planned_consumption  consumption_variance  variance_pct  \\\n",
      "0   OP-3438D5                 7.14                 -0.44      6.162465   \n",
      "1   OP-65B8AF               158.62                 -9.36      5.900895   \n",
      "2   OP-8D2D3D                 3.82                  0.18      4.712042   \n",
      "3   OP-3ADEC8               343.81                 42.05     12.230592   \n",
      "4   OP-3438D5              2932.70                 88.20      3.007468   \n",
      "\n",
      "     month  \n",
      "0  2025-01  \n",
      "1  2025-01  \n",
      "2  2025-01  \n",
      "3  2025-01  \n",
      "4  2025-01  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_material_lots_data(material_lots_file=\"data/material_lots.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated material lots data\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_file: CSV file containing material lots data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the material lots data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(material_lots_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Material lots data file {material_lots_file} not found.\")\n",
    "        print(\"Please run the material lots data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_batches_data(batches_file=\"data/batches.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated batches data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - batches_file: CSV file containing batches data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the batches data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(batches_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Batches data file {batches_file} not found.\")\n",
    "        print(\"Material consumption will be generated with synthetic batch IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_work_orders_data(work_orders_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated work orders data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_file: CSV file containing work orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the work orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(work_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Work orders data file {work_orders_file} not found.\")\n",
    "        print(\"Material consumption will be generated with synthetic work order IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Material consumption will be generated with synthetic equipment IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_batch_execution_data(batch_execution_file=\"data/batch_execution.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated batch execution data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_execution_file: CSV file containing batch execution data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the batch execution data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(batch_execution_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Batch execution data file {batch_execution_file} not found.\")\n",
    "        print(\"Material consumption will be generated with synthetic batch step IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Material consumption will be generated with synthetic operator IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_material_consumption(material_lots_df, batches_df=None, work_orders_df=None, \n",
    "                                equipment_df=None, batch_execution_df=None, personnel_df=None,\n",
    "                                num_consumptions=400, start_time=None, end_time=None,\n",
    "                                output_file=\"data/material_consumption.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the MaterialConsumption table from ISA-95 Level 3.\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_df: DataFrame containing material lots data\n",
    "    - batches_df: DataFrame containing batches data (optional)\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    - batch_execution_df: DataFrame containing batch execution data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_consumptions: Number of material consumption records to generate\n",
    "    - start_time: Start time for consumption dates (defaults to 180 days ago)\n",
    "    - end_time: End time for consumption dates (defaults to now)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated material consumption data\n",
    "    \"\"\"\n",
    "    if material_lots_df is None or len(material_lots_df) == 0:\n",
    "        print(\"Error: No material lots data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=180)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Filter material lots that can be consumed (active or in process)\n",
    "    consumable_lots = material_lots_df[\n",
    "        (material_lots_df['status'].isin(['Active', 'In Process', 'Consumed']))\n",
    "    ]\n",
    "    \n",
    "    if len(consumable_lots) == 0:\n",
    "        print(\"Warning: No consumable material lots found. Using all available lots.\")\n",
    "        consumable_lots = material_lots_df\n",
    "    \n",
    "    # Get batch IDs if available, otherwise generate synthetic ones\n",
    "    if batches_df is not None and len(batches_df) > 0:\n",
    "        batch_ids = batches_df['batch_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic batch IDs...\")\n",
    "        batch_ids = [f\"BATCH-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    \n",
    "    # Get work order IDs if available, otherwise generate synthetic ones\n",
    "    if work_orders_df is not None and len(work_orders_df) > 0:\n",
    "        work_order_ids = work_orders_df['work_order_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    \n",
    "    # Get equipment IDs if available, otherwise generate synthetic ones\n",
    "    if equipment_df is not None and len(equipment_df) > 0:\n",
    "        equipment_ids = equipment_df['equipment_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic equipment IDs...\")\n",
    "        equipment_ids = [f\"EQ-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    \n",
    "    # Get batch step IDs if available, otherwise generate synthetic ones\n",
    "    if batch_execution_df is not None and len(batch_execution_df) > 0:\n",
    "        batch_step_ids = batch_execution_df['step_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic batch step IDs...\")\n",
    "        batch_step_ids = [f\"STEP-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    \n",
    "    # Get operator IDs if available, otherwise generate synthetic ones\n",
    "    if personnel_df is not None and len(personnel_df) > 0:\n",
    "        operator_ids = personnel_df['personnel_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic operator IDs...\")\n",
    "        operator_ids = [f\"OP-{uuid.uuid4().hex[:6].upper()}\" for _ in range(15)]\n",
    "    \n",
    "    # Generate consumption data\n",
    "    data = {\n",
    "        \"consumption_id\": [f\"CONS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_consumptions)],\n",
    "        \"lot_id\": [],\n",
    "        \"batch_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"timestamp\": [],\n",
    "        \"quantity\": [],\n",
    "        \"unit\": [],\n",
    "        \"equipment_id\": [],\n",
    "        \"step_id\": [],\n",
    "        \"operator_id\": [],\n",
    "        \"planned_consumption\": [],\n",
    "        \"consumption_variance\": []\n",
    "    }\n",
    "    \n",
    "    # Generate timestamps distributed over the time range\n",
    "    time_range_minutes = int((end_time - start_time).total_seconds() / 60)\n",
    "    timestamps = []\n",
    "    \n",
    "    for _ in range(num_consumptions):\n",
    "        random_minutes = random.randint(0, time_range_minutes)\n",
    "        timestamp = start_time + timedelta(minutes=random_minutes)\n",
    "        timestamps.append(timestamp)\n",
    "    \n",
    "    # Sort timestamps (older to newer)\n",
    "    timestamps.sort()\n",
    "    \n",
    "    # Generate data for each consumption record\n",
    "    for i in range(num_consumptions):\n",
    "        # Select a material lot to consume\n",
    "        if len(consumable_lots) > 0:\n",
    "            lot = consumable_lots.sample(1).iloc[0]\n",
    "            data[\"lot_id\"].append(lot['lot_id'])\n",
    "            \n",
    "            # Use the lot's unit\n",
    "            unit = lot['quantity_unit']\n",
    "            data[\"unit\"].append(unit)\n",
    "            \n",
    "            # Maximum consumption is the lot quantity\n",
    "            max_consumption = float(lot['lot_quantity'])\n",
    "            \n",
    "            # Typical consumption is a portion of the lot\n",
    "            typical_consumption = max_consumption * random.uniform(0.05, 0.9)\n",
    "        else:\n",
    "            # Fallback if no lots are available\n",
    "            data[\"lot_id\"].append(f\"LOT-{uuid.uuid4().hex[:8].upper()}\")\n",
    "            unit = random.choice([\"kg\", \"L\", \"units\", \"g\", \"ml\", \"pieces\"])\n",
    "            data[\"unit\"].append(unit)\n",
    "            max_consumption = random.uniform(100, 5000)\n",
    "            typical_consumption = max_consumption * random.uniform(0.05, 0.9)\n",
    "        \n",
    "        # Set timestamp\n",
    "        data[\"timestamp\"].append(timestamps[i].strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Assign to batch and work order\n",
    "        # Consumption records typically have both, but we'll allow some variation\n",
    "        if random.random() < 0.9:  # 90% have batch\n",
    "            data[\"batch_id\"].append(random.choice(batch_ids))\n",
    "        else:\n",
    "            data[\"batch_id\"].append(\"\")\n",
    "            \n",
    "        if random.random() < 0.8:  # 80% have work order\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        else:\n",
    "            data[\"work_order_id\"].append(\"\")\n",
    "        \n",
    "        # Assign equipment\n",
    "        data[\"equipment_id\"].append(random.choice(equipment_ids))\n",
    "        \n",
    "        # Assign batch step\n",
    "        if random.random() < 0.7:  # 70% have specific step\n",
    "            data[\"step_id\"].append(random.choice(batch_step_ids))\n",
    "        else:\n",
    "            data[\"step_id\"].append(\"\")\n",
    "        \n",
    "        # Assign operator\n",
    "        data[\"operator_id\"].append(random.choice(operator_ids))\n",
    "        \n",
    "        # Generate consumption quantity\n",
    "        # Actual consumption has some variance from planned\n",
    "        planned_consumption = round(typical_consumption, 2)\n",
    "        data[\"planned_consumption\"].append(planned_consumption)\n",
    "        \n",
    "        # Actual consumption varies from planned\n",
    "        variation_pct = random.normalvariate(0, 0.05)  # Normal distribution around 0 with 5% std dev\n",
    "        actual_consumption = planned_consumption * (1 + variation_pct)\n",
    "        actual_consumption = round(min(max_consumption, max(0, actual_consumption)), 2)\n",
    "        data[\"quantity\"].append(actual_consumption)\n",
    "        \n",
    "        # Calculate variance\n",
    "        variance = actual_consumption - planned_consumption\n",
    "        data[\"consumption_variance\"].append(round(variance, 2))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} material consumption records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(material_consumption_df):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated material consumption data\n",
    "    \n",
    "    Parameters:\n",
    "    - material_consumption_df: DataFrame containing material consumption data\n",
    "    \"\"\"\n",
    "    if material_consumption_df is None or len(material_consumption_df) == 0:\n",
    "        print(\"No material consumption data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nMaterial Consumption Statistics:\")\n",
    "    print(f\"Total consumption records: {len(material_consumption_df)}\")\n",
    "    \n",
    "    # Batch connections\n",
    "    batch_count = material_consumption_df['batch_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nConsumption records with batch: {batch_count} ({batch_count/len(material_consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Work order connections\n",
    "    wo_count = material_consumption_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"Consumption records with work order: {wo_count} ({wo_count/len(material_consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Step connections\n",
    "    step_count = material_consumption_df['step_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"Consumption records with batch step: {step_count} ({step_count/len(material_consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Quantity statistics\n",
    "    print(\"\\nConsumption Quantity Statistics:\")\n",
    "    print(f\"  Total quantity consumed: {material_consumption_df['quantity'].sum():.1f} (in various units)\")\n",
    "    print(f\"  Average consumption quantity: {material_consumption_df['quantity'].mean():.1f}\")\n",
    "    print(f\"  Min consumption quantity: {material_consumption_df['quantity'].min():.1f}\")\n",
    "    print(f\"  Max consumption quantity: {material_consumption_df['quantity'].max():.1f}\")\n",
    "    \n",
    "    # Planned vs Actual statistics\n",
    "    print(\"\\nPlanned vs Actual Consumption:\")\n",
    "    print(f\"  Total planned consumption: {material_consumption_df['planned_consumption'].sum():.1f}\")\n",
    "    print(f\"  Total actual consumption: {material_consumption_df['quantity'].sum():.1f}\")\n",
    "    print(f\"  Total variance: {material_consumption_df['consumption_variance'].sum():.1f}\")\n",
    "    \n",
    "    # Calculate average absolute variance percentage\n",
    "    material_consumption_df['variance_pct'] = abs(material_consumption_df['consumption_variance'] / material_consumption_df['planned_consumption'] * 100)\n",
    "    avg_variance_pct = material_consumption_df['variance_pct'].mean()\n",
    "    print(f\"  Average absolute variance: {avg_variance_pct:.1f}%\")\n",
    "    \n",
    "    # Distribution of variances\n",
    "    over_consumption = material_consumption_df[material_consumption_df['consumption_variance'] > 0]\n",
    "    under_consumption = material_consumption_df[material_consumption_df['consumption_variance'] < 0]\n",
    "    on_target = material_consumption_df[material_consumption_df['consumption_variance'] == 0]\n",
    "    \n",
    "    print(\"\\nConsumption Variance Distribution:\")\n",
    "    print(f\"  Over-consumption: {len(over_consumption)} records ({len(over_consumption)/len(material_consumption_df)*100:.1f}%)\")\n",
    "    print(f\"  Under-consumption: {len(under_consumption)} records ({len(under_consumption)/len(material_consumption_df)*100:.1f}%)\")\n",
    "    print(f\"  On-target: {len(on_target)} records ({len(on_target)/len(material_consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Time distribution\n",
    "    material_consumption_df['timestamp'] = pd.to_datetime(material_consumption_df['timestamp'])\n",
    "    \n",
    "    # Group by month\n",
    "    material_consumption_df['month'] = material_consumption_df['timestamp'].dt.to_period('M')\n",
    "    monthly_counts = material_consumption_df['month'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nMonthly Consumption Distribution:\")\n",
    "    for month, count in monthly_counts.items():\n",
    "        print(f\"  {month}: {count} consumption records\")\n",
    "    \n",
    "    # Unit distribution\n",
    "    unit_counts = material_consumption_df['unit'].value_counts()\n",
    "    print(\"\\nUnit of Measurement Distribution:\")\n",
    "    for unit, count in unit_counts.items():\n",
    "        print(f\"  {unit}: {count} records ({count/len(material_consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Material lot frequency analysis\n",
    "    lot_frequency = material_consumption_df['lot_id'].value_counts()\n",
    "    print(\"\\nMaterial Lot Usage Frequency:\")\n",
    "    print(f\"  Average usage per lot: {lot_frequency.mean():.1f} times\")\n",
    "    print(f\"  Max usage per lot: {lot_frequency.max()} times\")\n",
    "    print(f\"  Single-use lots: {(lot_frequency == 1).sum()} lots\")\n",
    "    print(f\"  Multi-use lots: {(lot_frequency > 1).sum()} lots\")\n",
    "    \n",
    "    # Equipment distribution\n",
    "    equipment_counts = material_consumption_df['equipment_id'].value_counts().head(10)\n",
    "    print(\"\\nTop Equipment Usage:\")\n",
    "    for equipment_id, count in equipment_counts.items():\n",
    "        print(f\"  {equipment_id}: {count} consumption records\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    material_lots_df = load_material_lots_data()\n",
    "    batches_df = load_batches_data()\n",
    "    work_orders_df = load_work_orders_data()\n",
    "    equipment_df = load_equipment_data()\n",
    "    batch_execution_df = load_batch_execution_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    \n",
    "    if material_lots_df is not None:\n",
    "        # Generate material consumption data\n",
    "        material_consumption_df = generate_material_consumption(\n",
    "            material_lots_df,\n",
    "            batches_df,\n",
    "            work_orders_df,\n",
    "            equipment_df,\n",
    "            batch_execution_df,\n",
    "            personnel_df,\n",
    "            num_consumptions=400,  # Generate 400 material consumption records\n",
    "            output_file=\"data/material_consumption.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if material_consumption_df is not None:\n",
    "            display_statistics(material_consumption_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample material consumption data (first 5 records):\")\n",
    "            print(material_consumption_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5467c",
   "metadata": {},
   "source": [
    "Quality Test Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2217ce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Products data file data/products.csv not found.\n",
      "Quality tests will be generated with synthetic product IDs.\n",
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Quality tests will be generated with synthetic inspector IDs.\n",
      "Generating synthetic product IDs...\n",
      "Generating synthetic inspector IDs...\n",
      "Successfully generated 500 quality test records.\n",
      "Data saved to data/quality_tests.csv\n",
      "Successfully generated 100 quality event records.\n",
      "Data saved to data/quality_events.csv\n",
      "\n",
      "Quality Tests Statistics:\n",
      "Total quality tests: 500\n",
      "\n",
      "Test Type Distribution:\n",
      "  Chemical Analysis: 97 (19.4%)\n",
      "  Visual Inspection: 83 (16.6%)\n",
      "  Physical Test: 80 (16.0%)\n",
      "  Functional Test: 47 (9.4%)\n",
      "  Dimensional Check: 44 (8.8%)\n",
      "  Microbiological: 42 (8.4%)\n",
      "  Release Testing: 33 (6.6%)\n",
      "  Stability Test: 28 (5.6%)\n",
      "  Identification Test: 25 (5.0%)\n",
      "  Impurity Test: 21 (4.2%)\n",
      "\n",
      "Test Result Distribution:\n",
      "  Pass: 474 (94.8%)\n",
      "  Fail: 26 (5.2%)\n",
      "\n",
      "Tests flagged for retest: 31 (6.2%)\n",
      "\n",
      "Failed Tests by Type:\n",
      "  Chemical Analysis: 10/97 (10.3% failure rate)\n",
      "  Physical Test: 4/80 (5.0% failure rate)\n",
      "  Visual Inspection: 3/83 (3.6% failure rate)\n",
      "  Stability Test: 2/28 (7.1% failure rate)\n",
      "  Dimensional Check: 2/44 (4.5% failure rate)\n",
      "  Release Testing: 2/33 (6.1% failure rate)\n",
      "  Impurity Test: 1/21 (4.8% failure rate)\n",
      "  Functional Test: 1/47 (2.1% failure rate)\n",
      "  Microbiological: 1/42 (2.4% failure rate)\n",
      "\n",
      "Test Associations:\n",
      "  Tests associated with lots: 331 (66.2%)\n",
      "  Tests associated with products: 332 (66.4%)\n",
      "  Tests associated with batches: 358 (71.6%)\n",
      "  Tests associated with work orders: 300 (60.0%)\n",
      "\n",
      "Monthly Test Distribution:\n",
      "  2025-01: 33 tests\n",
      "  2025-02: 90 tests\n",
      "  2025-03: 87 tests\n",
      "  2025-04: 77 tests\n",
      "  2025-05: 80 tests\n",
      "  2025-06: 93 tests\n",
      "  2025-07: 40 tests\n",
      "\n",
      "Quality Events Statistics:\n",
      "Total quality events: 100\n",
      "\n",
      "Event Type Distribution:\n",
      "  Deviation: 26 (26.0%)\n",
      "  Non-conformance: 23 (23.0%)\n",
      "  Quality Incident: 20 (20.0%)\n",
      "  OOS Result: 18 (18.0%)\n",
      "  Complaint: 13 (13.0%)\n",
      "\n",
      "Severity Distribution:\n",
      "  Severity 1: 38 (38.0%)\n",
      "  Severity 2: 30 (30.0%)\n",
      "  Severity 3: 18 (18.0%)\n",
      "  Severity 4: 10 (10.0%)\n",
      "  Severity 5: 4 (4.0%)\n",
      "\n",
      "Status Distribution:\n",
      "  Closed: 57 (57.0%)\n",
      "  Corrective Action: 19 (19.0%)\n",
      "  Canceled: 15 (15.0%)\n",
      "  Under Investigation: 5 (5.0%)\n",
      "  Open: 4 (4.0%)\n",
      "\n",
      "Root Cause Distribution (Closed Events):\n",
      "  Training Issue: 9 (15.8%)\n",
      "  Process Deviation: 8 (14.0%)\n",
      "  Unknown: 8 (14.0%)\n",
      "  Material Quality: 7 (12.3%)\n",
      "  Documentation Error: 6 (10.5%)\n",
      "  Human Error: 6 (10.5%)\n",
      "  Equipment Malfunction: 5 (8.8%)\n",
      "  Contamination: 4 (7.0%)\n",
      "  Supplier Quality: 3 (5.3%)\n",
      "  Environmental Factors: 1 (1.8%)\n",
      "\n",
      "Time to Close Statistics:\n",
      "  Average days to close: 37.3 days\n",
      "  Minimum days to close: 4 days\n",
      "  Maximum days to close: 84 days\n",
      "\n",
      "Sample quality tests data (first 5 records):\n",
      "         test_id            test_type          test_method sample_id  \\\n",
      "0  TEST-CDCC75F3    Chemical Analysis                   GC   S615049   \n",
      "1  TEST-283F071F       Stability Test  Long-term Stability   S407860   \n",
      "2  TEST-D3AE1363  Identification Test          IR Identity   S389115   \n",
      "3  TEST-2F07198F    Visual Inspection  Packaging Integrity   S793838   \n",
      "4  TEST-2988BB9D        Physical Test        Particle Size   S662372   \n",
      "\n",
      "      product_id        lot_id        batch_id work_order_id  \\\n",
      "0                 LOT-59AD909E  BATCH-950D50BA   WO-5C09AE49   \n",
      "1  PROD-897550AC  LOT-D56276C6  BATCH-F2C69CC6                 \n",
      "2                 LOT-AA46953A                   WO-EFE0DA32   \n",
      "3  PROD-93D68BAB                BATCH-A3F786D5                 \n",
      "4  PROD-CE192160                BATCH-CB8DC106                 \n",
      "\n",
      "            timestamp parameter_name  ...  specification_lower_limit  \\\n",
      "0 2025-01-17 21:43:58   Conductivity  ...                       50.0   \n",
      "1 2025-01-18 14:37:58        Potency  ...                       90.0   \n",
      "2 2025-01-19 03:05:58       Identity  ...                        1.0   \n",
      "3 2025-01-19 04:05:58  Label Quality  ...                        4.0   \n",
      "4 2025-01-19 15:19:58    Dissolution  ...                       70.0   \n",
      "\n",
      "   specification_upper_limit actual_value   unit test_result  \\\n",
      "0                      150.0        87.79  µS/cm        Pass   \n",
      "1                      110.0       102.20      %        Pass   \n",
      "2                                    1.00  match        Pass   \n",
      "3                                    5.09  score        Pass   \n",
      "4                                   89.50      %        Pass   \n",
      "\n",
      "  test_equipment_id     analyst_id retest_flag                         notes  \\\n",
      "0       EQ-7C644D85  PERS-FD77C935       False  Test completed successfully.   \n",
      "1       EQ-054211C2  PERS-B6FA04F8       False                                 \n",
      "2       EQ-1F51855E  PERS-E2E63EC1       False  Test completed successfully.   \n",
      "3       EQ-51EB1E7A  PERS-EAA3BB83       False                                 \n",
      "4       EQ-7C644D85  PERS-A9A9CFDE       False  Test completed successfully.   \n",
      "\n",
      "     month  \n",
      "0  2025-01  \n",
      "1  2025-01  \n",
      "2  2025-01  \n",
      "3  2025-01  \n",
      "4  2025-01  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Sample quality events data (first 5 records):\n",
      "      event_id        event_type  severity  \\\n",
      "0  QE-EF3BB182        OOS Result         1   \n",
      "1  QE-C20DC50B   Non-conformance         2   \n",
      "2  QE-7D819584  Quality Incident         1   \n",
      "3  QE-815EE111         Deviation         1   \n",
      "4  QE-4F83FAF2         Deviation         2   \n",
      "\n",
      "                                         description detection_date  \\\n",
      "0  OOS Result for Density in Physical Test test. ...     2025-06-17   \n",
      "1  Non-conformance for Residual Solvent in Chemic...     2025-03-10   \n",
      "2  Quality Incident for Uniformity in Release Tes...     2025-01-25   \n",
      "3  Deviation for Assay Content in Chemical Analys...     2025-04-08   \n",
      "4  Deviation for Density in Physical Test test. A...     2025-05-17   \n",
      "\n",
      "              status     product_id        lot_id        batch_id  \\\n",
      "0  Corrective Action                 LOT-8AC63C83                   \n",
      "1             Closed  PROD-8C0A1B3C                BATCH-0CAB185E   \n",
      "2           Canceled  PROD-6D347DE2  LOT-9763DACF  BATCH-A0753853   \n",
      "3             Closed                 LOT-AAE4BF8F                   \n",
      "4             Closed                 LOT-8AC63C83                   \n",
      "\n",
      "  equipment_id        area_id    detected_by       assignee  \\\n",
      "0  EQ-DA1AB602                 PERS-2D761961  PERS-C3AF7C79   \n",
      "1  EQ-00022464                 PERS-35E2E4D6  PERS-FD77C935   \n",
      "2  EQ-EB008965  AREA-2583963E  PERS-C3AF7C79  PERS-2B7377CE   \n",
      "3  EQ-EB008965  AREA-4C5B89FD  PERS-2B7377CE  PERS-EAA3BB83   \n",
      "4  EQ-DA1AB602                 PERS-2D761961  PERS-68E7EF79   \n",
      "\n",
      "              root_cause      corrective_action closure_date  \n",
      "0         Training Issue  Equipment Maintenance               \n",
      "1  Equipment Malfunction     Additional Testing   2025-05-15  \n",
      "2                                                             \n",
      "3         Training Issue    Enhanced Monitoring   2025-06-01  \n",
      "4       Material Quality    Enhanced Monitoring   2025-06-25  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114075/3498726793.py:824: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  closed_events['detection_date'] = pd.to_datetime(closed_events['detection_date'])\n",
      "/tmp/ipykernel_114075/3498726793.py:825: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  closed_events['closure_date'] = pd.to_datetime(closed_events['closure_date'])\n",
      "/tmp/ipykernel_114075/3498726793.py:828: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  closed_events['days_to_close'] = (closed_events['closure_date'] - closed_events['detection_date']).dt.days\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_material_lots_data(material_lots_file=\"data/material_lots.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated material lots data\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_file: CSV file containing material lots data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the material lots data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(material_lots_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Material lots data file {material_lots_file} not found.\")\n",
    "        print(\"Please run the material lots data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_products_data(products_file=\"data/products.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated products data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - products_file: CSV file containing products data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the products data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(products_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Products data file {products_file} not found.\")\n",
    "        print(\"Quality tests will be generated with synthetic product IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_batches_data(batches_file=\"data/batches.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated batches data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - batches_file: CSV file containing batches data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the batches data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(batches_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Batches data file {batches_file} not found.\")\n",
    "        print(\"Quality tests will be generated with synthetic batch IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_work_orders_data(work_orders_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated work orders data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_file: CSV file containing work orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the work orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(work_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Work orders data file {work_orders_file} not found.\")\n",
    "        print(\"Quality tests will be generated with synthetic work order IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Quality tests will be generated with synthetic equipment IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Quality tests will be generated with synthetic inspector IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_quality_tests(material_lots_df, products_df=None, batches_df=None, \n",
    "                          work_orders_df=None, equipment_df=None, personnel_df=None,\n",
    "                          num_tests=500, start_time=None, end_time=None,\n",
    "                          output_file=\"data/quality_tests.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the QualityTests table from ISA-95 Level 3.\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_df: DataFrame containing material lots data\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - batches_df: DataFrame containing batches data (optional)\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_tests: Number of quality test records to generate\n",
    "    - start_time: Start time for test dates (defaults to 180 days ago)\n",
    "    - end_time: End time for test dates (defaults to now)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated quality tests data\n",
    "    \"\"\"\n",
    "    if material_lots_df is None or len(material_lots_df) == 0:\n",
    "        print(\"Error: No material lots data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=180)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Get product IDs if available, otherwise generate synthetic ones\n",
    "    if products_df is not None and len(products_df) > 0:\n",
    "        product_ids = products_df['product_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic product IDs...\")\n",
    "        product_ids = [f\"PROD-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    \n",
    "    # Get batch IDs if available, otherwise generate synthetic ones\n",
    "    if batches_df is not None and len(batches_df) > 0:\n",
    "        batch_ids = batches_df['batch_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic batch IDs...\")\n",
    "        batch_ids = [f\"BATCH-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    \n",
    "    # Get work order IDs if available, otherwise generate synthetic ones\n",
    "    if work_orders_df is not None and len(work_orders_df) > 0:\n",
    "        work_order_ids = work_orders_df['work_order_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    \n",
    "    # Get equipment IDs (specifically for test equipment) if available, otherwise generate synthetic ones\n",
    "    if equipment_df is not None and len(equipment_df) > 0:\n",
    "        test_equipment_ids = equipment_df.sample(min(10, len(equipment_df)))['equipment_id'].tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic test equipment IDs...\")\n",
    "        test_equipment_ids = [f\"EQ-{uuid.uuid4().hex[:8].upper()}\" for _ in range(10)]\n",
    "    \n",
    "    # Get inspector IDs if available, otherwise generate synthetic ones\n",
    "    if personnel_df is not None and len(personnel_df) > 0:\n",
    "        inspector_ids = personnel_df.sample(min(20, len(personnel_df)))['personnel_id'].tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic inspector IDs...\")\n",
    "        inspector_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(15)]\n",
    "    \n",
    "    # Define test types and their probabilities\n",
    "    test_types = {\n",
    "        \"Chemical Analysis\": 0.2,\n",
    "        \"Physical Test\": 0.15,\n",
    "        \"Visual Inspection\": 0.15,\n",
    "        \"Microbiological\": 0.1,\n",
    "        \"Stability Test\": 0.05,\n",
    "        \"Dimensional Check\": 0.1,\n",
    "        \"Functional Test\": 0.1,\n",
    "        \"Identification Test\": 0.05,\n",
    "        \"Impurity Test\": 0.05,\n",
    "        \"Release Testing\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Define test methods for each test type\n",
    "    test_methods = {\n",
    "        \"Chemical Analysis\": [\"HPLC\", \"GC\", \"MS\", \"IR Spectroscopy\", \"UV-Vis Spectroscopy\", \"Titration\", \"pH Measurement\", \"Conductivity\"],\n",
    "        \"Physical Test\": [\"Viscosity\", \"Density\", \"Particle Size\", \"Hardness\", \"Tensile Strength\", \"Melting Point\", \"Dissolution\", \"Friability\"],\n",
    "        \"Visual Inspection\": [\"Appearance\", \"Color\", \"Clarity\", \"Foreign Particles\", \"Visible Defects\", \"Packaging Integrity\"],\n",
    "        \"Microbiological\": [\"Total Plate Count\", \"Microbial Enumeration\", \"Sterility\", \"Endotoxin\", \"Bioburden\", \"Antimicrobial Effectiveness\"],\n",
    "        \"Stability Test\": [\"Accelerated Stability\", \"Long-term Stability\", \"Photostability\", \"Temperature Cycling\", \"Stress Testing\"],\n",
    "        \"Dimensional Check\": [\"Height\", \"Width\", \"Diameter\", \"Thickness\", \"Weight\", \"Volume\", \"Surface Area\"],\n",
    "        \"Functional Test\": [\"Performance Test\", \"Operational Check\", \"Power Consumption\", \"Response Time\", \"Load Test\", \"Durability\"],\n",
    "        \"Identification Test\": [\"IR Identity\", \"Chemical Identity\", \"Chromatographic Identity\", \"Spectral Comparison\"],\n",
    "        \"Impurity Test\": [\"Related Substances\", \"Residual Solvents\", \"Heavy Metals\", \"Organic Impurities\", \"Inorganic Impurities\"],\n",
    "        \"Release Testing\": [\"Final Product Test\", \"Certificate of Analysis\", \"Conformance Test\", \"Quality Control Release\"]\n",
    "    }\n",
    "    \n",
    "    # Define test parameters and their specifications for each test type\n",
    "    test_parameters = {\n",
    "        \"Chemical Analysis\": {\n",
    "            \"Assay Content\": {\"unit\": \"%\", \"target\": 100.0, \"range\": 5.0},\n",
    "            \"pH\": {\"unit\": \"pH units\", \"target\": 7.0, \"range\": 1.0},\n",
    "            \"Residual Solvent\": {\"unit\": \"ppm\", \"target\": 0.0, \"range\": 1000.0, \"upper_only\": True},\n",
    "            \"Active Ingredient\": {\"unit\": \"mg/mL\", \"target\": 10.0, \"range\": 1.0},\n",
    "            \"Conductivity\": {\"unit\": \"µS/cm\", \"target\": 100.0, \"range\": 50.0}\n",
    "        },\n",
    "        \"Physical Test\": {\n",
    "            \"Viscosity\": {\"unit\": \"cP\", \"target\": 1000.0, \"range\": 200.0},\n",
    "            \"Density\": {\"unit\": \"g/cm³\", \"target\": 1.05, \"range\": 0.1},\n",
    "            \"Particle Size\": {\"unit\": \"µm\", \"target\": 50.0, \"range\": 10.0},\n",
    "            \"Hardness\": {\"unit\": \"kP\", \"target\": 12.0, \"range\": 3.0},\n",
    "            \"Dissolution\": {\"unit\": \"%\", \"target\": 85.0, \"range\": 15.0, \"lower_only\": True}\n",
    "        },\n",
    "        \"Visual Inspection\": {\n",
    "            \"Appearance\": {\"unit\": \"score\", \"target\": 5.0, \"range\": 1.0, \"lower_only\": True},\n",
    "            \"Color Conformity\": {\"unit\": \"score\", \"target\": 5.0, \"range\": 1.0, \"lower_only\": True},\n",
    "            \"Visible Defects\": {\"unit\": \"count\", \"target\": 0.0, \"range\": 3.0, \"upper_only\": True},\n",
    "            \"Label Quality\": {\"unit\": \"score\", \"target\": 5.0, \"range\": 1.0, \"lower_only\": True}\n",
    "        },\n",
    "        \"Microbiological\": {\n",
    "            \"Total Aerobic Count\": {\"unit\": \"CFU/g\", \"target\": 0.0, \"range\": 1000.0, \"upper_only\": True},\n",
    "            \"E. coli\": {\"unit\": \"CFU/g\", \"target\": 0.0, \"range\": 0.0, \"upper_only\": True}, # Zero tolerance\n",
    "            \"Yeast & Mold\": {\"unit\": \"CFU/g\", \"target\": 0.0, \"range\": 100.0, \"upper_only\": True},\n",
    "            \"Salmonella\": {\"unit\": \"presence\", \"target\": 0.0, \"range\": 0.0, \"upper_only\": True} # Pass/fail\n",
    "        },\n",
    "        \"Stability Test\": {\n",
    "            \"Potency\": {\"unit\": \"%\", \"target\": 100.0, \"range\": 10.0},\n",
    "            \"Degradation Products\": {\"unit\": \"%\", \"target\": 0.0, \"range\": 2.0, \"upper_only\": True},\n",
    "            \"pH Change\": {\"unit\": \"pH units\", \"target\": 0.0, \"range\": 1.0, \"absolute\": True}\n",
    "        },\n",
    "        \"Dimensional Check\": {\n",
    "            \"Length\": {\"unit\": \"mm\", \"target\": 100.0, \"range\": 1.0},\n",
    "            \"Width\": {\"unit\": \"mm\", \"target\": 50.0, \"range\": 0.5},\n",
    "            \"Height\": {\"unit\": \"mm\", \"target\": 25.0, \"range\": 0.5},\n",
    "            \"Weight\": {\"unit\": \"g\", \"target\": 500.0, \"range\": 15.0}\n",
    "        },\n",
    "        \"Functional Test\": {\n",
    "            \"Operation Time\": {\"unit\": \"seconds\", \"target\": 60.0, \"range\": 10.0},\n",
    "            \"Power Output\": {\"unit\": \"W\", \"target\": 1000.0, \"range\": 100.0},\n",
    "            \"Efficiency\": {\"unit\": \"%\", \"target\": 95.0, \"range\": 5.0},\n",
    "            \"Response Time\": {\"unit\": \"ms\", \"target\": 100.0, \"range\": 20.0}\n",
    "        },\n",
    "        \"Identification Test\": {\n",
    "            \"Identity\": {\"unit\": \"match\", \"target\": 1.0, \"range\": 0.0, \"lower_only\": True}, # Pass/fail\n",
    "            \"Purity\": {\"unit\": \"%\", \"target\": 99.0, \"range\": 1.0, \"lower_only\": True}\n",
    "        },\n",
    "        \"Impurity Test\": {\n",
    "            \"Individual Impurity\": {\"unit\": \"%\", \"target\": 0.0, \"range\": 0.5, \"upper_only\": True},\n",
    "            \"Total Impurities\": {\"unit\": \"%\", \"target\": 0.0, \"range\": 2.0, \"upper_only\": True},\n",
    "            \"Heavy Metals\": {\"unit\": \"ppm\", \"target\": 0.0, \"range\": 10.0, \"upper_only\": True}\n",
    "        },\n",
    "        \"Release Testing\": {\n",
    "            \"Final Potency\": {\"unit\": \"%\", \"target\": 100.0, \"range\": 5.0},\n",
    "            \"Final Impurities\": {\"unit\": \"%\", \"target\": 0.0, \"range\": 2.0, \"upper_only\": True},\n",
    "            \"Uniformity\": {\"unit\": \"%RSD\", \"target\": 0.0, \"range\": 5.0, \"upper_only\": True}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"test_id\": [f\"TEST-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_tests)],\n",
    "        \"test_type\": [],\n",
    "        \"test_method\": [],\n",
    "        \"sample_id\": [],\n",
    "        \"product_id\": [],\n",
    "        \"lot_id\": [],\n",
    "        \"batch_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"timestamp\": [],\n",
    "        \"parameter_name\": [],\n",
    "        \"specification_target\": [],\n",
    "        \"specification_lower_limit\": [],\n",
    "        \"specification_upper_limit\": [],\n",
    "        \"actual_value\": [],\n",
    "        \"unit\": [],\n",
    "        \"test_result\": [],\n",
    "        \"test_equipment_id\": [],\n",
    "        \"analyst_id\": [],\n",
    "        \"retest_flag\": [],\n",
    "        \"notes\": []\n",
    "    }\n",
    "    \n",
    "    # Generate timestamps distributed over the time range\n",
    "    time_range_minutes = int((end_time - start_time).total_seconds() / 60)\n",
    "    timestamps = []\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        random_minutes = random.randint(0, time_range_minutes)\n",
    "        timestamp = start_time + timedelta(minutes=random_minutes)\n",
    "        timestamps.append(timestamp)\n",
    "    \n",
    "    # Sort timestamps (older to newer)\n",
    "    timestamps.sort()\n",
    "    \n",
    "    # Generate data for each test record\n",
    "    for i in range(num_tests):\n",
    "        # Select test type (weighted random)\n",
    "        test_type = random.choices(list(test_types.keys()), weights=list(test_types.values()))[0]\n",
    "        data[\"test_type\"].append(test_type)\n",
    "        \n",
    "        # Select test method for this type\n",
    "        test_method = random.choice(test_methods[test_type])\n",
    "        data[\"test_method\"].append(test_method)\n",
    "        \n",
    "        # Generate sample ID\n",
    "        data[\"sample_id\"].append(f\"S{random.randint(100000, 999999)}\")\n",
    "        \n",
    "        # Decide what's being tested: material lot, product, or both\n",
    "        test_target = random.choice([\"lot\", \"product\", \"both\"])\n",
    "        \n",
    "        if test_target in [\"lot\", \"both\"]:\n",
    "            # Test is for a material lot\n",
    "            lot_id = random.choice(material_lots_df['lot_id'].tolist())\n",
    "            data[\"lot_id\"].append(lot_id)\n",
    "            \n",
    "            # Get associated material info if possible\n",
    "            try:\n",
    "                lot_info = material_lots_df[material_lots_df['lot_id'] == lot_id].iloc[0]\n",
    "                material_id = lot_info.get('material_id', \"\")\n",
    "            except (IndexError, KeyError):\n",
    "                material_id = \"\"\n",
    "        else:\n",
    "            data[\"lot_id\"].append(\"\")\n",
    "            material_id = \"\"\n",
    "            \n",
    "        if test_target in [\"product\", \"both\"]:\n",
    "            # Test is for a product\n",
    "            data[\"product_id\"].append(random.choice(product_ids))\n",
    "        else:\n",
    "            data[\"product_id\"].append(\"\")\n",
    "        \n",
    "        # Associate with batch and work order\n",
    "        if random.random() < 0.7:  # 70% associated with batch\n",
    "            data[\"batch_id\"].append(random.choice(batch_ids))\n",
    "        else:\n",
    "            data[\"batch_id\"].append(\"\")\n",
    "            \n",
    "        if random.random() < 0.6:  # 60% associated with work order\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        else:\n",
    "            data[\"work_order_id\"].append(\"\")\n",
    "        \n",
    "        # Set timestamp\n",
    "        data[\"timestamp\"].append(timestamps[i].strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Select test parameter for this type\n",
    "        parameter_name = random.choice(list(test_parameters[test_type].keys()))\n",
    "        data[\"parameter_name\"].append(parameter_name)\n",
    "        \n",
    "        # Get parameter specs\n",
    "        param_specs = test_parameters[test_type][parameter_name]\n",
    "        target_value = param_specs[\"target\"]\n",
    "        range_value = param_specs[\"range\"]\n",
    "        unit = param_specs[\"unit\"]\n",
    "        \n",
    "        # Set specification limits\n",
    "        upper_only = param_specs.get(\"upper_only\", False)\n",
    "        lower_only = param_specs.get(\"lower_only\", False)\n",
    "        absolute = param_specs.get(\"absolute\", False)\n",
    "        \n",
    "        if upper_only:\n",
    "            # Only upper limit (max allowed)\n",
    "            lower_limit = \"\"\n",
    "            upper_limit = target_value + range_value\n",
    "        elif lower_only:\n",
    "            # Only lower limit (min required)\n",
    "            lower_limit = target_value - range_value\n",
    "            upper_limit = \"\"\n",
    "        else:\n",
    "            # Both limits\n",
    "            lower_limit = target_value - range_value\n",
    "            upper_limit = target_value + range_value\n",
    "        \n",
    "        # Special handling for zero or near-zero targets\n",
    "        if abs(target_value) < 0.001 and not absolute:\n",
    "            lower_limit = 0.0\n",
    "        \n",
    "        data[\"specification_target\"].append(target_value)\n",
    "        data[\"specification_lower_limit\"].append(lower_limit)\n",
    "        data[\"specification_upper_limit\"].append(upper_limit)\n",
    "        data[\"unit\"].append(unit)\n",
    "        \n",
    "        # Generate actual test value (normally distributed around target with occasional outliers)\n",
    "        if random.random() < 0.05:  # 5% chance of outlier\n",
    "            # Generate outlier value\n",
    "            if upper_only:\n",
    "                # For upper-only specs, generate occasional high outliers\n",
    "                actual_value = target_value + (range_value * random.uniform(1.1, 2.0))\n",
    "            elif lower_only:\n",
    "                # For lower-only specs, generate occasional low outliers\n",
    "                actual_value = target_value - (range_value * random.uniform(1.1, 2.0))\n",
    "            else:\n",
    "                # For two-sided specs, generate outliers on either side\n",
    "                if random.random() < 0.5:\n",
    "                    actual_value = target_value + (range_value * random.uniform(1.1, 1.5))\n",
    "                else:\n",
    "                    actual_value = target_value - (range_value * random.uniform(1.1, 1.5))\n",
    "        else:\n",
    "            # Generate normal value (normally distributed around target)\n",
    "            std_dev = range_value / 3.0  # 3-sigma rule: most values within spec\n",
    "            actual_value = random.normalvariate(target_value, std_dev)\n",
    "        \n",
    "        # Handle special cases\n",
    "        if unit == \"presence\" or unit == \"match\":\n",
    "            # These are pass/fail tests, actual value should be 0 or 1\n",
    "            if random.random() < 0.95:  # 95% pass rate\n",
    "                actual_value = 1 if unit == \"match\" else 0  # Match=1 is good, Presence=0 is good\n",
    "            else:\n",
    "                actual_value = 0 if unit == \"match\" else 1\n",
    "        \n",
    "        # Round actual value based on the unit precision\n",
    "        if \"%\" in unit:\n",
    "            actual_value = round(actual_value, 1)  # One decimal for percentages\n",
    "        elif unit in [\"g/cm³\", \"pH units\"]:\n",
    "            actual_value = round(actual_value, 2)  # Two decimals for density, pH\n",
    "        elif unit in [\"µm\", \"mg/mL\", \"ppm\"]:\n",
    "            actual_value = round(actual_value, 1)  # One decimal for small measurements\n",
    "        else:\n",
    "            actual_value = round(actual_value, 2)  # Default precision\n",
    "        \n",
    "        data[\"actual_value\"].append(actual_value)\n",
    "        \n",
    "        # Determine test result\n",
    "        if upper_only and upper_limit != \"\":\n",
    "            test_result = \"Pass\" if actual_value <= upper_limit else \"Fail\"\n",
    "        elif lower_only and lower_limit != \"\":\n",
    "            test_result = \"Pass\" if actual_value >= lower_limit else \"Fail\"\n",
    "        elif upper_limit != \"\" and lower_limit != \"\":\n",
    "            test_result = \"Pass\" if lower_limit <= actual_value <= upper_limit else \"Fail\"\n",
    "        else:\n",
    "            # Default for unusual cases\n",
    "            test_result = \"Pass\" if random.random() < 0.95 else \"Fail\"\n",
    "        \n",
    "        data[\"test_result\"].append(test_result)\n",
    "        \n",
    "        # Assign test equipment\n",
    "        data[\"test_equipment_id\"].append(random.choice(test_equipment_ids))\n",
    "        \n",
    "        # Assign analyst/inspector\n",
    "        data[\"analyst_id\"].append(random.choice(inspector_ids))\n",
    "        \n",
    "        # Set retest flag (more likely for failed tests)\n",
    "        if test_result == \"Fail\":\n",
    "            retest_flag = random.random() < 0.7  # 70% of failures get retested\n",
    "        else:\n",
    "            retest_flag = random.random() < 0.05  # 5% of passes get retested\n",
    "        \n",
    "        data[\"retest_flag\"].append(retest_flag)\n",
    "        \n",
    "        # Generate notes (more detailed for failures)\n",
    "        if test_result == \"Fail\":\n",
    "            notes_options = [\n",
    "                f\"Out of specification. Retest authorized.\",\n",
    "                f\"Value exceeds {parameter_name} limit. Investigation required.\",\n",
    "                f\"Failed {test_method} test. Checking calibration.\",\n",
    "                f\"OOS result confirmed on duplicate test.\",\n",
    "                f\"Deviation reported, sample under investigation.\"\n",
    "            ]\n",
    "        else:\n",
    "            notes_options = [\n",
    "                f\"Result within specification.\",\n",
    "                f\"Test completed successfully.\",\n",
    "                f\"Verified against standard.\",\n",
    "                f\"\",  # Empty note for many passing tests\n",
    "                f\"\"\n",
    "            ]\n",
    "        \n",
    "        data[\"notes\"].append(random.choice(notes_options))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} quality test records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_quality_events(quality_tests_df, material_lots_df=None, batches_df=None, \n",
    "                          equipment_df=None, process_areas_df=None, personnel_df=None,\n",
    "                          num_events=100, output_file=\"data/quality_events.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the QualityEvents table based on quality tests.\n",
    "    \n",
    "    Parameters:\n",
    "    - quality_tests_df: DataFrame containing quality tests data\n",
    "    - material_lots_df: DataFrame containing material lots data (optional)\n",
    "    - batches_df: DataFrame containing batches data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    - process_areas_df: DataFrame containing process areas data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_events: Number of quality event records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated quality events data\n",
    "    \"\"\"\n",
    "    if quality_tests_df is None or len(quality_tests_df) == 0:\n",
    "        print(\"Error: No quality tests data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Get failed tests as potential sources for quality events\n",
    "    failed_tests = quality_tests_df[quality_tests_df['test_result'] == \"Fail\"]\n",
    "    \n",
    "    if len(failed_tests) == 0:\n",
    "        print(\"Warning: No failed quality tests found. Generating generic quality events.\")\n",
    "        test_based_events = False\n",
    "    else:\n",
    "        test_based_events = True\n",
    "    \n",
    "    # Get material lot IDs\n",
    "    if material_lots_df is not None and len(material_lots_df) > 0:\n",
    "        lot_ids = material_lots_df['lot_id'].unique().tolist()\n",
    "    else:\n",
    "        lot_ids = quality_tests_df['lot_id'].unique().tolist()\n",
    "        lot_ids = [lot for lot in lot_ids if lot]  # Remove empty strings\n",
    "    \n",
    "    # Get batch IDs\n",
    "    if batches_df is not None and len(batches_df) > 0:\n",
    "        batch_ids = batches_df['batch_id'].unique().tolist()\n",
    "    else:\n",
    "        batch_ids = quality_tests_df['batch_id'].unique().tolist()\n",
    "        batch_ids = [batch for batch in batch_ids if batch]  # Remove empty strings\n",
    "    \n",
    "    # Get equipment IDs\n",
    "    if equipment_df is not None and len(equipment_df) > 0:\n",
    "        equipment_ids = equipment_df['equipment_id'].unique().tolist()\n",
    "    else:\n",
    "        equipment_ids = quality_tests_df['test_equipment_id'].unique().tolist()\n",
    "    \n",
    "    # Get process area IDs if available, otherwise generate synthetic ones\n",
    "    if process_areas_df is not None and len(process_areas_df) > 0:\n",
    "        area_ids = process_areas_df['area_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic process area IDs...\")\n",
    "        area_ids = [f\"AREA-{uuid.uuid4().hex[:8].upper()}\" for _ in range(10)]\n",
    "    \n",
    "    # Get personnel IDs\n",
    "    if personnel_df is not None and len(personnel_df) > 0:\n",
    "        personnel_ids = personnel_df['personnel_id'].unique().tolist()\n",
    "    else:\n",
    "        personnel_ids = quality_tests_df['analyst_id'].unique().tolist()\n",
    "    \n",
    "    # Define event types and their probabilities\n",
    "    event_types = {\n",
    "        \"Deviation\": 0.3,\n",
    "        \"Non-conformance\": 0.25,\n",
    "        \"Quality Incident\": 0.2,\n",
    "        \"OOS Result\": 0.15,\n",
    "        \"Complaint\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Define severity levels\n",
    "    severity_levels = [1, 2, 3, 4, 5]  # 1 = minor, 5 = critical\n",
    "    severity_weights = [0.3, 0.3, 0.2, 0.15, 0.05]  # Most events are lower severity\n",
    "    \n",
    "    # Define status options\n",
    "    event_statuses = [\"Open\", \"Under Investigation\", \"Corrective Action\", \"Closed\", \"Canceled\"]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"event_id\": [f\"QE-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_events)],\n",
    "        \"event_type\": [],\n",
    "        \"severity\": [],\n",
    "        \"description\": [],\n",
    "        \"detection_date\": [],\n",
    "        \"status\": [],\n",
    "        \"product_id\": [],\n",
    "        \"lot_id\": [],\n",
    "        \"batch_id\": [],\n",
    "        \"equipment_id\": [],\n",
    "        \"area_id\": [],\n",
    "        \"detected_by\": [],\n",
    "        \"assignee\": [],\n",
    "        \"root_cause\": [],\n",
    "        \"corrective_action\": [],\n",
    "        \"closure_date\": []\n",
    "    }\n",
    "    \n",
    "    # Generate a timeframe for events (past 180 days)\n",
    "    start_time = datetime.now() - timedelta(days=180)\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Root cause categories\n",
    "    root_causes = [\n",
    "        \"Process Deviation\", \"Equipment Malfunction\", \"Human Error\", \"Material Quality\", \n",
    "        \"Environmental Factors\", \"Contamination\", \"Documentation Error\", \"Training Issue\",\n",
    "        \"Supplier Quality\", \"Unknown\"\n",
    "    ]\n",
    "    \n",
    "    # Corrective action templates\n",
    "    corrective_actions = [\n",
    "        \"Retrain Personnel\", \"Update Procedure\", \"Equipment Maintenance\", \"Material Replacement\",\n",
    "        \"Process Modification\", \"Enhanced Monitoring\", \"Supplier Audit\", \"CAPA Implementation\",\n",
    "        \"Preventive Maintenance\", \"Additional Testing\"\n",
    "    ]\n",
    "    \n",
    "    # Generate data for each quality event\n",
    "    for i in range(num_events):\n",
    "        # Select event type (weighted random)\n",
    "        event_type = random.choices(list(event_types.keys()), weights=list(event_types.values()))[0]\n",
    "        data[\"event_type\"].append(event_type)\n",
    "        \n",
    "        # Set severity (weighted random)\n",
    "        severity = random.choices(severity_levels, weights=severity_weights)[0]\n",
    "        data[\"severity\"].append(severity)\n",
    "        \n",
    "        # Determine if event is based on failed test\n",
    "        if test_based_events and random.random() < 0.7:  # 70% of events based on failed tests\n",
    "            # Select a random failed test\n",
    "            test = failed_tests.sample(1).iloc[0]\n",
    "            \n",
    "            # Use test information\n",
    "            test_id = test['test_id']\n",
    "            parameter = test['parameter_name']\n",
    "            test_type = test['test_type']\n",
    "            actual_value = test['actual_value']\n",
    "            target_value = test['specification_target']\n",
    "            unit = test['unit']\n",
    "            \n",
    "            # Create description based on test\n",
    "            description = f\"{event_type} for {parameter} in {test_type} test. Actual: {actual_value} {unit}, Target: {target_value} {unit}. Test ID: {test_id}\"\n",
    "            \n",
    "            # Link to the same entities\n",
    "            product_id = test['product_id']\n",
    "            lot_id = test['lot_id']\n",
    "            batch_id = test['batch_id']\n",
    "            equipment_id = test['test_equipment_id']\n",
    "            detected_by = test['analyst_id']\n",
    "            \n",
    "        else:\n",
    "            # Generate generic quality event\n",
    "            templates = [\n",
    "                f\"{event_type}: {random.choice(['High', 'Low', 'Out of range', 'Unexpected'])} {random.choice(['viscosity', 'content', 'weight', 'appearance', 'dissolution'])} result\",\n",
    "                f\"{event_type}: {random.choice(['Foreign material', 'Contamination', 'Incorrect label', 'Missing component', 'Wrong color'])} detected\",\n",
    "                f\"{event_type}: {random.choice(['Process parameter', 'Equipment', 'Material', 'Documentation'])} {random.choice(['issue', 'failure', 'deviation', 'error'])}\"\n",
    "            ]\n",
    "            description = random.choice(templates)\n",
    "            \n",
    "            # Random associations\n",
    "            product_id = random.choice(quality_tests_df['product_id'].unique().tolist()) if random.random() < 0.7 else \"\"\n",
    "            lot_id = random.choice(lot_ids) if lot_ids and random.random() < 0.8 else \"\"\n",
    "            batch_id = random.choice(batch_ids) if batch_ids and random.random() < 0.7 else \"\"\n",
    "            equipment_id = random.choice(equipment_ids) if random.random() < 0.6 else \"\"\n",
    "            detected_by = random.choice(personnel_ids)\n",
    "        \n",
    "        data[\"description\"].append(description)\n",
    "        data[\"product_id\"].append(product_id)\n",
    "        data[\"lot_id\"].append(lot_id)\n",
    "        data[\"batch_id\"].append(batch_id)\n",
    "        data[\"equipment_id\"].append(equipment_id)\n",
    "        data[\"detected_by\"].append(detected_by)\n",
    "        \n",
    "        # Generate detection date\n",
    "        detection_date = start_time + timedelta(days=random.randint(0, 180))\n",
    "        data[\"detection_date\"].append(detection_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Assign process area\n",
    "        data[\"area_id\"].append(random.choice(area_ids) if random.random() < 0.8 else \"\")\n",
    "        \n",
    "        # Assign different person as assignee\n",
    "        available_assignees = [p for p in personnel_ids if p != detected_by]\n",
    "        assignee = random.choice(available_assignees) if available_assignees else random.choice(personnel_ids)\n",
    "        data[\"assignee\"].append(assignee)\n",
    "        \n",
    "        # Determine status (time-dependent)\n",
    "        days_since_detection = (datetime.now() - detection_date).days\n",
    "        \n",
    "        if days_since_detection < 7:\n",
    "            # Recent events are typically still open\n",
    "            status = random.choice([\"Open\", \"Under Investigation\"])\n",
    "        elif days_since_detection < 30:\n",
    "            # Medium-term events are in progress\n",
    "            status = random.choice([\"Under Investigation\", \"Corrective Action\", \"Open\"])\n",
    "        else:\n",
    "            # Older events are likely closed\n",
    "            status = random.choice([\"Closed\", \"Closed\", \"Closed\", \"Corrective Action\", \"Canceled\"])\n",
    "        \n",
    "        data[\"status\"].append(status)\n",
    "        \n",
    "        # Set root cause and corrective action (only for investigated/closed events)\n",
    "        if status in [\"Corrective Action\", \"Closed\"]:\n",
    "            data[\"root_cause\"].append(random.choice(root_causes))\n",
    "            data[\"corrective_action\"].append(random.choice(corrective_actions))\n",
    "        else:\n",
    "            data[\"root_cause\"].append(\"\")\n",
    "            data[\"corrective_action\"].append(\"\")\n",
    "        \n",
    "        # Set closure date (only for closed events)\n",
    "        if status == \"Closed\":\n",
    "            # Closure date is after detection date\n",
    "            min_closure_delay = 3  # Minimum 3 days to close\n",
    "            max_closure_delay = min(90, days_since_detection)  # Up to 90 days or available time\n",
    "            closure_days = random.randint(min_closure_delay, max(min_closure_delay, max_closure_delay))\n",
    "            closure_date = detection_date + timedelta(days=closure_days)\n",
    "            data[\"closure_date\"].append(closure_date.strftime(\"%Y-%m-%d\"))\n",
    "        else:\n",
    "            data[\"closure_date\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} quality event records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(quality_tests_df, quality_events_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated quality tests and events data\n",
    "    \n",
    "    Parameters:\n",
    "    - quality_tests_df: DataFrame containing quality tests data\n",
    "    - quality_events_df: DataFrame containing quality events data (optional)\n",
    "    \"\"\"\n",
    "    if quality_tests_df is None or len(quality_tests_df) == 0:\n",
    "        print(\"No quality tests data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nQuality Tests Statistics:\")\n",
    "    print(f\"Total quality tests: {len(quality_tests_df)}\")\n",
    "    \n",
    "    # Test type distribution\n",
    "    print(\"\\nTest Type Distribution:\")\n",
    "    type_counts = quality_tests_df['test_type'].value_counts()\n",
    "    for test_type, count in type_counts.items():\n",
    "        print(f\"  {test_type}: {count} ({count/len(quality_tests_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Test result distribution\n",
    "    print(\"\\nTest Result Distribution:\")\n",
    "    result_counts = quality_tests_df['test_result'].value_counts()\n",
    "    for result, count in result_counts.items():\n",
    "        print(f\"  {result}: {count} ({count/len(quality_tests_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Retest flag distribution\n",
    "    retest_count = quality_tests_df['retest_flag'].sum()\n",
    "    print(f\"\\nTests flagged for retest: {retest_count} ({retest_count/len(quality_tests_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Failed test distribution by type\n",
    "    failed_tests = quality_tests_df[quality_tests_df['test_result'] == \"Fail\"]\n",
    "    if len(failed_tests) > 0:\n",
    "        print(\"\\nFailed Tests by Type:\")\n",
    "        failed_type_counts = failed_tests['test_type'].value_counts()\n",
    "        for test_type, count in failed_type_counts.items():\n",
    "            type_total = type_counts[test_type]\n",
    "            print(f\"  {test_type}: {count}/{type_total} ({count/type_total*100:.1f}% failure rate)\")\n",
    "    \n",
    "    # Connections to other entities\n",
    "    lot_count = quality_tests_df['lot_id'].apply(lambda x: x != \"\").sum()\n",
    "    product_count = quality_tests_df['product_id'].apply(lambda x: x != \"\").sum()\n",
    "    batch_count = quality_tests_df['batch_id'].apply(lambda x: x != \"\").sum()\n",
    "    wo_count = quality_tests_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "    \n",
    "    print(\"\\nTest Associations:\")\n",
    "    print(f\"  Tests associated with lots: {lot_count} ({lot_count/len(quality_tests_df)*100:.1f}%)\")\n",
    "    print(f\"  Tests associated with products: {product_count} ({product_count/len(quality_tests_df)*100:.1f}%)\")\n",
    "    print(f\"  Tests associated with batches: {batch_count} ({batch_count/len(quality_tests_df)*100:.1f}%)\")\n",
    "    print(f\"  Tests associated with work orders: {wo_count} ({wo_count/len(quality_tests_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Time distribution\n",
    "    quality_tests_df['timestamp'] = pd.to_datetime(quality_tests_df['timestamp'])\n",
    "    \n",
    "    # Group by month\n",
    "    quality_tests_df['month'] = quality_tests_df['timestamp'].dt.to_period('M')\n",
    "    monthly_counts = quality_tests_df['month'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nMonthly Test Distribution:\")\n",
    "    for month, count in monthly_counts.items():\n",
    "        print(f\"  {month}: {count} tests\")\n",
    "    \n",
    "    # Quality events statistics (if available)\n",
    "    if quality_events_df is not None and len(quality_events_df) > 0:\n",
    "        print(\"\\nQuality Events Statistics:\")\n",
    "        print(f\"Total quality events: {len(quality_events_df)}\")\n",
    "        \n",
    "        # Event type distribution\n",
    "        print(\"\\nEvent Type Distribution:\")\n",
    "        event_type_counts = quality_events_df['event_type'].value_counts()\n",
    "        for event_type, count in event_type_counts.items():\n",
    "            print(f\"  {event_type}: {count} ({count/len(quality_events_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Severity distribution\n",
    "        print(\"\\nSeverity Distribution:\")\n",
    "        severity_counts = quality_events_df['severity'].value_counts().sort_index()\n",
    "        for severity, count in severity_counts.items():\n",
    "            print(f\"  Severity {severity}: {count} ({count/len(quality_events_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Status distribution\n",
    "        print(\"\\nStatus Distribution:\")\n",
    "        status_counts = quality_events_df['status'].value_counts()\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"  {status}: {count} ({count/len(quality_events_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Root cause distribution (for closed events)\n",
    "        closed_events = quality_events_df[quality_events_df['status'] == \"Closed\"]\n",
    "        if len(closed_events) > 0:\n",
    "            print(\"\\nRoot Cause Distribution (Closed Events):\")\n",
    "            root_cause_counts = closed_events['root_cause'].value_counts().head(10)\n",
    "            for cause, count in root_cause_counts.items():\n",
    "                if cause:  # Skip empty causes\n",
    "                    print(f\"  {cause}: {count} ({count/len(closed_events)*100:.1f}%)\")\n",
    "        \n",
    "        # Time to close\n",
    "        if len(closed_events) > 0:\n",
    "            # Convert to datetime\n",
    "            closed_events['detection_date'] = pd.to_datetime(closed_events['detection_date'])\n",
    "            closed_events['closure_date'] = pd.to_datetime(closed_events['closure_date'])\n",
    "            \n",
    "            # Calculate days to close\n",
    "            closed_events['days_to_close'] = (closed_events['closure_date'] - closed_events['detection_date']).dt.days\n",
    "            \n",
    "            print(\"\\nTime to Close Statistics:\")\n",
    "            print(f\"  Average days to close: {closed_events['days_to_close'].mean():.1f} days\")\n",
    "            print(f\"  Minimum days to close: {closed_events['days_to_close'].min()} days\")\n",
    "            print(f\"  Maximum days to close: {closed_events['days_to_close'].max()} days\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    material_lots_df = load_material_lots_data()\n",
    "    products_df = load_products_data()\n",
    "    batches_df = load_batches_data()\n",
    "    work_orders_df = load_work_orders_data()\n",
    "    equipment_df = load_equipment_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    \n",
    "    if material_lots_df is not None:\n",
    "        # Generate quality tests data\n",
    "        quality_tests_df = generate_quality_tests(\n",
    "            material_lots_df,\n",
    "            products_df,\n",
    "            batches_df,\n",
    "            work_orders_df,\n",
    "            equipment_df,\n",
    "            personnel_df,\n",
    "            num_tests=500,  # Generate 500 quality test records\n",
    "            output_file=\"data/quality_tests.csv\"\n",
    "        )\n",
    "        \n",
    "        # Generate quality events based on tests\n",
    "        if quality_tests_df is not None:\n",
    "            # Try to load process areas data if available\n",
    "            try:\n",
    "                process_areas_df = pd.read_csv(\"data/process_areas.csv\")\n",
    "            except FileNotFoundError:\n",
    "                process_areas_df = None\n",
    "            \n",
    "            quality_events_df = generate_quality_events(\n",
    "                quality_tests_df,\n",
    "                material_lots_df,\n",
    "                batches_df,\n",
    "                equipment_df,\n",
    "                process_areas_df,\n",
    "                personnel_df,\n",
    "                num_events=100,  # Generate 100 quality event records\n",
    "                output_file=\"data/quality_events.csv\"\n",
    "            )\n",
    "        else:\n",
    "            quality_events_df = None\n",
    "        \n",
    "        # Display statistics\n",
    "        if quality_tests_df is not None:\n",
    "            display_statistics(quality_tests_df, quality_events_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample quality tests data (first 5 records):\")\n",
    "            print(quality_tests_df.head(5))\n",
    "            \n",
    "            if quality_events_df is not None:\n",
    "                print(\"\\nSample quality events data (first 5 records):\")\n",
    "                print(quality_events_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1cbfe",
   "metadata": {},
   "source": [
    "Resource Utilization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc0de25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Resource utilization will be generated with synthetic personnel IDs.\n",
      "Generating resource utilization data...\n",
      "Successfully generated 1000 resource utilization records.\n",
      "Data saved to data/resource_utilization.csv\n",
      "\n",
      "Resource Utilization Statistics:\n",
      "Total utilization records: 1000\n",
      "\n",
      "Resource Type Distribution:\n",
      "  Material: 521 (52.1%)\n",
      "  Equipment: 416 (41.6%)\n",
      "  Personnel: 50 (5.0%)\n",
      "  Utility: 13 (1.3%)\n",
      "\n",
      "Availability Status Distribution:\n",
      "  In Use: 433 (43.3%)\n",
      "  Running: 372 (37.2%)\n",
      "  Available: 70 (7.0%)\n",
      "  Down: 44 (4.4%)\n",
      "  Assigned: 43 (4.3%)\n",
      "  On Hold: 31 (3.1%)\n",
      "  Unavailable: 7 (0.7%)\n",
      "\n",
      "Records with downtime: 139 (13.9%)\n",
      "\n",
      "Downtime Reason Distribution:\n",
      "  Not Required: 57 (41.0%)\n",
      "  Replenishment: 8 (5.8%)\n",
      "  Operator Break: 7 (5.0%)\n",
      "  Quality Hold: 6 (4.3%)\n",
      "  Inventory Count: 5 (3.6%)\n",
      "  Scheduled Maintenance: 5 (3.6%)\n",
      "  Cleaning: 5 (3.6%)\n",
      "  Transfer in Progress: 5 (3.6%)\n",
      "  Power Outage: 4 (2.9%)\n",
      "  Tool Change: 4 (2.9%)\n",
      "\n",
      "Utilization Statistics by Resource Type:\n",
      "\n",
      "  Material:\n",
      "    Records: 521\n",
      "    Avg Planned Utilization: 39.3%\n",
      "    Avg Actual Utilization: 36.3%\n",
      "    Avg Utilization Percentage: 100.1%\n",
      "    Downtime Percentage: 16.9%\n",
      "    Top Downtime Reason: Not Required\n",
      "\n",
      "  Equipment:\n",
      "    Records: 416\n",
      "    Avg Planned Utilization: 77.8%\n",
      "    Avg Actual Utilization: 69.5%\n",
      "    Avg Utilization Percentage: 89.9%\n",
      "    Downtime Percentage: 10.6%\n",
      "    Top Downtime Reason: Operator Break\n",
      "\n",
      "  Personnel:\n",
      "    Records: 50\n",
      "    Avg Planned Utilization: 68.7%\n",
      "    Avg Actual Utilization: 60.0%\n",
      "    Avg Utilization Percentage: 88.5%\n",
      "    Downtime Percentage: 14.0%\n",
      "    Top Downtime Reason: Shift Change\n",
      "\n",
      "  Utility:\n",
      "    Records: 13\n",
      "    Avg Planned Utilization: 50.3%\n",
      "    Avg Actual Utilization: 36.7%\n",
      "    Avg Utilization Percentage: 73.9%\n",
      "\n",
      "Daily Average Utilization:\n",
      "  2025-06-15: 51.1%\n",
      "  2025-06-16: 51.9%\n",
      "  2025-06-17: 50.0%\n",
      "\n",
      "Records associated with work orders: 1000 (100.0%)\n",
      "\n",
      "Total unique resources tracked: 349\n",
      "Unique resources by type:\n",
      "  Material: 180\n",
      "  Equipment: 145\n",
      "  Personnel: 19\n",
      "  Utility: 5\n",
      "\n",
      "Sample resource utilization data (first 5 records):\n",
      "            timestamp   resource_id resource_type     order_id  \\\n",
      "0 2025-06-15 19:16:58   EQ-EAAF1096     Equipment          NaN   \n",
      "1 2025-06-15 19:16:58   EQ-95C44441     Equipment  WO-379AFE33   \n",
      "2 2025-06-15 19:16:58  LOT-A8D99CB4      Material          NaN   \n",
      "3 2025-06-15 19:16:58   EQ-277C7BA4     Equipment  WO-24D6C2AB   \n",
      "4 2025-06-15 19:16:58  LOT-A19F5832      Material  WO-8025B7E2   \n",
      "\n",
      "   utilization_percentage  planned_utilization  actual_utilization  \\\n",
      "0                   105.5                 74.9                79.0   \n",
      "1                    99.7                 78.5                78.2   \n",
      "2                     9.6                 54.7                 5.3   \n",
      "3                   101.1                 87.3                88.3   \n",
      "4                    27.6                 29.0                 8.0   \n",
      "\n",
      "  availability_status  downtime downtime_reason         day  \n",
      "0             Running         0             NaN  2025-06-15  \n",
      "1             Running         0             NaN  2025-06-15  \n",
      "2              In Use         0             NaN  2025-06-15  \n",
      "3             Running         0             NaN  2025-06-15  \n",
      "4              In Use         0             NaN  2025-06-15  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Please run the equipment data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Resource utilization will be generated with synthetic personnel IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_material_lots_data(material_lots_file=\"data/material_lots.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated material lots data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_file: CSV file containing material lots data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the material lots data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(material_lots_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Material lots data file {material_lots_file} not found.\")\n",
    "        print(\"Resource utilization will be generated with synthetic material IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_work_orders_data(work_orders_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated work orders data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_file: CSV file containing work orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the work orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(work_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Work orders data file {work_orders_file} not found.\")\n",
    "        print(\"Resource utilization will be generated with synthetic work order IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_resource_utilization(equipment_df, personnel_df=None, material_lots_df=None, work_orders_df=None,\n",
    "                               num_records=1000, time_interval_minutes=60, start_time=None, end_time=None,\n",
    "                               output_file=\"data/resource_utilization.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the ResourceUtilization table from ISA-95 Level 3.\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_df: DataFrame containing equipment data\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - material_lots_df: DataFrame containing material lots data (optional)\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - num_records: Number of resource utilization records to generate\n",
    "    - time_interval_minutes: Time interval between records in minutes\n",
    "    - start_time: Start time for utilization data (defaults to 30 days ago)\n",
    "    - end_time: End time for utilization data (defaults to now)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing a sample of the generated resource utilization data\n",
    "    \"\"\"\n",
    "    if equipment_df is None or len(equipment_df) == 0:\n",
    "        print(\"Error: No equipment data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=30)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Get work order IDs if available, otherwise generate synthetic ones\n",
    "    if work_orders_df is not None and len(work_orders_df) > 0:\n",
    "        work_order_ids = work_orders_df['work_order_id'].unique().tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    \n",
    "    # Create resource IDs from available data\n",
    "    resource_ids = []\n",
    "    resource_types = []\n",
    "    \n",
    "    # Add equipment resources\n",
    "    for _, equipment in equipment_df.iterrows():\n",
    "        resource_ids.append(equipment['equipment_id'])\n",
    "        resource_types.append(\"Equipment\")\n",
    "    \n",
    "    # Add personnel resources if available\n",
    "    if personnel_df is not None and len(personnel_df) > 0:\n",
    "        for _, person in personnel_df.iterrows():\n",
    "            resource_ids.append(person['personnel_id'])\n",
    "            resource_types.append(\"Personnel\")\n",
    "    else:\n",
    "        # Add synthetic personnel\n",
    "        for i in range(20):\n",
    "            resource_ids.append(f\"PERS-{uuid.uuid4().hex[:8].upper()}\")\n",
    "            resource_types.append(\"Personnel\")\n",
    "    \n",
    "    # Add material resources if available\n",
    "    if material_lots_df is not None and len(material_lots_df) > 0:\n",
    "        # Only include active materials\n",
    "        active_materials = material_lots_df[material_lots_df['status'] == 'Active']\n",
    "        if len(active_materials) > 0:\n",
    "            for _, material in active_materials.iterrows():\n",
    "                resource_ids.append(material['lot_id'])\n",
    "                resource_types.append(\"Material\")\n",
    "        else:\n",
    "            # Add some materials even if none are active\n",
    "            for _, material in material_lots_df.sample(min(20, len(material_lots_df))).iterrows():\n",
    "                resource_ids.append(material['lot_id'])\n",
    "                resource_types.append(\"Material\")\n",
    "    else:\n",
    "        # Add synthetic materials\n",
    "        for i in range(15):\n",
    "            resource_ids.append(f\"LOT-{uuid.uuid4().hex[:8].upper()}\")\n",
    "            resource_types.append(\"Material\")\n",
    "    \n",
    "    # Add utility resources (synthetic)\n",
    "    utility_types = [\"Electricity\", \"Water\", \"Steam\", \"Compressed Air\", \"Cooling Water\", \"Natural Gas\", \"Nitrogen\"]\n",
    "    for utility in utility_types:\n",
    "        resource_ids.append(f\"UTIL-{utility.upper().replace(' ', '')}\")\n",
    "        resource_types.append(\"Utility\")\n",
    "    \n",
    "    # Create a resource map for lookup\n",
    "    resource_map = dict(zip(resource_ids, resource_types))\n",
    "    \n",
    "    # Define downtime reasons by resource type\n",
    "    downtime_reasons = {\n",
    "        \"Equipment\": [\n",
    "            \"Preventive Maintenance\", \"Breakdown\", \"Setup/Changeover\", \"Calibration\", \n",
    "            \"Cleaning\", \"Operator Break\", \"Material Shortage\", \"Quality Issue\", \n",
    "            \"Scheduled Maintenance\", \"Tool Change\", \"Software Update\", \"Power Outage\"\n",
    "        ],\n",
    "        \"Personnel\": [\n",
    "            \"Break\", \"Training\", \"Meeting\", \"Shift Change\", \"Absence\", \n",
    "            \"Documentation\", \"Administrative Task\", \"Support Activity\"\n",
    "        ],\n",
    "        \"Material\": [\n",
    "            \"Quality Hold\", \"Awaiting Test Results\", \"Inventory Count\", \n",
    "            \"Transfer in Progress\", \"Shortage\", \"Replenishment\"\n",
    "        ],\n",
    "        \"Utility\": [\n",
    "            \"Maintenance\", \"Supply Interruption\", \"External Outage\", \n",
    "            \"Capacity Limit\", \"Pressure Drop\", \"Temperature Deviation\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate timestamp sequence\n",
    "    time_range_minutes = int((end_time - start_time).total_seconds() / 60)\n",
    "    num_intervals = min(num_records, time_range_minutes // time_interval_minutes)\n",
    "    \n",
    "    timestamps = [\n",
    "        start_time + timedelta(minutes=i * time_interval_minutes)\n",
    "        for i in range(num_intervals)\n",
    "    ]\n",
    "    \n",
    "    # Prepare to write data directly to CSV for memory efficiency\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = [\n",
    "            'timestamp', 'resource_id', 'resource_type', 'order_id',\n",
    "            'utilization_percentage', 'planned_utilization', 'actual_utilization',\n",
    "            'availability_status', 'downtime', 'downtime_reason'\n",
    "        ]\n",
    "        writer = pd.DataFrame(columns=fieldnames)\n",
    "        \n",
    "        print(f\"Generating resource utilization data...\")\n",
    "        records_generated = 0\n",
    "        data_rows = []\n",
    "        \n",
    "        # For each timestamp, generate utilization for a subset of resources\n",
    "        for timestamp in timestamps:\n",
    "            # Determine how many resources to include at this timestamp\n",
    "            # (not all resources are tracked at every interval)\n",
    "            num_resources = random.randint(10, min(50, len(resource_ids)))\n",
    "            \n",
    "            # Select random resources to track at this interval\n",
    "            selected_resources = random.sample(resource_ids, num_resources)\n",
    "            \n",
    "            # For each selected resource, generate utilization data\n",
    "            for resource_id in selected_resources:\n",
    "                resource_type = resource_map[resource_id]\n",
    "                \n",
    "                # Determine if associated with a work order\n",
    "                if random.random() < 0.7:  # 70% associated with work order\n",
    "                    order_id = random.choice(work_order_ids)\n",
    "                else:\n",
    "                    order_id = \"\"\n",
    "                \n",
    "                # Generate utilization data based on resource type\n",
    "                if resource_type == \"Equipment\":\n",
    "                    # Equipment tends to have higher utilization\n",
    "                    planned_utilization = random.uniform(60, 95)\n",
    "                    \n",
    "                    # Determine if equipment is down\n",
    "                    if random.random() < 0.1:  # 10% chance of downtime\n",
    "                        availability_status = \"Down\"\n",
    "                        actual_utilization = 0.0\n",
    "                        downtime = time_interval_minutes\n",
    "                        downtime_reason = random.choice(downtime_reasons[\"Equipment\"])\n",
    "                    else:\n",
    "                        # Variation from planned (normally distributed)\n",
    "                        variation = random.normalvariate(0, 10)  # Mean 0, std dev 10 percentage points\n",
    "                        actual_utilization = max(0, min(100, planned_utilization + variation))\n",
    "                        \n",
    "                        if actual_utilization < 5:\n",
    "                            availability_status = \"Idle\"\n",
    "                            downtime = time_interval_minutes\n",
    "                            downtime_reason = \"No Production Scheduled\"\n",
    "                        else:\n",
    "                            availability_status = \"Running\"\n",
    "                            downtime = 0\n",
    "                            downtime_reason = \"\"\n",
    "                \n",
    "                elif resource_type == \"Personnel\":\n",
    "                    # Personnel utilization tends to be more varied\n",
    "                    planned_utilization = random.uniform(50, 90)\n",
    "                    \n",
    "                    # Determine if personnel is unavailable\n",
    "                    if random.random() < 0.15:  # 15% chance of unavailability\n",
    "                        availability_status = \"Unavailable\"\n",
    "                        actual_utilization = 0.0\n",
    "                        downtime = time_interval_minutes\n",
    "                        downtime_reason = random.choice(downtime_reasons[\"Personnel\"])\n",
    "                    else:\n",
    "                        # Variation from planned (more variable than equipment)\n",
    "                        variation = random.normalvariate(0, 15)  # Mean 0, std dev 15 percentage points\n",
    "                        actual_utilization = max(0, min(100, planned_utilization + variation))\n",
    "                        \n",
    "                        if actual_utilization < 10:\n",
    "                            availability_status = \"Available\"\n",
    "                            downtime = time_interval_minutes\n",
    "                            downtime_reason = \"Waiting for Assignment\"\n",
    "                        else:\n",
    "                            availability_status = \"Assigned\"\n",
    "                            downtime = 0\n",
    "                            downtime_reason = \"\"\n",
    "                \n",
    "                elif resource_type == \"Material\":\n",
    "                    # Material utilization is typically lower and spiky\n",
    "                    planned_utilization = random.uniform(20, 60)\n",
    "                    \n",
    "                    # Determine if material is unavailable\n",
    "                    if random.random() < 0.05:  # 5% chance of unavailability\n",
    "                        availability_status = \"On Hold\"\n",
    "                        actual_utilization = 0.0\n",
    "                        downtime = time_interval_minutes\n",
    "                        downtime_reason = random.choice(downtime_reasons[\"Material\"])\n",
    "                    else:\n",
    "                        # Materials often have bursts of usage\n",
    "                        if random.random() < 0.3:  # 30% chance of high usage\n",
    "                            actual_utilization = random.uniform(70, 100)\n",
    "                            availability_status = \"In Use\"\n",
    "                            downtime = 0\n",
    "                            downtime_reason = \"\"\n",
    "                        else:\n",
    "                            actual_utilization = random.uniform(0, planned_utilization)\n",
    "                            if actual_utilization < 5:\n",
    "                                availability_status = \"Available\"\n",
    "                                downtime = time_interval_minutes\n",
    "                                downtime_reason = \"Not Required\"\n",
    "                            else:\n",
    "                                availability_status = \"In Use\"\n",
    "                                downtime = 0\n",
    "                                downtime_reason = \"\"\n",
    "                \n",
    "                else:  # Utility\n",
    "                    # Utilities typically have high availability but variable usage\n",
    "                    planned_utilization = random.uniform(30, 70)\n",
    "                    \n",
    "                    # Determine if utility is unavailable\n",
    "                    if random.random() < 0.03:  # 3% chance of outage\n",
    "                        availability_status = \"Outage\"\n",
    "                        actual_utilization = 0.0\n",
    "                        downtime = time_interval_minutes\n",
    "                        downtime_reason = random.choice(downtime_reasons[\"Utility\"])\n",
    "                    else:\n",
    "                        # Utilities can have peak usage periods\n",
    "                        hour_of_day = timestamp.hour\n",
    "                        \n",
    "                        # Higher usage during standard working hours\n",
    "                        if 8 <= hour_of_day <= 17:\n",
    "                            usage_factor = random.uniform(0.8, 1.2)\n",
    "                        else:\n",
    "                            usage_factor = random.uniform(0.5, 0.9)\n",
    "                            \n",
    "                        actual_utilization = min(100, planned_utilization * usage_factor)\n",
    "                        availability_status = \"Available\"\n",
    "                        downtime = 0\n",
    "                        downtime_reason = \"\"\n",
    "                \n",
    "                # Calculate utilization percentage (actual vs. planned)\n",
    "                if planned_utilization > 0:\n",
    "                    utilization_percentage = (actual_utilization / planned_utilization) * 100\n",
    "                else:\n",
    "                    utilization_percentage = 0.0\n",
    "                \n",
    "                # Round values for cleaner data\n",
    "                planned_utilization = round(planned_utilization, 1)\n",
    "                actual_utilization = round(actual_utilization, 1)\n",
    "                utilization_percentage = round(utilization_percentage, 1)\n",
    "                \n",
    "                # Add row to data\n",
    "                data_rows.append({\n",
    "                    'timestamp': timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    'resource_id': resource_id,\n",
    "                    'resource_type': resource_type,\n",
    "                    'order_id': order_id,\n",
    "                    'utilization_percentage': utilization_percentage,\n",
    "                    'planned_utilization': planned_utilization,\n",
    "                    'actual_utilization': actual_utilization,\n",
    "                    'availability_status': availability_status,\n",
    "                    'downtime': downtime,\n",
    "                    'downtime_reason': downtime_reason\n",
    "                })\n",
    "                \n",
    "                records_generated += 1\n",
    "                if records_generated % 10000 == 0:\n",
    "                    print(f\"Generated {records_generated} utilization records so far...\")\n",
    "                \n",
    "                # If we've hit our target number of records, stop\n",
    "                if records_generated >= num_records:\n",
    "                    break\n",
    "            \n",
    "            # If we've hit our target number of records, stop\n",
    "            if records_generated >= num_records:\n",
    "                break\n",
    "        \n",
    "        # Create DataFrame from rows and save to CSV\n",
    "        writer = pd.DataFrame(data_rows)\n",
    "        writer.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {records_generated} resource utilization records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    # Return a sample of the data (first 1000 rows) for preview\n",
    "    return pd.read_csv(output_file, nrows=1000)\n",
    "\n",
    "def display_statistics(resource_utilization_df):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated resource utilization data\n",
    "    \n",
    "    Parameters:\n",
    "    - resource_utilization_df: DataFrame containing resource utilization data\n",
    "    \"\"\"\n",
    "    if resource_utilization_df is None or len(resource_utilization_df) == 0:\n",
    "        print(\"No resource utilization data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nResource Utilization Statistics:\")\n",
    "    print(f\"Total utilization records: {len(resource_utilization_df)}\")\n",
    "    \n",
    "    # Resource type distribution\n",
    "    print(\"\\nResource Type Distribution:\")\n",
    "    type_counts = resource_utilization_df['resource_type'].value_counts()\n",
    "    for res_type, count in type_counts.items():\n",
    "        print(f\"  {res_type}: {count} ({count/len(resource_utilization_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Availability status distribution\n",
    "    print(\"\\nAvailability Status Distribution:\")\n",
    "    status_counts = resource_utilization_df['availability_status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(resource_utilization_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Downtime analysis\n",
    "    downtime_records = resource_utilization_df[resource_utilization_df['downtime'] > 0]\n",
    "    downtime_pct = len(downtime_records) / len(resource_utilization_df) * 100\n",
    "    print(f\"\\nRecords with downtime: {len(downtime_records)} ({downtime_pct:.1f}%)\")\n",
    "    \n",
    "    if len(downtime_records) > 0:\n",
    "        print(\"\\nDowntime Reason Distribution:\")\n",
    "        reason_counts = downtime_records['downtime_reason'].value_counts().head(10)\n",
    "        for reason, count in reason_counts.items():\n",
    "            print(f\"  {reason}: {count} ({count/len(downtime_records)*100:.1f}%)\")\n",
    "    \n",
    "    # Utilization statistics by resource type\n",
    "    print(\"\\nUtilization Statistics by Resource Type:\")\n",
    "    for res_type in type_counts.index:\n",
    "        type_data = resource_utilization_df[resource_utilization_df['resource_type'] == res_type]\n",
    "        \n",
    "        print(f\"\\n  {res_type}:\")\n",
    "        print(f\"    Records: {len(type_data)}\")\n",
    "        print(f\"    Avg Planned Utilization: {type_data['planned_utilization'].mean():.1f}%\")\n",
    "        print(f\"    Avg Actual Utilization: {type_data['actual_utilization'].mean():.1f}%\")\n",
    "        print(f\"    Avg Utilization Percentage: {type_data['utilization_percentage'].mean():.1f}%\")\n",
    "        \n",
    "        # Downtime specific to this resource type\n",
    "        type_downtime = type_data[type_data['downtime'] > 0]\n",
    "        if len(type_downtime) > 0:\n",
    "            downtime_pct = len(type_downtime) / len(type_data) * 100\n",
    "            print(f\"    Downtime Percentage: {downtime_pct:.1f}%\")\n",
    "            print(f\"    Top Downtime Reason: {type_downtime['downtime_reason'].value_counts().index[0]}\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    resource_utilization_df['timestamp'] = pd.to_datetime(resource_utilization_df['timestamp'])\n",
    "    \n",
    "    # Group by day\n",
    "    resource_utilization_df['day'] = resource_utilization_df['timestamp'].dt.date\n",
    "    daily_utilization = resource_utilization_df.groupby('day')['actual_utilization'].mean()\n",
    "    \n",
    "    print(\"\\nDaily Average Utilization:\")\n",
    "    for day, util in daily_utilization.head(7).items():\n",
    "        print(f\"  {day}: {util:.1f}%\")\n",
    "    \n",
    "    # Work order connections\n",
    "    wo_count = resource_utilization_df['order_id'].apply(lambda x: x != \"\").sum()\n",
    "    wo_pct = wo_count / len(resource_utilization_df) * 100\n",
    "    print(f\"\\nRecords associated with work orders: {wo_count} ({wo_pct:.1f}%)\")\n",
    "    \n",
    "    # Resource count\n",
    "    resource_count = resource_utilization_df['resource_id'].nunique()\n",
    "    print(f\"\\nTotal unique resources tracked: {resource_count}\")\n",
    "    \n",
    "    # Resources by type\n",
    "    print(\"Unique resources by type:\")\n",
    "    for res_type in type_counts.index:\n",
    "        type_data = resource_utilization_df[resource_utilization_df['resource_type'] == res_type]\n",
    "        type_resources = type_data['resource_id'].nunique()\n",
    "        print(f\"  {res_type}: {type_resources}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    equipment_df = load_equipment_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    material_lots_df = load_material_lots_data()\n",
    "    work_orders_df = load_work_orders_data()\n",
    "    \n",
    "    if equipment_df is not None:\n",
    "        # Generate resource utilization data\n",
    "        resource_utilization_df = generate_resource_utilization(\n",
    "            equipment_df,\n",
    "            personnel_df,\n",
    "            material_lots_df,\n",
    "            work_orders_df,\n",
    "            num_records=1000,  # Generate 1000 utilization records\n",
    "            time_interval_minutes=60,  # Hourly intervals\n",
    "            output_file=\"data/resource_utilization.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if resource_utilization_df is not None:\n",
    "            display_statistics(resource_utilization_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample resource utilization data (first 5 records):\")\n",
    "            print(resource_utilization_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b59a08",
   "metadata": {},
   "source": [
    "Maintenance Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2089fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Maintenance activities will be generated with synthetic technician IDs.\n",
      "Generating synthetic technician IDs...\n",
      "Successfully generated 300 maintenance activity records.\n",
      "Data saved to data/maintenance_activities.csv\n",
      "\n",
      "Maintenance Activities Statistics:\n",
      "Total maintenance activities: 300\n",
      "\n",
      "Activity Type Distribution:\n",
      "  Preventive: 127 (42.3%)\n",
      "  Corrective: 91 (30.3%)\n",
      "  Inspection: 33 (11.0%)\n",
      "  Predictive: 33 (11.0%)\n",
      "  Calibration: 11 (3.7%)\n",
      "  Overhaul: 4 (1.3%)\n",
      "  Upgrade: 1 (0.3%)\n",
      "\n",
      "Status Distribution:\n",
      "  Completed: 220 (73.3%)\n",
      "  Planned: 50 (16.7%)\n",
      "  Canceled: 18 (6.0%)\n",
      "  Scheduled: 12 (4.0%)\n",
      "\n",
      "Priority Distribution:\n",
      "  Priority 1: 28 (9.3%)\n",
      "  Priority 2: 91 (30.3%)\n",
      "  Priority 3: 115 (38.3%)\n",
      "  Priority 4: 46 (15.3%)\n",
      "  Priority 5: 20 (6.7%)\n",
      "\n",
      "Activities requiring downtime: 197 (65.7%)\n",
      "\n",
      "Planned Duration Statistics:\n",
      "  Average planned duration: 7.4 hours\n",
      "  Minimum planned duration: 0.5 hours\n",
      "  Maximum planned duration: 66.9 hours\n",
      "\n",
      "Actual Duration Statistics (Completed Activities):\n",
      "  Average actual duration: 6.3 hours\n",
      "  Minimum actual duration: 0.3 hours\n",
      "  Maximum actual duration: 36.0 hours\n",
      "\n",
      "Duration Variance (Completed Activities):\n",
      "  Average variance: -0.3 hours\n",
      "  Average variance percentage: -1.6%\n",
      "  On time (±10%): 87 (39.5%)\n",
      "  Faster than planned: 74 (33.6%)\n",
      "  Slower than planned: 59 (26.8%)\n",
      "\n",
      "Time Distribution:\n",
      "  Past activities: 238 (79.3%)\n",
      "  Future activities: 62 (20.7%)\n",
      "\n",
      "Monthly Distribution (top 6 months):\n",
      "  2024-07: 3 activities\n",
      "  2024-08: 13 activities\n",
      "  2024-09: 11 activities\n",
      "  2024-10: 9 activities\n",
      "  2024-11: 13 activities\n",
      "  2024-12: 18 activities\n",
      "\n",
      "Top 5 Equipment by Maintenance Frequency:\n",
      "  EQ-38DD265D: 8 activities\n",
      "  EQ-DFB13E01: 7 activities\n",
      "  EQ-FB623D68: 7 activities\n",
      "  EQ-EAAF1096: 7 activities\n",
      "  EQ-91F2E6D5: 6 activities\n",
      "\n",
      "Top 5 Technicians by Activity Assignment:\n",
      "  TECH-0490DC84: 26 activities\n",
      "  TECH-38FFABF7: 24 activities\n",
      "  TECH-B37FB6BA: 24 activities\n",
      "  TECH-1B97C2D2: 23 activities\n",
      "  TECH-80BDB776: 22 activities\n",
      "\n",
      "Downtime Requirement by Activity Type:\n",
      "  Preventive: 83/127 (65.4%)\n",
      "  Corrective: 88/91 (96.7%)\n",
      "  Inspection: 6/33 (18.2%)\n",
      "  Predictive: 11/33 (33.3%)\n",
      "  Calibration: 5/11 (45.5%)\n",
      "  Overhaul: 3/4 (75.0%)\n",
      "  Upgrade: 1/1 (100.0%)\n",
      "\n",
      "Sample maintenance activities data (first 5 records):\n",
      "      activity_id activity_type equipment_id work_order_id  \\\n",
      "0  MAINT-5E8E6467    Preventive  EQ-D7327866   WO-2BEB29B2   \n",
      "1  MAINT-88DEF52E    Preventive  EQ-FBC39D48   WO-4AD4E54B   \n",
      "2  MAINT-7DE9883A    Corrective  EQ-F4696348   WO-51015968   \n",
      "3  MAINT-37A4DBD8    Inspection  EQ-F3D6BF0F   WO-BB61CFED   \n",
      "4  MAINT-1F3AB856    Predictive  EQ-DFB13E01   WO-2BEB29B2   \n",
      "\n",
      "   planned_start_date   actual_start_date    planned_end_date  \\\n",
      "0 2024-12-18 06:16:58 2024-12-18 07:20:58 2024-12-18 09:27:39   \n",
      "1 2024-11-14 09:16:58 2024-11-14 10:50:58 2024-11-14 16:11:19   \n",
      "2 2024-09-05 05:16:58 2024-09-05 03:47:58 2024-09-06 03:46:03   \n",
      "3 2025-07-14 11:16:58 2025-07-14 11:00:58 2025-07-14 13:12:41   \n",
      "4 2025-06-24 10:16:58 2025-06-24 09:59:58 2025-06-24 11:53:12   \n",
      "\n",
      "      actual_end_date     status  priority                       description  \\\n",
      "0 2024-12-18 11:00:20  Completed         4  Routine maintenance per schedule   \n",
      "1 2024-11-14 17:20:56  Completed         2     Planned component replacement   \n",
      "2 2024-09-05 21:49:45  Completed         2           Repair electrical fault   \n",
      "3 2025-07-14 12:50:35  Completed         1       Regulatory compliance check   \n",
      "4 2025-06-24 11:32:25  Completed         2       Condition-based maintenance   \n",
      "\n",
      "   technician_id  downtime_required actual_downtime_minutes  \\\n",
      "0  TECH-38FFABF7               True                     219   \n",
      "1  TECH-0790D4F4               True                     390   \n",
      "2  TECH-FD14DAC6               True                    1082   \n",
      "3  TECH-1B97C2D2               True                     110   \n",
      "4  TECH-0490DC84              False                      92   \n",
      "\n",
      "   planned_duration_hours    month  \n",
      "0                3.178056  2024-12  \n",
      "1                6.905833  2024-11  \n",
      "2               22.484722  2024-09  \n",
      "3                1.928611  2025-07  \n",
      "4                1.603889  2025-06  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114075/861228857.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  completed['actual_duration_hours'] = (\n",
      "/tmp/ipykernel_114075/861228857.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  completed['duration_variance'] = completed['actual_duration_hours'] - completed['planned_duration_hours']\n",
      "/tmp/ipykernel_114075/861228857.py:476: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  completed['duration_variance_pct'] = (completed['duration_variance'] / completed['planned_duration_hours']) * 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Please run the equipment data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_work_orders_data(work_orders_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated work orders data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_file: CSV file containing work orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the work orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(work_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Work orders data file {work_orders_file} not found.\")\n",
    "        print(\"Maintenance activities will be generated with synthetic work order IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Maintenance activities will be generated with synthetic technician IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_maintenance_activities(equipment_df, work_orders_df=None, personnel_df=None,\n",
    "                                  num_activities=300, start_time=None, end_time=None,\n",
    "                                  output_file=\"data/maintenance_activities.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the MaintenanceActivities table from ISA-95 Level 3.\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_df: DataFrame containing equipment data\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_activities: Number of maintenance activity records to generate\n",
    "    - start_time: Start time for activity dates (defaults to 365 days ago)\n",
    "    - end_time: End time for activity dates (defaults to 30 days in the future)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated maintenance activities data\n",
    "    \"\"\"\n",
    "    if equipment_df is None or len(equipment_df) == 0:\n",
    "        print(\"Error: No equipment data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=365)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now() + timedelta(days=30)\n",
    "    \n",
    "    # Extract or generate work order IDs\n",
    "    if work_orders_df is not None and len(work_orders_df) > 0:\n",
    "        # Filter for maintenance work orders\n",
    "        maintenance_wos = work_orders_df[work_orders_df['work_order_type'] == 'Maintenance']\n",
    "        \n",
    "        if len(maintenance_wos) > 0:\n",
    "            work_order_ids = maintenance_wos['work_order_id'].tolist()\n",
    "        else:\n",
    "            # If no maintenance work orders, generate synthetic ones\n",
    "            print(\"No maintenance work orders found. Generating synthetic work order IDs...\")\n",
    "            work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_activities)]\n",
    "    else:\n",
    "        # Generate synthetic work order IDs\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_activities)]\n",
    "    \n",
    "    # Extract or generate technician IDs\n",
    "    if personnel_df is not None and len(personnel_df) > 0:\n",
    "        technician_ids = personnel_df['personnel_id'].sample(min(20, len(personnel_df))).tolist()\n",
    "    else:\n",
    "        # Generate synthetic technician IDs\n",
    "        print(\"Generating synthetic technician IDs...\")\n",
    "        technician_ids = [f\"TECH-{uuid.uuid4().hex[:8].upper()}\" for _ in range(15)]\n",
    "    \n",
    "    # Define maintenance activity types and their probabilities\n",
    "    activity_types = {\n",
    "        \"Preventive\": 0.4,       # Regular scheduled maintenance\n",
    "        \"Corrective\": 0.3,       # Fix after failure\n",
    "        \"Predictive\": 0.1,       # Based on condition monitoring\n",
    "        \"Inspection\": 0.1,       # Regular checks\n",
    "        \"Calibration\": 0.05,     # Calibrating instruments\n",
    "        \"Overhaul\": 0.03,        # Major maintenance\n",
    "        \"Upgrade\": 0.02          # Improving equipment\n",
    "    }\n",
    "    \n",
    "    # Define priority levels\n",
    "    priority_levels = [1, 2, 3, 4, 5]  # 1 = highest, 5 = lowest\n",
    "    priority_weights = [0.1, 0.2, 0.4, 0.2, 0.1]  # Most activities are medium priority\n",
    "    \n",
    "    # Define maintenance activity durations (in hours) by type\n",
    "    activity_durations = {\n",
    "        \"Preventive\": (2, 8),       # 2-8 hours\n",
    "        \"Corrective\": (1, 24),      # 1-24 hours\n",
    "        \"Predictive\": (1, 4),       # 1-4 hours\n",
    "        \"Inspection\": (0.5, 2),     # 30 min - 2 hours\n",
    "        \"Calibration\": (1, 6),      # 1-6 hours\n",
    "        \"Overhaul\": (8, 72),        # 8-72 hours\n",
    "        \"Upgrade\": (4, 48)          # 4-48 hours\n",
    "    }\n",
    "    \n",
    "    # Define activity statuses and their time-based probabilities\n",
    "    activity_statuses = [\"Planned\", \"Scheduled\", \"In Progress\", \"Completed\", \"Canceled\"]\n",
    "    \n",
    "    # Define common descriptions by activity type\n",
    "    activity_descriptions = {\n",
    "        \"Preventive\": [\n",
    "            \"Routine maintenance per schedule\",\n",
    "            \"Preventive maintenance as per manual\",\n",
    "            \"Scheduled lubrication and inspection\",\n",
    "            \"Regular service check\",\n",
    "            \"Planned component replacement\"\n",
    "        ],\n",
    "        \"Corrective\": [\n",
    "            \"Repair after failure\",\n",
    "            \"Fix mechanical issue\",\n",
    "            \"Replace worn component\",\n",
    "            \"Repair electrical fault\",\n",
    "            \"Emergency fix after breakdown\"\n",
    "        ],\n",
    "        \"Predictive\": [\n",
    "            \"Maintenance based on vibration analysis\",\n",
    "            \"Service based on oil analysis results\",\n",
    "            \"Pre-emptive repair based on monitoring\",\n",
    "            \"Condition-based maintenance\",\n",
    "            \"Thermography-indicated maintenance\"\n",
    "        ],\n",
    "        \"Inspection\": [\n",
    "            \"Safety inspection\",\n",
    "            \"Regulatory compliance check\",\n",
    "            \"Visual inspection of components\",\n",
    "            \"Operational check\",\n",
    "            \"Performance verification\"\n",
    "        ],\n",
    "        \"Calibration\": [\n",
    "            \"Sensor calibration\",\n",
    "            \"Instrument accuracy verification\",\n",
    "            \"Scale calibration\",\n",
    "            \"Control system tuning\",\n",
    "            \"Measurement system adjustment\"\n",
    "        ],\n",
    "        \"Overhaul\": [\n",
    "            \"Complete system teardown and rebuild\",\n",
    "            \"Major component replacement\",\n",
    "            \"Full mechanical overhaul\",\n",
    "            \"Comprehensive service\",\n",
    "            \"Complete system restoration\"\n",
    "        ],\n",
    "        \"Upgrade\": [\n",
    "            \"Software update installation\",\n",
    "            \"Hardware upgrade\",\n",
    "            \"Performance enhancement modification\",\n",
    "            \"Component upgrade installation\",\n",
    "            \"Feature addition\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"activity_id\": [f\"MAINT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_activities)],\n",
    "        \"activity_type\": [],\n",
    "        \"equipment_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"planned_start_date\": [],\n",
    "        \"actual_start_date\": [],\n",
    "        \"planned_end_date\": [],\n",
    "        \"actual_end_date\": [],\n",
    "        \"status\": [],\n",
    "        \"priority\": [],\n",
    "        \"description\": [],\n",
    "        \"technician_id\": [],\n",
    "        \"downtime_required\": [],\n",
    "        \"actual_downtime_minutes\": []\n",
    "    }\n",
    "    \n",
    "    # Create date distribution for maintenance activities\n",
    "    # More activities in recent past and near future, fewer in distant past/future\n",
    "    date_weights = []\n",
    "    time_range_days = (end_time - start_time).days\n",
    "    \n",
    "    for i in range(time_range_days):\n",
    "        # Weight activities to be more common in recent times\n",
    "        days_from_now = abs((start_time + timedelta(days=i) - datetime.now()).days)\n",
    "        if days_from_now <= 30:\n",
    "            # Recent past or near future (high density)\n",
    "            weight = 1.0\n",
    "        elif days_from_now <= 90:\n",
    "            # Medium past/future (medium density)\n",
    "            weight = 0.5\n",
    "        else:\n",
    "            # Distant past/future (low density)\n",
    "            weight = 0.2\n",
    "        date_weights.append(weight)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(date_weights)\n",
    "    date_weights = [w / total_weight for w in date_weights]\n",
    "    \n",
    "    # Generate data for each maintenance activity\n",
    "    for i in range(num_activities):\n",
    "        # Select activity type (weighted random)\n",
    "        activity_type = random.choices(\n",
    "            list(activity_types.keys()), \n",
    "            weights=list(activity_types.values())\n",
    "        )[0]\n",
    "        data[\"activity_type\"].append(activity_type)\n",
    "        \n",
    "        # Select equipment ID (favor older equipment for more maintenance)\n",
    "        if 'installation_date' in equipment_df.columns:\n",
    "            # Convert to datetime if it's not already\n",
    "            if not pd.api.types.is_datetime64_dtype(equipment_df['installation_date']):\n",
    "                equipment_df['installation_date'] = pd.to_datetime(equipment_df['installation_date'], errors='coerce')\n",
    "            \n",
    "            # Calculate equipment age\n",
    "            current_date = datetime.now()\n",
    "            equipment_df['age_days'] = (current_date - equipment_df['installation_date']).dt.days\n",
    "            \n",
    "            # Weight by age (older equipment needs more maintenance)\n",
    "            weights = equipment_df['age_days'].fillna(365).values\n",
    "            weights = weights / max(1, weights.sum())  # Normalize\n",
    "            \n",
    "            # Select equipment with probability proportional to age\n",
    "            selected_idx = random.choices(range(len(equipment_df)), weights=weights)[0]\n",
    "            equipment_id = equipment_df.iloc[selected_idx]['equipment_id']\n",
    "        else:\n",
    "            # If no installation date, select randomly\n",
    "            equipment_id = random.choice(equipment_df['equipment_id'].tolist())\n",
    "        \n",
    "        data[\"equipment_id\"].append(equipment_id)\n",
    "        \n",
    "        # Select work order ID\n",
    "        if len(work_order_ids) > 0:\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        else:\n",
    "            data[\"work_order_id\"].append(f\"WO-{uuid.uuid4().hex[:8].upper()}\")\n",
    "        \n",
    "        # Generate planned start date\n",
    "        day_idx = random.choices(range(time_range_days), weights=date_weights)[0]\n",
    "        planned_start_date = start_time + timedelta(days=day_idx)\n",
    "        \n",
    "        # Add random hours to make times more realistic\n",
    "        planned_start_date += timedelta(hours=random.randint(7, 16))  # Business hours\n",
    "        \n",
    "        data[\"planned_start_date\"].append(planned_start_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Get duration range for this activity type\n",
    "        min_hours, max_hours = activity_durations[activity_type]\n",
    "        \n",
    "        # Generate planned duration\n",
    "        planned_duration_hours = random.uniform(min_hours, max_hours)\n",
    "        planned_end_date = planned_start_date + timedelta(hours=planned_duration_hours)\n",
    "        \n",
    "        data[\"planned_end_date\"].append(planned_end_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Determine status based on dates\n",
    "        current_date = datetime.now()\n",
    "        \n",
    "        if planned_start_date > current_date:\n",
    "            # Future activity\n",
    "            if (planned_start_date - current_date).days < 7:\n",
    "                # Near future\n",
    "                status = random.choices([\"Planned\", \"Scheduled\"], weights=[0.3, 0.7])[0]\n",
    "            else:\n",
    "                # More distant future\n",
    "                status = \"Planned\"\n",
    "            \n",
    "            # Future activities don't have actual dates yet\n",
    "            data[\"actual_start_date\"].append(\"\")\n",
    "            data[\"actual_end_date\"].append(\"\")\n",
    "            data[\"actual_downtime_minutes\"].append(\"\")\n",
    "            \n",
    "        elif planned_start_date <= current_date and planned_end_date > current_date:\n",
    "            # Current activity\n",
    "            if random.random() < 0.8:  # 80% chance it started on time\n",
    "                status = \"In Progress\"\n",
    "                \n",
    "                # Activity started but not finished\n",
    "                actual_start_date = planned_start_date + timedelta(minutes=random.randint(-60, 60))\n",
    "                data[\"actual_start_date\"].append(actual_start_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                data[\"actual_end_date\"].append(\"\")\n",
    "                \n",
    "                # Partial downtime so far\n",
    "                current_downtime = (current_date - actual_start_date).total_seconds() / 60\n",
    "                data[\"actual_downtime_minutes\"].append(round(current_downtime))\n",
    "                \n",
    "            else:\n",
    "                # Activity delayed\n",
    "                status = random.choices([\"Planned\", \"Scheduled\"], weights=[0.3, 0.7])[0]\n",
    "                data[\"actual_start_date\"].append(\"\")\n",
    "                data[\"actual_end_date\"].append(\"\")\n",
    "                data[\"actual_downtime_minutes\"].append(\"\")\n",
    "                \n",
    "        else:\n",
    "            # Past activity\n",
    "            if random.random() < 0.9:  # 90% chance it was completed\n",
    "                status = \"Completed\"\n",
    "                \n",
    "                # Actual start date might vary from planned\n",
    "                start_variation_minutes = random.randint(-120, 120)  # +/- 2 hours\n",
    "                actual_start_date = planned_start_date + timedelta(minutes=start_variation_minutes)\n",
    "                \n",
    "                # Actual duration might vary from planned\n",
    "                duration_variation = random.normalvariate(1.0, 0.2)  # Mean 1.0, std dev 0.2\n",
    "                actual_duration_hours = max(0.1, planned_duration_hours * duration_variation)\n",
    "                actual_end_date = actual_start_date + timedelta(hours=actual_duration_hours)\n",
    "                \n",
    "                data[\"actual_start_date\"].append(actual_start_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                data[\"actual_end_date\"].append(actual_end_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                \n",
    "                # Calculate actual downtime\n",
    "                actual_downtime = actual_duration_hours * 60  # Convert to minutes\n",
    "                data[\"actual_downtime_minutes\"].append(round(actual_downtime))\n",
    "                \n",
    "            else:\n",
    "                # Activity was canceled\n",
    "                status = \"Canceled\"\n",
    "                data[\"actual_start_date\"].append(\"\")\n",
    "                data[\"actual_end_date\"].append(\"\")\n",
    "                data[\"actual_downtime_minutes\"].append(\"\")\n",
    "        \n",
    "        data[\"status\"].append(status)\n",
    "        \n",
    "        # Set priority (weighted random)\n",
    "        priority = random.choices(priority_levels, weights=priority_weights)[0]\n",
    "        \n",
    "        # For corrective maintenance, increase priority (more urgent)\n",
    "        if activity_type == \"Corrective\" and priority > 2:\n",
    "            priority -= 1\n",
    "            \n",
    "        data[\"priority\"].append(priority)\n",
    "        \n",
    "        # Set description\n",
    "        if activity_type in activity_descriptions:\n",
    "            description = random.choice(activity_descriptions[activity_type])\n",
    "        else:\n",
    "            description = f\"{activity_type} maintenance activity\"\n",
    "            \n",
    "        data[\"description\"].append(description)\n",
    "        \n",
    "        # Assign technician\n",
    "        data[\"technician_id\"].append(random.choice(technician_ids))\n",
    "        \n",
    "        # Determine if downtime is required\n",
    "        # Certain activity types almost always require downtime\n",
    "        if activity_type in [\"Corrective\", \"Overhaul\", \"Upgrade\"]:\n",
    "            downtime_required = random.random() < 0.95  # 95% require downtime\n",
    "        elif activity_type in [\"Preventive\", \"Calibration\"]:\n",
    "            downtime_required = random.random() < 0.7  # 70% require downtime\n",
    "        else:\n",
    "            downtime_required = random.random() < 0.3  # 30% require downtime\n",
    "            \n",
    "        data[\"downtime_required\"].append(downtime_required)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} maintenance activity records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(maintenance_activities_df):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated maintenance activities data\n",
    "    \n",
    "    Parameters:\n",
    "    - maintenance_activities_df: DataFrame containing maintenance activities data\n",
    "    \"\"\"\n",
    "    if maintenance_activities_df is None or len(maintenance_activities_df) == 0:\n",
    "        print(\"No maintenance activities data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nMaintenance Activities Statistics:\")\n",
    "    print(f\"Total maintenance activities: {len(maintenance_activities_df)}\")\n",
    "    \n",
    "    # Activity type distribution\n",
    "    print(\"\\nActivity Type Distribution:\")\n",
    "    type_counts = maintenance_activities_df['activity_type'].value_counts()\n",
    "    for activity_type, count in type_counts.items():\n",
    "        print(f\"  {activity_type}: {count} ({count/len(maintenance_activities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Status distribution\n",
    "    print(\"\\nStatus Distribution:\")\n",
    "    status_counts = maintenance_activities_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(maintenance_activities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Priority distribution\n",
    "    print(\"\\nPriority Distribution:\")\n",
    "    priority_counts = maintenance_activities_df['priority'].value_counts().sort_index()\n",
    "    for priority, count in priority_counts.items():\n",
    "        print(f\"  Priority {priority}: {count} ({count/len(maintenance_activities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Downtime required\n",
    "    downtime_required = maintenance_activities_df['downtime_required'].sum()\n",
    "    print(f\"\\nActivities requiring downtime: {downtime_required} ({downtime_required/len(maintenance_activities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Convert date columns to datetime for analysis\n",
    "    maintenance_activities_df['planned_start_date'] = pd.to_datetime(maintenance_activities_df['planned_start_date'], errors='coerce')\n",
    "    maintenance_activities_df['planned_end_date'] = pd.to_datetime(maintenance_activities_df['planned_end_date'], errors='coerce')\n",
    "    \n",
    "    # Filter out empty strings before conversion\n",
    "    maintenance_activities_df['actual_start_date'] = pd.to_datetime(\n",
    "        maintenance_activities_df['actual_start_date'].replace('', pd.NaT), errors='coerce'\n",
    "    )\n",
    "    maintenance_activities_df['actual_end_date'] = pd.to_datetime(\n",
    "        maintenance_activities_df['actual_end_date'].replace('', pd.NaT), errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Calculate planned duration\n",
    "    maintenance_activities_df['planned_duration_hours'] = (\n",
    "        maintenance_activities_df['planned_end_date'] - maintenance_activities_df['planned_start_date']\n",
    "    ).dt.total_seconds() / 3600\n",
    "    \n",
    "    print(\"\\nPlanned Duration Statistics:\")\n",
    "    print(f\"  Average planned duration: {maintenance_activities_df['planned_duration_hours'].mean():.1f} hours\")\n",
    "    print(f\"  Minimum planned duration: {maintenance_activities_df['planned_duration_hours'].min():.1f} hours\")\n",
    "    print(f\"  Maximum planned duration: {maintenance_activities_df['planned_duration_hours'].max():.1f} hours\")\n",
    "    \n",
    "    # Calculate actual duration for completed activities\n",
    "    completed = maintenance_activities_df[maintenance_activities_df['status'] == \"Completed\"]\n",
    "    \n",
    "    if len(completed) > 0:\n",
    "        completed['actual_duration_hours'] = (\n",
    "            completed['actual_end_date'] - completed['actual_start_date']\n",
    "        ).dt.total_seconds() / 3600\n",
    "        \n",
    "        print(\"\\nActual Duration Statistics (Completed Activities):\")\n",
    "        print(f\"  Average actual duration: {completed['actual_duration_hours'].mean():.1f} hours\")\n",
    "        print(f\"  Minimum actual duration: {completed['actual_duration_hours'].min():.1f} hours\")\n",
    "        print(f\"  Maximum actual duration: {completed['actual_duration_hours'].max():.1f} hours\")\n",
    "        \n",
    "        # Calculate duration variance\n",
    "        completed['duration_variance'] = completed['actual_duration_hours'] - completed['planned_duration_hours']\n",
    "        completed['duration_variance_pct'] = (completed['duration_variance'] / completed['planned_duration_hours']) * 100\n",
    "        \n",
    "        print(\"\\nDuration Variance (Completed Activities):\")\n",
    "        print(f\"  Average variance: {completed['duration_variance'].mean():.1f} hours\")\n",
    "        print(f\"  Average variance percentage: {completed['duration_variance_pct'].mean():.1f}%\")\n",
    "        \n",
    "        # Categorize activities by variance\n",
    "        on_time = len(completed[abs(completed['duration_variance_pct']) <= 10])\n",
    "        faster = len(completed[completed['duration_variance_pct'] < -10])\n",
    "        slower = len(completed[completed['duration_variance_pct'] > 10])\n",
    "        \n",
    "        print(f\"  On time (±10%): {on_time} ({on_time/len(completed)*100:.1f}%)\")\n",
    "        print(f\"  Faster than planned: {faster} ({faster/len(completed)*100:.1f}%)\")\n",
    "        print(f\"  Slower than planned: {slower} ({slower/len(completed)*100:.1f}%)\")\n",
    "    \n",
    "    # Time distribution\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    past_activities = maintenance_activities_df[maintenance_activities_df['planned_start_date'] < current_date]\n",
    "    future_activities = maintenance_activities_df[maintenance_activities_df['planned_start_date'] >= current_date]\n",
    "    \n",
    "    print(\"\\nTime Distribution:\")\n",
    "    print(f\"  Past activities: {len(past_activities)} ({len(past_activities)/len(maintenance_activities_df)*100:.1f}%)\")\n",
    "    print(f\"  Future activities: {len(future_activities)} ({len(future_activities)/len(maintenance_activities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Monthly distribution\n",
    "    maintenance_activities_df['month'] = maintenance_activities_df['planned_start_date'].dt.to_period('M')\n",
    "    monthly_counts = maintenance_activities_df['month'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nMonthly Distribution (top 6 months):\")\n",
    "    for month, count in monthly_counts.head(6).items():\n",
    "        print(f\"  {month}: {count} activities\")\n",
    "    \n",
    "    # Equipment distribution\n",
    "    equipment_counts = maintenance_activities_df['equipment_id'].value_counts()\n",
    "    \n",
    "    print(\"\\nTop 5 Equipment by Maintenance Frequency:\")\n",
    "    for equipment_id, count in equipment_counts.head(5).items():\n",
    "        print(f\"  {equipment_id}: {count} activities\")\n",
    "    \n",
    "    # Technician distribution\n",
    "    technician_counts = maintenance_activities_df['technician_id'].value_counts()\n",
    "    \n",
    "    print(\"\\nTop 5 Technicians by Activity Assignment:\")\n",
    "    for technician_id, count in technician_counts.head(5).items():\n",
    "        print(f\"  {technician_id}: {count} activities\")\n",
    "    \n",
    "    # Activity type by downtime requirement\n",
    "    print(\"\\nDowntime Requirement by Activity Type:\")\n",
    "    for activity_type in type_counts.index:\n",
    "        type_data = maintenance_activities_df[maintenance_activities_df['activity_type'] == activity_type]\n",
    "        downtime_count = type_data['downtime_required'].sum()\n",
    "        print(f\"  {activity_type}: {downtime_count}/{len(type_data)} ({downtime_count/len(type_data)*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    equipment_df = load_equipment_data()\n",
    "    work_orders_df = load_work_orders_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    \n",
    "    if equipment_df is not None:\n",
    "        # Generate maintenance activities data\n",
    "        maintenance_activities_df = generate_maintenance_activities(\n",
    "            equipment_df,\n",
    "            work_orders_df,\n",
    "            personnel_df,\n",
    "            num_activities=300,  # Generate 300 maintenance activity records\n",
    "            output_file=\"data/maintenance_activities.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if maintenance_activities_df is not None:\n",
    "            display_statistics(maintenance_activities_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample maintenance activities data (first 5 records):\")\n",
    "            print(maintenance_activities_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7b7e5",
   "metadata": {},
   "source": [
    "Production Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a43614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 1000 production performance records.\n",
      "Data saved to data/production_performance.csv\n",
      "\n",
      "Production Performance Statistics:\n",
      "Total performance records: 1000\n",
      "\n",
      "Time Period Distribution:\n",
      "  Hour: 1000 (100.0%)\n",
      "\n",
      "OEE Statistics:\n",
      "  Average OEE: 72.1%\n",
      "  Minimum OEE: 25.8%\n",
      "  Maximum OEE: 98.7%\n",
      "\n",
      "OEE Component Statistics:\n",
      "  Average Availability: 88.2%\n",
      "  Average Performance: 83.6%\n",
      "  Average Quality: 97.0%\n",
      "\n",
      "Production Statistics:\n",
      "  Total production: 75408 units\n",
      "  Total rejects: 1549 units\n",
      "  Overall reject rate: 2.05%\n",
      "  Average production per period: 75.4 units\n",
      "  Average rejects per period: 1.5 units\n",
      "\n",
      "Downtime Statistics:\n",
      "  Total downtime: 7089 minutes (118.2 hours)\n",
      "  Average downtime per period: 7.1 minutes\n",
      "\n",
      "Cycle Time Statistics:\n",
      "  Average cycle time: 109.3 seconds\n",
      "  Minimum cycle time: 53.0 seconds\n",
      "  Maximum cycle time: 202.0 seconds\n",
      "\n",
      "Unique equipment tracked: 150\n",
      "\n",
      "Top 5 Equipment by Production Volume:\n",
      "  EQ-6D152291: 1323 units\n",
      "  EQ-39407330: 1083 units\n",
      "  EQ-720A372C: 1026 units\n",
      "  EQ-EF7B7DD7: 1003 units\n",
      "  EQ-64B92860: 884 units\n",
      "\n",
      "Bottom 5 Equipment by Average OEE:\n",
      "  EQ-63431F60: 50.8%\n",
      "  EQ-1F51855E: 56.3%\n",
      "  EQ-68410A17: 56.7%\n",
      "  EQ-DEB311BD: 58.9%\n",
      "  EQ-DA1AB602: 60.8%\n",
      "\n",
      "Records with work order association: 807 (80.7%)\n",
      "\n",
      "Daily Average OEE (first 7 days):\n",
      "  2025-04-16: 73.4%\n",
      "  2025-04-17: 75.0%\n",
      "  2025-04-18: 75.8%\n",
      "  2025-04-19: 63.9%\n",
      "  2025-04-20: 70.1%\n",
      "  2025-04-21: 70.6%\n",
      "  2025-04-22: 73.5%\n",
      "\n",
      "Performance by Shift:\n",
      "  SHIFT-237E6E68:\n",
      "    OEE: 72.1%\n",
      "    Availability: 87.8%\n",
      "    Performance: 84.1%\n",
      "    Quality: 96.8%\n",
      "  SHIFT-41AC8959:\n",
      "    OEE: 72.2%\n",
      "    Availability: 88.6%\n",
      "    Performance: 83.3%\n",
      "    Quality: 96.9%\n",
      "  SHIFT-9386D241:\n",
      "    OEE: 71.8%\n",
      "    Availability: 88.4%\n",
      "    Performance: 83.2%\n",
      "    Quality: 97.1%\n",
      "  SHIFT-9950D819:\n",
      "    OEE: 72.3%\n",
      "    Availability: 88.0%\n",
      "    Performance: 83.9%\n",
      "    Quality: 97.2%\n",
      "\n",
      "Sample production performance data (first 5 records):\n",
      "  performance_id equipment_id work_order_id        shift_id  \\\n",
      "0  PERF-1D84C79D  EQ-3089140E   WO-1359E438  SHIFT-41AC8959   \n",
      "1  PERF-F039EF96  EQ-B498F03F   WO-5B176E6E  SHIFT-237E6E68   \n",
      "2  PERF-20F13236  EQ-277C7BA4                SHIFT-9950D819   \n",
      "3  PERF-04924A3B  EQ-277C7BA4   WO-195C1792  SHIFT-9386D241   \n",
      "4  PERF-55E2148E  EQ-422154D2   WO-87C97130  SHIFT-41AC8959   \n",
      "\n",
      "            timestamp time_period  availability_percent  performance_percent  \\\n",
      "0 2025-04-16 19:16:59        Hour                  92.5                 79.5   \n",
      "1 2025-04-16 20:16:59        Hour                  87.2                 77.9   \n",
      "2 2025-04-16 21:16:59        Hour                  93.9                 76.9   \n",
      "3 2025-04-16 22:16:59        Hour                  82.4                 85.0   \n",
      "4 2025-04-16 23:16:59        Hour                  95.6                 92.6   \n",
      "\n",
      "   quality_percent  oee_percent  production_count  reject_count  \\\n",
      "0            100.0         73.5                63             0   \n",
      "1             98.0         66.5                52             1   \n",
      "2             99.3         71.7                79             0   \n",
      "3             97.0         68.0                75             2   \n",
      "4             98.6         87.3                58             0   \n",
      "\n",
      "   downtime_minutes  cycle_time_seconds         day  \n",
      "0                 4                 125  2025-04-16  \n",
      "1                 8                 113  2025-04-16  \n",
      "2                 4                 102  2025-04-16  \n",
      "3                11                 129  2025-04-16  \n",
      "4                 3                 104  2025-04-16  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Please run the equipment data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_work_orders_data(work_orders_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated work orders data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_file: CSV file containing work orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the work orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(work_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Work orders data file {work_orders_file} not found.\")\n",
    "        print(\"Production performance will be generated with synthetic work order IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_maintenance_activities(maintenance_file=\"data/maintenance_activities.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated maintenance activities data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - maintenance_file: CSV file containing maintenance activities data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the maintenance activities data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(maintenance_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Maintenance activities file {maintenance_file} not found.\")\n",
    "        print(\"Production performance will not incorporate maintenance-related downtime.\")\n",
    "        return None\n",
    "\n",
    "def load_equipment_states(equipment_states_file=\"data/equipment_states.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment states data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_states_file: CSV file containing equipment states data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment states data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_states_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Equipment states file {equipment_states_file} not found.\")\n",
    "        print(\"Production performance will not incorporate equipment state history.\")\n",
    "        return None\n",
    "\n",
    "def generate_production_performance(equipment_df, work_orders_df=None, maintenance_df=None, equipment_states_df=None,\n",
    "                                  num_periods=1000, time_period=\"Hour\", \n",
    "                                  start_time=None, end_time=None,\n",
    "                                  output_file=\"data/production_performance.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the ProductionPerformance table from ISA-95 Level 3.\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_df: DataFrame containing equipment data\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - maintenance_df: DataFrame containing maintenance activities data (optional)\n",
    "    - equipment_states_df: DataFrame containing equipment states data (optional)\n",
    "    - num_periods: Number of performance records to generate\n",
    "    - time_period: Time period for metrics (Hour, Shift, Day, Week)\n",
    "    - start_time: Start time for performance data (defaults to 90 days ago)\n",
    "    - end_time: End time for performance data (defaults to now)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated production performance data\n",
    "    \"\"\"\n",
    "    if equipment_df is None or len(equipment_df) == 0:\n",
    "        print(\"Error: No equipment data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=90)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Determine time period duration in minutes\n",
    "    if time_period == \"Hour\":\n",
    "        period_minutes = 60\n",
    "    elif time_period == \"Shift\":\n",
    "        period_minutes = 480  # 8-hour shift\n",
    "    elif time_period == \"Day\":\n",
    "        period_minutes = 1440  # 24 hours\n",
    "    elif time_period == \"Week\":\n",
    "        period_minutes = 10080  # 7 days\n",
    "    else:\n",
    "        # Default to hourly\n",
    "        period_minutes = 60\n",
    "        time_period = \"Hour\"\n",
    "    \n",
    "    # Get work order IDs if available, otherwise generate synthetic ones\n",
    "    if work_orders_df is not None and len(work_orders_df) > 0:\n",
    "        # Filter for production work orders\n",
    "        production_wos = work_orders_df[work_orders_df['work_order_type'] == 'Production']\n",
    "        \n",
    "        if len(production_wos) > 0:\n",
    "            work_order_ids = production_wos['work_order_id'].tolist()\n",
    "        else:\n",
    "            # If no production work orders, generate synthetic ones\n",
    "            print(\"No production work orders found. Generating synthetic work order IDs...\")\n",
    "            work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    else:\n",
    "        # Generate synthetic work order IDs\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    \n",
    "    # Create shifts for time periods (used to maintain consistent shift IDs)\n",
    "    if time_period == \"Shift\":\n",
    "        # Create 3 shifts per day\n",
    "        shift_ids = [f\"SHIFT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(3)]\n",
    "    else:\n",
    "        # For non-shift time periods, create a few generic shift IDs\n",
    "        shift_ids = [f\"SHIFT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(4)]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"performance_id\": [f\"PERF-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_periods)],\n",
    "        \"equipment_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"shift_id\": [],\n",
    "        \"timestamp\": [],\n",
    "        \"time_period\": [],\n",
    "        \"availability_percent\": [],\n",
    "        \"performance_percent\": [],\n",
    "        \"quality_percent\": [],\n",
    "        \"oee_percent\": [],\n",
    "        \"production_count\": [],\n",
    "        \"reject_count\": [],\n",
    "        \"downtime_minutes\": [],\n",
    "        \"cycle_time_seconds\": []\n",
    "    }\n",
    "    \n",
    "    # Calculate time points for performance records\n",
    "    time_range_minutes = int((end_time - start_time).total_seconds() / 60)\n",
    "    \n",
    "    # Determine how many time periods we can fit in the range\n",
    "    max_periods = time_range_minutes // period_minutes\n",
    "    \n",
    "    if max_periods < num_periods:\n",
    "        print(f\"Warning: Time range can only fit {max_periods} {time_period} periods.\")\n",
    "        print(f\"Reducing requested number of periods from {num_periods} to {max_periods}.\")\n",
    "        num_periods = max_periods\n",
    "    \n",
    "    # Select random time points within the range\n",
    "    period_starts = []\n",
    "    \n",
    "    # If using hour or shift, ensure periods don't overlap\n",
    "    if time_period in [\"Hour\", \"Shift\"]:\n",
    "        # Create evenly spaced periods\n",
    "        for i in range(num_periods):\n",
    "            if i < max_periods:\n",
    "                # For periods that fit within the range, space them evenly\n",
    "                period_start = start_time + timedelta(minutes=i * period_minutes)\n",
    "                period_starts.append(period_start)\n",
    "            else:\n",
    "                # If we need more periods than the range allows, start reusing time points\n",
    "                # with different equipment\n",
    "                period_start = period_starts[i % max_periods]\n",
    "                period_starts.append(period_start)\n",
    "    else:\n",
    "        # For day and week, allow some overlap (different equipment can have records for the same day)\n",
    "        for _ in range(num_periods):\n",
    "            random_minutes = random.randint(0, time_range_minutes - period_minutes)\n",
    "            period_start = start_time + timedelta(minutes=random_minutes)\n",
    "            period_starts.append(period_start)\n",
    "    \n",
    "    # Define equipment categories for different performance profiles\n",
    "    # Extract equipment types if available, otherwise create synthetic categories\n",
    "    if 'equipment_type' in equipment_df.columns:\n",
    "        equipment_categories = equipment_df['equipment_type'].unique().tolist()\n",
    "    else:\n",
    "        equipment_categories = [\n",
    "            \"Production\", \"Packaging\", \"Assembly\", \"Processing\", \n",
    "            \"Machining\", \"Filling\", \"Testing\", \"Utility\"\n",
    "        ]\n",
    "    \n",
    "    # Create performance profiles by equipment category\n",
    "    performance_profiles = {}\n",
    "    \n",
    "    for category in equipment_categories:\n",
    "        # Base performance parameters\n",
    "        if category in [\"Production\", \"Processing\"]:\n",
    "            # Process equipment typically has high availability, variable performance\n",
    "            profile = {\n",
    "                \"availability\": {\"base\": 0.92, \"std\": 0.05},\n",
    "                \"performance\": {\"base\": 0.85, \"std\": 0.08},\n",
    "                \"quality\": {\"base\": 0.98, \"std\": 0.02},\n",
    "                \"cycle_time\": {\"base\": 120, \"std\": 20}\n",
    "            }\n",
    "        elif category in [\"Packaging\", \"Filling\"]:\n",
    "            # Packaging equipment has moderate availability, high performance\n",
    "            profile = {\n",
    "                \"availability\": {\"base\": 0.88, \"std\": 0.07},\n",
    "                \"performance\": {\"base\": 0.9, \"std\": 0.05},\n",
    "                \"quality\": {\"base\": 0.99, \"std\": 0.01},\n",
    "                \"cycle_time\": {\"base\": 30, \"std\": 5}\n",
    "            }\n",
    "        elif category in [\"Assembly\", \"Machining\"]:\n",
    "            # Assembly and machining have lower availability, high quality\n",
    "            profile = {\n",
    "                \"availability\": {\"base\": 0.85, \"std\": 0.08},\n",
    "                \"performance\": {\"base\": 0.8, \"std\": 0.1},\n",
    "                \"quality\": {\"base\": 0.995, \"std\": 0.005},\n",
    "                \"cycle_time\": {\"base\": 180, \"std\": 30}\n",
    "            }\n",
    "        elif category == \"Testing\":\n",
    "            # Testing equipment has high availability, consistent performance\n",
    "            profile = {\n",
    "                \"availability\": {\"base\": 0.95, \"std\": 0.03},\n",
    "                \"performance\": {\"base\": 0.9, \"std\": 0.04},\n",
    "                \"quality\": {\"base\": 0.999, \"std\": 0.001},\n",
    "                \"cycle_time\": {\"base\": 60, \"std\": 10}\n",
    "            }\n",
    "        else:\n",
    "            # Default profile\n",
    "            profile = {\n",
    "                \"availability\": {\"base\": 0.9, \"std\": 0.06},\n",
    "                \"performance\": {\"base\": 0.85, \"std\": 0.07},\n",
    "                \"quality\": {\"base\": 0.98, \"std\": 0.02},\n",
    "                \"cycle_time\": {\"base\": 90, \"std\": 15}\n",
    "            }\n",
    "        \n",
    "        performance_profiles[category] = profile\n",
    "    \n",
    "    # Default production rate by category (units per hour)\n",
    "    production_rates = {\n",
    "        \"Production\": {\"base\": 100, \"std\": 20},\n",
    "        \"Processing\": {\"base\": 120, \"std\": 25},\n",
    "        \"Packaging\": {\"base\": 500, \"std\": 50},\n",
    "        \"Filling\": {\"base\": 600, \"std\": 60},\n",
    "        \"Assembly\": {\"base\": 80, \"std\": 15},\n",
    "        \"Machining\": {\"base\": 40, \"std\": 10},\n",
    "        \"Testing\": {\"base\": 200, \"std\": 30},\n",
    "        \"Utility\": {\"base\": 50, \"std\": 10}\n",
    "    }\n",
    "    \n",
    "    # Adjust production rate for different time periods\n",
    "    if time_period == \"Shift\":\n",
    "        period_factor = 8  # 8 hours per shift\n",
    "    elif time_period == \"Day\":\n",
    "        period_factor = 24  # 24 hours per day\n",
    "    elif time_period == \"Week\":\n",
    "        period_factor = 168  # 168 hours per week\n",
    "    else:\n",
    "        period_factor = 1  # default for Hour\n",
    "    \n",
    "    # Generate performance data for each period\n",
    "    for i in range(num_periods):\n",
    "        # Select equipment (with replacement)\n",
    "        equipment = equipment_df.sample(1).iloc[0]\n",
    "        equipment_id = equipment['equipment_id']\n",
    "        \n",
    "        # Get equipment category\n",
    "        if 'equipment_type' in equipment.index:\n",
    "            category = equipment['equipment_type']\n",
    "        else:\n",
    "            category = random.choice(equipment_categories)\n",
    "        \n",
    "        data[\"equipment_id\"].append(equipment_id)\n",
    "        \n",
    "        # Assign work order (80% of records have work orders)\n",
    "        if random.random() < 0.8:\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        else:\n",
    "            data[\"work_order_id\"].append(\"\")\n",
    "        \n",
    "        # Assign shift ID based on time of day\n",
    "        timestamp = period_starts[i]\n",
    "        hour_of_day = timestamp.hour\n",
    "        \n",
    "        if time_period == \"Shift\":\n",
    "            # Morning shift (6am-2pm), Afternoon shift (2pm-10pm), Night shift (10pm-6am)\n",
    "            if 6 <= hour_of_day < 14:\n",
    "                shift_id = shift_ids[0]  # Morning shift\n",
    "            elif 14 <= hour_of_day < 22:\n",
    "                shift_id = shift_ids[1]  # Afternoon shift\n",
    "            else:\n",
    "                shift_id = shift_ids[2]  # Night shift\n",
    "        else:\n",
    "            # For non-shift periods, use a simple rotation\n",
    "            shift_idx = i % len(shift_ids)\n",
    "            shift_id = shift_ids[shift_idx]\n",
    "        \n",
    "        data[\"shift_id\"].append(shift_id)\n",
    "        \n",
    "        # Set timestamp\n",
    "        data[\"timestamp\"].append(timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Set time period\n",
    "        data[\"time_period\"].append(time_period)\n",
    "        \n",
    "        # Get performance profile for this equipment category\n",
    "        if category in performance_profiles:\n",
    "            profile = performance_profiles[category]\n",
    "        else:\n",
    "            # Use default profile if category not found\n",
    "            profile = performance_profiles[equipment_categories[0]]\n",
    "        \n",
    "        # Generate performance metrics with some time correlation\n",
    "        # This creates more realistic patterns where consecutive periods have similar performance\n",
    "        \n",
    "        # Determine if this is a \"bad day\" (10% chance)\n",
    "        bad_day = random.random() < 0.1\n",
    "        \n",
    "        # Generate availability percentage\n",
    "        if bad_day:\n",
    "            # Lower availability on \"bad days\"\n",
    "            availability = max(0, min(100, 100 * random.normalvariate(\n",
    "                profile[\"availability\"][\"base\"] * 0.7,  # 30% reduction on bad days\n",
    "                profile[\"availability\"][\"std\"]\n",
    "            )))\n",
    "        else:\n",
    "            availability = max(0, min(100, 100 * random.normalvariate(\n",
    "                profile[\"availability\"][\"base\"],\n",
    "                profile[\"availability\"][\"std\"]\n",
    "            )))\n",
    "        \n",
    "        # Generate performance percentage\n",
    "        if bad_day:\n",
    "            # Lower performance on \"bad days\"\n",
    "            performance = max(0, min(100, 100 * random.normalvariate(\n",
    "                profile[\"performance\"][\"base\"] * 0.8,  # 20% reduction on bad days\n",
    "                profile[\"performance\"][\"std\"]\n",
    "            )))\n",
    "        else:\n",
    "            performance = max(0, min(100, 100 * random.normalvariate(\n",
    "                profile[\"performance\"][\"base\"],\n",
    "                profile[\"performance\"][\"std\"]\n",
    "            )))\n",
    "        \n",
    "        # Generate quality percentage\n",
    "        if bad_day:\n",
    "            # Lower quality on \"bad days\"\n",
    "            quality = max(0, min(100, 100 * random.normalvariate(\n",
    "                profile[\"quality\"][\"base\"] * 0.9,  # 10% reduction on bad days\n",
    "                profile[\"quality\"][\"std\"] * 1.5  # More variability on bad days\n",
    "            )))\n",
    "        else:\n",
    "            quality = max(0, min(100, 100 * random.normalvariate(\n",
    "                profile[\"quality\"][\"base\"],\n",
    "                profile[\"quality\"][\"std\"]\n",
    "            )))\n",
    "        \n",
    "        # Calculate OEE (Overall Equipment Effectiveness)\n",
    "        oee = (availability * performance * quality) / 10000  # Convert from percentage\n",
    "        \n",
    "        # Round metrics to 1 decimal place\n",
    "        availability = round(availability, 1)\n",
    "        performance = round(performance, 1)\n",
    "        quality = round(quality, 1)\n",
    "        oee = round(oee, 1)\n",
    "        \n",
    "        data[\"availability_percent\"].append(availability)\n",
    "        data[\"performance_percent\"].append(performance)\n",
    "        data[\"quality_percent\"].append(quality)\n",
    "        data[\"oee_percent\"].append(oee)\n",
    "        \n",
    "        # Calculate production counts based on availability, performance, and time period\n",
    "        if category in production_rates:\n",
    "            base_rate = production_rates[category][\"base\"]\n",
    "            rate_std = production_rates[category][\"std\"]\n",
    "        else:\n",
    "            base_rate = 100\n",
    "            rate_std = 20\n",
    "        \n",
    "        # Adjust production rate based on performance\n",
    "        rate_factor = performance / 100\n",
    "        \n",
    "        # Add some random variation\n",
    "        production_rate = random.normalvariate(base_rate * rate_factor, rate_std * rate_factor)\n",
    "        \n",
    "        # Scale by time period\n",
    "        production_count = int(production_rate * period_factor * (availability / 100))\n",
    "        \n",
    "        # Calculate rejects based on quality percentage\n",
    "        reject_rate = 1 - (quality / 100)\n",
    "        reject_count = int(production_count * reject_rate)\n",
    "        \n",
    "        # Adjust production count to be gross production (including rejects)\n",
    "        production_count += reject_count\n",
    "        \n",
    "        data[\"production_count\"].append(production_count)\n",
    "        data[\"reject_count\"].append(reject_count)\n",
    "        \n",
    "        # Calculate downtime based on availability\n",
    "        downtime_minutes = period_minutes * (1 - availability / 100)\n",
    "        data[\"downtime_minutes\"].append(round(downtime_minutes))\n",
    "        \n",
    "        # Calculate cycle time\n",
    "        base_cycle_time = profile[\"cycle_time\"][\"base\"]\n",
    "        cycle_std = profile[\"cycle_time\"][\"std\"]\n",
    "        \n",
    "        # Adjust cycle time based on performance (lower performance = higher cycle time)\n",
    "        cycle_factor = 100 / performance\n",
    "        cycle_time = random.normalvariate(base_cycle_time * cycle_factor, cycle_std)\n",
    "        \n",
    "        # Ensure cycle time is positive\n",
    "        cycle_time = max(1, round(cycle_time))\n",
    "        \n",
    "        data[\"cycle_time_seconds\"].append(cycle_time)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} production performance records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(production_performance_df):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated production performance data\n",
    "    \n",
    "    Parameters:\n",
    "    - production_performance_df: DataFrame containing production performance data\n",
    "    \"\"\"\n",
    "    if production_performance_df is None or len(production_performance_df) == 0:\n",
    "        print(\"No production performance data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nProduction Performance Statistics:\")\n",
    "    print(f\"Total performance records: {len(production_performance_df)}\")\n",
    "    \n",
    "    # Time period distribution\n",
    "    print(\"\\nTime Period Distribution:\")\n",
    "    period_counts = production_performance_df['time_period'].value_counts()\n",
    "    for period, count in period_counts.items():\n",
    "        print(f\"  {period}: {count} ({count/len(production_performance_df)*100:.1f}%)\")\n",
    "    \n",
    "    # OEE statistics\n",
    "    print(\"\\nOEE Statistics:\")\n",
    "    print(f\"  Average OEE: {production_performance_df['oee_percent'].mean():.1f}%\")\n",
    "    print(f\"  Minimum OEE: {production_performance_df['oee_percent'].min():.1f}%\")\n",
    "    print(f\"  Maximum OEE: {production_performance_df['oee_percent'].max():.1f}%\")\n",
    "    \n",
    "    # OEE component statistics\n",
    "    print(\"\\nOEE Component Statistics:\")\n",
    "    print(f\"  Average Availability: {production_performance_df['availability_percent'].mean():.1f}%\")\n",
    "    print(f\"  Average Performance: {production_performance_df['performance_percent'].mean():.1f}%\")\n",
    "    print(f\"  Average Quality: {production_performance_df['quality_percent'].mean():.1f}%\")\n",
    "    \n",
    "    # Production and reject statistics\n",
    "    total_production = production_performance_df['production_count'].sum()\n",
    "    total_rejects = production_performance_df['reject_count'].sum()\n",
    "    reject_rate = (total_rejects / total_production) * 100 if total_production > 0 else 0\n",
    "    \n",
    "    print(\"\\nProduction Statistics:\")\n",
    "    print(f\"  Total production: {total_production} units\")\n",
    "    print(f\"  Total rejects: {total_rejects} units\")\n",
    "    print(f\"  Overall reject rate: {reject_rate:.2f}%\")\n",
    "    print(f\"  Average production per period: {production_performance_df['production_count'].mean():.1f} units\")\n",
    "    print(f\"  Average rejects per period: {production_performance_df['reject_count'].mean():.1f} units\")\n",
    "    \n",
    "    # Downtime statistics\n",
    "    total_downtime = production_performance_df['downtime_minutes'].sum()\n",
    "    avg_downtime = production_performance_df['downtime_minutes'].mean()\n",
    "    \n",
    "    print(\"\\nDowntime Statistics:\")\n",
    "    print(f\"  Total downtime: {total_downtime} minutes ({total_downtime/60:.1f} hours)\")\n",
    "    print(f\"  Average downtime per period: {avg_downtime:.1f} minutes\")\n",
    "    \n",
    "    # Cycle time statistics\n",
    "    print(\"\\nCycle Time Statistics:\")\n",
    "    print(f\"  Average cycle time: {production_performance_df['cycle_time_seconds'].mean():.1f} seconds\")\n",
    "    print(f\"  Minimum cycle time: {production_performance_df['cycle_time_seconds'].min():.1f} seconds\")\n",
    "    print(f\"  Maximum cycle time: {production_performance_df['cycle_time_seconds'].max():.1f} seconds\")\n",
    "    \n",
    "    # Equipment statistics\n",
    "    equipment_count = production_performance_df['equipment_id'].nunique()\n",
    "    print(f\"\\nUnique equipment tracked: {equipment_count}\")\n",
    "    \n",
    "    # Top 5 equipment by production volume\n",
    "    equipment_production = production_performance_df.groupby('equipment_id')['production_count'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Equipment by Production Volume:\")\n",
    "    for equipment_id, production in equipment_production.head(5).items():\n",
    "        print(f\"  {equipment_id}: {production} units\")\n",
    "    \n",
    "    # Bottom 5 equipment by OEE\n",
    "    equipment_oee = production_performance_df.groupby('equipment_id')['oee_percent'].mean().sort_values()\n",
    "    \n",
    "    print(\"\\nBottom 5 Equipment by Average OEE:\")\n",
    "    for equipment_id, oee in equipment_oee.head(5).items():\n",
    "        print(f\"  {equipment_id}: {oee:.1f}%\")\n",
    "    \n",
    "    # Work order statistics\n",
    "    wo_count = production_performance_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "    wo_percentage = (wo_count / len(production_performance_df)) * 100\n",
    "    \n",
    "    print(f\"\\nRecords with work order association: {wo_count} ({wo_percentage:.1f}%)\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    production_performance_df['timestamp'] = pd.to_datetime(production_performance_df['timestamp'])\n",
    "    \n",
    "    # Group by day\n",
    "    production_performance_df['day'] = production_performance_df['timestamp'].dt.date\n",
    "    daily_oee = production_performance_df.groupby('day')['oee_percent'].mean()\n",
    "    \n",
    "    print(\"\\nDaily Average OEE (first 7 days):\")\n",
    "    for day, oee in daily_oee.head(7).items():\n",
    "        print(f\"  {day}: {oee:.1f}%\")\n",
    "    \n",
    "    # Shift performance comparison\n",
    "    shift_performance = production_performance_df.groupby('shift_id')[\n",
    "        ['oee_percent', 'availability_percent', 'performance_percent', 'quality_percent']\n",
    "    ].mean()\n",
    "    \n",
    "    print(\"\\nPerformance by Shift:\")\n",
    "    for shift_id, metrics in shift_performance.iterrows():\n",
    "        print(f\"  {shift_id}:\")\n",
    "        print(f\"    OEE: {metrics['oee_percent']:.1f}%\")\n",
    "        print(f\"    Availability: {metrics['availability_percent']:.1f}%\")\n",
    "        print(f\"    Performance: {metrics['performance_percent']:.1f}%\")\n",
    "        print(f\"    Quality: {metrics['quality_percent']:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    equipment_df = load_equipment_data()\n",
    "    work_orders_df = load_work_orders_data()\n",
    "    maintenance_df = load_maintenance_activities()\n",
    "    equipment_states_df = load_equipment_states()\n",
    "    \n",
    "    if equipment_df is not None:\n",
    "        # Generate production performance data\n",
    "        production_performance_df = generate_production_performance(\n",
    "            equipment_df,\n",
    "            work_orders_df,\n",
    "            maintenance_df,\n",
    "            equipment_states_df,\n",
    "            num_periods=1000,  # Generate 1000 performance records\n",
    "            time_period=\"Hour\",  # Hourly metrics\n",
    "            output_file=\"data/production_performance.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if production_performance_df is not None:\n",
    "            display_statistics(production_performance_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample production performance data (first 5 records):\")\n",
    "            print(production_performance_df.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
