{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8606d7",
   "metadata": {},
   "source": [
    " Level 4: Business Planning & Logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228f017",
   "metadata": {},
   "source": [
    "Products & Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d938aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 product records.\n",
      "Data saved to data/products.csv\n",
      "Successfully generated 150 material records.\n",
      "Data saved to data/materials.csv\n",
      "\n",
      "Products Statistics:\n",
      "Total products: 100\n",
      "\n",
      "Product Family Distribution:\n",
      "  Automotive: 24 (24.0%)\n",
      "  Consumer Goods: 19 (19.0%)\n",
      "  Food & Beverage: 16 (16.0%)\n",
      "  Pharmaceutical: 15 (15.0%)\n",
      "  Electronics: 15 (15.0%)\n",
      "  Chemical: 11 (11.0%)\n",
      "\n",
      "Product Status Distribution:\n",
      "  Active: 69 (69.0%)\n",
      "  In Development: 12 (12.0%)\n",
      "  Obsolete: 11 (11.0%)\n",
      "  Discontinued: 5 (5.0%)\n",
      "  On Hold: 3 (3.0%)\n",
      "\n",
      "Units of Measure Distribution:\n",
      "  unit: 20 (20.0%)\n",
      "  pallet: 8 (8.0%)\n",
      "  assembly: 8 (8.0%)\n",
      "  piece: 8 (8.0%)\n",
      "  kit: 7 (7.0%)\n",
      "  pack: 7 (7.0%)\n",
      "  set: 6 (6.0%)\n",
      "  ml: 6 (6.0%)\n",
      "  g: 4 (4.0%)\n",
      "  L: 4 (4.0%)\n",
      "\n",
      "Price Statistics:\n",
      "  Average base cost: $246.03\n",
      "  Average list price: $471.71\n",
      "  Average markup: 1.89x\n",
      "\n",
      "Shelf Life Statistics:\n",
      "  Average shelf life: 1673.8 days (4.6 years)\n",
      "  Minimum shelf life: 64 days (0.2 years)\n",
      "  Maximum shelf life: 3620 days (9.9 years)\n",
      "\n",
      "Product Age Statistics:\n",
      "  Average product age: 1708.8 days (4.7 years)\n",
      "  Newest product: 7 days old\n",
      "  Oldest product: 3515 days old (9.6 years)\n",
      "\n",
      "Products with parent: 15 (15.0%)\n",
      "\n",
      "Materials Statistics:\n",
      "Total materials: 150\n",
      "\n",
      "Material Type Distribution:\n",
      "  Raw Material: 56 (37.3%)\n",
      "  Packaging: 32 (21.3%)\n",
      "  WIP: 23 (15.3%)\n",
      "  Intermediate: 20 (13.3%)\n",
      "  Consumable: 19 (12.7%)\n",
      "\n",
      "Material Status Distribution:\n",
      "  Active: 127 (84.7%)\n",
      "  Discontinued: 8 (5.3%)\n",
      "  On Hold: 7 (4.7%)\n",
      "  Obsolete: 6 (4.0%)\n",
      "  Pending Approval: 2 (1.3%)\n",
      "\n",
      "Cost Statistics:\n",
      "  Average standard cost: $301.45\n",
      "  Minimum standard cost: $0.52\n",
      "  Maximum standard cost: $4387.08\n",
      "\n",
      "Lead Time Statistics:\n",
      "  Average lead time: 40.6 days\n",
      "  Minimum lead time: 7 days\n",
      "  Maximum lead time: 120 days\n",
      "\n",
      "Hazard Classification Distribution:\n",
      "  Non-Hazardous: 73 (48.7%)\n",
      "  None: 22 (14.7%)\n",
      "  Flammable: 13 (8.7%)\n",
      "  Irritant: 11 (7.3%)\n",
      "  Toxic: 9 (6.0%)\n",
      "  Environmentally Hazardous: 8 (5.3%)\n",
      "  Corrosive: 5 (3.3%)\n",
      "  Explosive: 4 (2.7%)\n",
      "  Oxidizer: 2 (1.3%)\n",
      "  Carcinogen: 2 (1.3%)\n",
      "  Pressurized Gas: 1 (0.7%)\n",
      "\n",
      "Average approved suppliers per material: 2.0\n",
      "\n",
      "Sample products data (first 5 records):\n",
      "      product_id           product_name product_code   product_family  \\\n",
      "0  PROD-BCD04292  Personal Care Max 622       CPC622   Consumer Goods   \n",
      "1  PROD-88D9DAF5      Cream Premium 407        PC407   Pharmaceutical   \n",
      "2  PROD-7ACC7C5D         Engine Pro 592        AE592       Automotive   \n",
      "3  PROD-5B8E29A6    Cooling Premium 121        AC121       Automotive   \n",
      "4  PROD-E0ED1A01        Dried Ultra 604        FD604  Food & Beverage   \n",
      "\n",
      "                                         description unit_of_measure  \\\n",
      "0             Standard personal care for general use          pallet   \n",
      "1                     Standard cream for general use         capsule   \n",
      "2                     Cost-effective engine solution             kit   \n",
      "3  Industrial grade cooling for professional appl...             set   \n",
      "4                     Standard dried for general use              ml   \n",
      "\n",
      "           status introduction_date discontinuation_date revision  base_cost  \\\n",
      "0        Obsolete        2017-07-17           2025-01-26      3.6      16.56   \n",
      "1        Obsolete        2022-02-11           2022-09-13      3.1     369.28   \n",
      "2         On Hold        2018-07-23                           1.6     250.90   \n",
      "3        Obsolete        2022-03-07           2022-07-05      1.4      75.67   \n",
      "4  In Development        2024-12-24                           3.9      15.26   \n",
      "\n",
      "   list_price  shelf_life_days   storage_requirements parent_product_id  \\\n",
      "0       35.33              935  Fragile Item Handling     PROD-36292BDD   \n",
      "1      542.47              625         Frozen (-20°C)     PROD-6590FF6A   \n",
      "2      387.43             2851     Climate Controlled                     \n",
      "3      186.17              869     Climate Controlled                     \n",
      "4       37.34              357   Protected from Light                     \n",
      "\n",
      "   age_days  \n",
      "0      2921  \n",
      "1      1251  \n",
      "2      2550  \n",
      "3      1227  \n",
      "4       204  \n",
      "\n",
      "Sample materials data (first 5 records):\n",
      "    material_id         material_name material_type  \\\n",
      "0  MAT-C06E3CCC      Colorant USP 630           WIP   \n",
      "1  MAT-853907EB      Base Premium 925     Packaging   \n",
      "2  MAT-E12A1BAC         Bottle EP 519     Packaging   \n",
      "3  MAT-EA895957  Insert Technical 490  Intermediate   \n",
      "4  MAT-CDB4D157     Wood Standard 610  Raw Material   \n",
      "\n",
      "                                         description unit_of_measure  \\\n",
      "0            USP grade colorant for wip applications           batch   \n",
      "1                      Standard base for general use            roll   \n",
      "2      Industrial bottle for manufacturing processes           piece   \n",
      "3                    Standard insert for general use           batch   \n",
      "4  Standard grade wood for raw material applications             ton   \n",
      "\n",
      "   standard_cost  lead_time_days  minimum_order_quantity  \\\n",
      "0         655.75              12                      25   \n",
      "1           0.52              26                      50   \n",
      "2          18.88              47                      25   \n",
      "3         494.93              20                      10   \n",
      "4         427.79              86                    1000   \n",
      "\n",
      "                                 approved_suppliers  safety_stock  \\\n",
      "0                                  ['SUP-4D930C4F']         58.86   \n",
      "1                                  ['SUP-BB76A82C']        122.24   \n",
      "2  ['SUP-7C9F481C', 'SUP-F5580FB9', 'SUP-09FC1AC0']         50.84   \n",
      "3                  ['SUP-5C5B3567', 'SUP-B41745E3']         30.20   \n",
      "4  ['SUP-09FC1AC0', 'SUP-D570E12B', 'SUP-071D81D2']      26014.95   \n",
      "\n",
      "   reorder_point    status       storage_requirements hazard_classification  \n",
      "0         114.58    Active       Refrigerated (2-8°C)         Non-Hazardous  \n",
      "1         199.06  Obsolete       Protected from Light             Explosive  \n",
      "2         120.56    Active        Protected from Dust              Irritant  \n",
      "3          46.34    Active    Protected from Moisture         Non-Hazardous  \n",
      "4       46395.32    Active  Protected from Corrosives         Non-Hazardous  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_products_data(num_products=100, output_file=\"data/products.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the Products table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_products: Number of product records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated products data\n",
    "    \"\"\"\n",
    "    # Define product categories and types\n",
    "    product_categories = {\n",
    "        \"Pharmaceutical\": [\n",
    "            \"Tablet\", \"Capsule\", \"Liquid\", \"Injection\", \"Cream\", \"Ointment\", \n",
    "            \"Inhaler\", \"Patch\", \"Suppository\", \"Suspension\"\n",
    "        ],\n",
    "        \"Food & Beverage\": [\n",
    "            \"Dairy\", \"Bakery\", \"Beverage\", \"Snack\", \"Frozen\", \"Canned\", \n",
    "            \"Condiment\", \"Ready Meal\", \"Confectionery\", \"Dried\"\n",
    "        ],\n",
    "        \"Chemical\": [\n",
    "            \"Industrial\", \"Agricultural\", \"Cleaning\", \"Coating\", \"Adhesive\", \n",
    "            \"Solvent\", \"Catalyst\", \"Resin\", \"Pigment\", \"Additive\"\n",
    "        ],\n",
    "        \"Electronics\": [\n",
    "            \"Consumer\", \"Industrial\", \"Component\", \"Circuit Board\", \"Sensor\", \n",
    "            \"Power Supply\", \"Display\", \"Memory\", \"Processor\", \"Controller\"\n",
    "        ],\n",
    "        \"Automotive\": [\n",
    "            \"Engine\", \"Transmission\", \"Chassis\", \"Interior\", \"Exterior\", \n",
    "            \"Electrical\", \"Cooling\", \"Fuel\", \"Suspension\", \"Braking\"\n",
    "        ],\n",
    "        \"Consumer Goods\": [\n",
    "            \"Personal Care\", \"Household\", \"Paper\", \"Textile\", \"Appliance\", \n",
    "            \"Furniture\", \"Toy\", \"Sporting\", \"Tool\", \"Office\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define units of measurement by category\n",
    "    units_of_measure = {\n",
    "        \"Pharmaceutical\": [\"mg\", \"g\", \"ml\", \"tablet\", \"capsule\", \"vial\", \"patch\", \"unit\"],\n",
    "        \"Food & Beverage\": [\"g\", \"kg\", \"ml\", \"L\", \"oz\", \"lb\", \"piece\", \"pack\"],\n",
    "        \"Chemical\": [\"g\", \"kg\", \"L\", \"gal\", \"ton\", \"drum\", \"pallet\", \"IBC\"],\n",
    "        \"Electronics\": [\"unit\", \"piece\", \"kit\", \"set\", \"assembly\", \"module\"],\n",
    "        \"Automotive\": [\"unit\", \"piece\", \"kit\", \"set\", \"assembly\"],\n",
    "        \"Consumer Goods\": [\"unit\", \"piece\", \"pack\", \"box\", \"case\", \"pallet\"]\n",
    "    }\n",
    "    \n",
    "    # Define status options\n",
    "    product_statuses = [\"Active\", \"In Development\", \"Obsolete\", \"On Hold\", \"Discontinued\"]\n",
    "    status_weights = [0.7, 0.1, 0.1, 0.05, 0.05]  # Mostly active products\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"product_id\": [f\"PROD-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_products)],\n",
    "        \"product_name\": [],\n",
    "        \"product_code\": [],\n",
    "        \"product_family\": [],\n",
    "        \"description\": [],\n",
    "        \"unit_of_measure\": [],\n",
    "        \"status\": [],\n",
    "        \"introduction_date\": [],\n",
    "        \"discontinuation_date\": [],\n",
    "        \"revision\": [],\n",
    "        \"base_cost\": [],\n",
    "        \"list_price\": [],\n",
    "        \"shelf_life_days\": [],\n",
    "        \"storage_requirements\": [],\n",
    "        \"parent_product_id\": []\n",
    "    }\n",
    "    \n",
    "    # Define storage requirements by category\n",
    "    storage_requirements = {\n",
    "        \"Pharmaceutical\": [\n",
    "            \"Room Temperature (15-25°C)\", \"Refrigerated (2-8°C)\", \"Frozen (-20°C)\", \n",
    "            \"Cold Chain\", \"Controlled Room Temperature (20-25°C)\", \"Protected from Light\",\n",
    "            \"Protected from Moisture\", \"Controlled Humidity (<60% RH)\"\n",
    "        ],\n",
    "        \"Food & Beverage\": [\n",
    "            \"Room Temperature\", \"Refrigerated\", \"Frozen\", \"Cool and Dry\", \n",
    "            \"Protected from Light\", \"Protected from Moisture\", \"Ambient\"\n",
    "        ],\n",
    "        \"Chemical\": [\n",
    "            \"Room Temperature\", \"Controlled Temperature\", \"Flammable Storage\", \n",
    "            \"Acid Cabinet\", \"Base Cabinet\", \"Ventilated Area\", \"Away from Incompatibles\",\n",
    "            \"Protected from Moisture\", \"Protected from Light\"\n",
    "        ],\n",
    "        \"Electronics\": [\n",
    "            \"ESD Protected\", \"Temperature Controlled\", \"Humidity Controlled\", \n",
    "            \"Dust-Free Environment\", \"Standard Warehouse\", \"Clean Room\"\n",
    "        ],\n",
    "        \"Automotive\": [\n",
    "            \"Standard Warehouse\", \"Climate Controlled\", \"Covered Storage\", \n",
    "            \"Protected from Moisture\", \"Protected from Corrosives\"\n",
    "        ],\n",
    "        \"Consumer Goods\": [\n",
    "            \"Standard Warehouse\", \"Temperature Controlled\", \"Humidity Controlled\", \n",
    "            \"Stack Limit\", \"Heavy Item Storage\", \"Fragile Item Handling\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Track products for potential parent-child relationships\n",
    "    all_products = data[\"product_id\"].copy()\n",
    "    potential_parents = random.sample(all_products, int(len(all_products) * 0.2))  # 20% can be parents\n",
    "    \n",
    "    # Generate data for each product\n",
    "    for i in range(num_products):\n",
    "        # Select product category\n",
    "        category = random.choice(list(product_categories.keys()))\n",
    "        \n",
    "        # Select product type within category\n",
    "        product_type = random.choice(product_categories[category])\n",
    "        \n",
    "        # Set product family (category)\n",
    "        data[\"product_family\"].append(category)\n",
    "        \n",
    "        # Generate product name and code\n",
    "        product_series = random.choice([\"Pro\", \"Elite\", \"Standard\", \"Premium\", \"Ultra\", \"Max\", \"Advanced\", \"Basic\"])\n",
    "        product_number = random.randint(100, 999)\n",
    "        \n",
    "        product_name = f\"{product_type} {product_series} {product_number}\"\n",
    "        data[\"product_name\"].append(product_name)\n",
    "        \n",
    "        # Generate product code (category abbreviation + type abbreviation + number)\n",
    "        category_abbr = category[0].upper()\n",
    "        type_abbr = ''.join([word[0].upper() for word in product_type.split()])\n",
    "        product_code = f\"{category_abbr}{type_abbr}{product_number}\"\n",
    "        data[\"product_code\"].append(product_code)\n",
    "        \n",
    "        # Generate description\n",
    "        descriptions = [\n",
    "            f\"Standard {product_type.lower()} for general use\",\n",
    "            f\"Premium quality {product_type.lower()} with enhanced features\",\n",
    "            f\"Industrial grade {product_type.lower()} for professional applications\",\n",
    "            f\"Cost-effective {product_type.lower()} solution\",\n",
    "            f\"High-performance {product_type.lower()} designed for demanding environments\"\n",
    "        ]\n",
    "        data[\"description\"].append(random.choice(descriptions))\n",
    "        \n",
    "        # Select unit of measure\n",
    "        if category in units_of_measure:\n",
    "            unit = random.choice(units_of_measure[category])\n",
    "        else:\n",
    "            unit = \"unit\"\n",
    "        data[\"unit_of_measure\"].append(unit)\n",
    "        \n",
    "        # Set status (weighted random)\n",
    "        status = random.choices(product_statuses, weights=status_weights)[0]\n",
    "        data[\"status\"].append(status)\n",
    "        \n",
    "        # Generate introduction date (between 10 years ago and now)\n",
    "        intro_days_ago = random.randint(0, 3650)\n",
    "        intro_date = datetime.now() - timedelta(days=intro_days_ago)\n",
    "        data[\"introduction_date\"].append(intro_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Generate discontinuation date (only for obsolete or discontinued products)\n",
    "        if status in [\"Obsolete\", \"Discontinued\"]:\n",
    "            # Discontinuation date is after introduction but before now\n",
    "            min_disc_days_ago = min(intro_days_ago - 1, 1)  # At least 1 day after intro\n",
    "            disc_days_ago = random.randint(min_disc_days_ago, intro_days_ago - 1)\n",
    "            disc_date = datetime.now() - timedelta(days=disc_days_ago)\n",
    "            data[\"discontinuation_date\"].append(disc_date.strftime(\"%Y-%m-%d\"))\n",
    "        else:\n",
    "            data[\"discontinuation_date\"].append(\"\")\n",
    "        \n",
    "        # Generate revision (format: 1.0, 1.1, 2.0, etc.)\n",
    "        major_revision = random.randint(1, 3)\n",
    "        minor_revision = random.randint(0, 9)\n",
    "        data[\"revision\"].append(f\"{major_revision}.{minor_revision}\")\n",
    "        \n",
    "        # Generate cost and price\n",
    "        # Different price ranges for different categories\n",
    "        if category == \"Pharmaceutical\":\n",
    "            base_cost = random.uniform(5, 500)\n",
    "        elif category == \"Food & Beverage\":\n",
    "            base_cost = random.uniform(1, 50)\n",
    "        elif category == \"Chemical\":\n",
    "            base_cost = random.uniform(10, 200)\n",
    "        elif category == \"Electronics\":\n",
    "            base_cost = random.uniform(20, 1000)\n",
    "        elif category == \"Automotive\":\n",
    "            base_cost = random.uniform(15, 800)\n",
    "        else:  # Consumer Goods\n",
    "            base_cost = random.uniform(2, 100)\n",
    "        \n",
    "        data[\"base_cost\"].append(round(base_cost, 2))\n",
    "        \n",
    "        # List price is typically cost + markup\n",
    "        markup = random.uniform(1.2, 2.5)  # 20% to 150% markup\n",
    "        list_price = base_cost * markup\n",
    "        data[\"list_price\"].append(round(list_price, 2))\n",
    "        \n",
    "        # Generate shelf life\n",
    "        # Different shelf life ranges for different categories\n",
    "        if category == \"Pharmaceutical\":\n",
    "            shelf_life = random.randint(365, 1825)  # 1-5 years\n",
    "        elif category == \"Food & Beverage\":\n",
    "            shelf_life = random.randint(30, 730)  # 1 month to 2 years\n",
    "        elif category == \"Chemical\":\n",
    "            shelf_life = random.randint(365, 3650)  # 1-10 years\n",
    "        elif category in [\"Electronics\", \"Automotive\", \"Consumer Goods\"]:\n",
    "            shelf_life = random.randint(730, 3650)  # 2-10 years\n",
    "        else:\n",
    "            shelf_life = random.randint(365, 1825)  # 1-5 years\n",
    "        \n",
    "        data[\"shelf_life_days\"].append(shelf_life)\n",
    "        \n",
    "        # Set storage requirements\n",
    "        if category in storage_requirements:\n",
    "            data[\"storage_requirements\"].append(random.choice(storage_requirements[category]))\n",
    "        else:\n",
    "            data[\"storage_requirements\"].append(\"Standard Storage\")\n",
    "        \n",
    "        # Determine parent product (if any)\n",
    "        # About 15% of products will have a parent\n",
    "        if (data[\"product_id\"][i] not in potential_parents and \n",
    "            random.random() < 0.15 and \n",
    "            potential_parents):  # Make sure there are potential parents\n",
    "            \n",
    "            parent_id = random.choice(potential_parents)\n",
    "            data[\"parent_product_id\"].append(parent_id)\n",
    "        else:\n",
    "            data[\"parent_product_id\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} product records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_materials_data(products_df=None, num_materials=150, output_file=\"data/materials.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the Materials table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - num_materials: Number of material records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated materials data\n",
    "    \"\"\"\n",
    "    # Define material types and their probabilities\n",
    "    material_types = {\n",
    "        \"Raw Material\": 0.4,\n",
    "        \"Packaging\": 0.25,\n",
    "        \"WIP\": 0.15,\n",
    "        \"Intermediate\": 0.1,\n",
    "        \"Consumable\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Define material categories\n",
    "    material_categories = {\n",
    "        \"Chemical\": [\n",
    "            \"Solvent\", \"Reagent\", \"Catalyst\", \"Acid\", \"Base\", \"Salt\", \n",
    "            \"Polymer\", \"Monomer\", \"Pigment\", \"Additive\"\n",
    "        ],\n",
    "        \"Pharmaceutical\": [\n",
    "            \"API\", \"Excipient\", \"Binder\", \"Filler\", \"Coating\", \"Disintegrant\", \n",
    "            \"Preservative\", \"Stabilizer\", \"Colorant\", \"Flavoring\"\n",
    "        ],\n",
    "        \"Food\": [\n",
    "            \"Ingredient\", \"Additive\", \"Flavoring\", \"Coloring\", \"Preservative\", \n",
    "            \"Thickener\", \"Sweetener\", \"Emulsifier\", \"Stabilizer\", \"Nutrient\"\n",
    "        ],\n",
    "        \"Packaging\": [\n",
    "            \"Container\", \"Closure\", \"Label\", \"Carton\", \"Insert\", \"Film\", \n",
    "            \"Foil\", \"Bottle\", \"Cap\", \"Box\"\n",
    "        ],\n",
    "        \"Industrial\": [\n",
    "            \"Metal\", \"Plastic\", \"Rubber\", \"Glass\", \"Ceramic\", \"Composite\", \n",
    "            \"Textile\", \"Paper\", \"Wood\", \"Lubricant\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define units of measurement by material type\n",
    "    units_of_measure = {\n",
    "        \"Raw Material\": [\"kg\", \"L\", \"g\", \"ton\", \"m³\"],\n",
    "        \"Packaging\": [\"piece\", \"unit\", \"roll\", \"sheet\", \"box\"],\n",
    "        \"WIP\": [\"kg\", \"L\", \"batch\", \"unit\"],\n",
    "        \"Intermediate\": [\"kg\", \"L\", \"batch\", \"unit\"],\n",
    "        \"Consumable\": [\"piece\", \"unit\", \"kg\", \"L\"]\n",
    "    }\n",
    "    \n",
    "    # Define storage requirements by material category\n",
    "    storage_requirements = {\n",
    "        \"Chemical\": [\n",
    "            \"Flammable Storage\", \"Acid Cabinet\", \"Base Cabinet\", \"Ventilated Area\", \n",
    "            \"Temperature Controlled (15-25°C)\", \"Protected from Light\", \"Protected from Moisture\",\n",
    "            \"Refrigerated (2-8°C)\", \"Freezer (-20°C)\"\n",
    "        ],\n",
    "        \"Pharmaceutical\": [\n",
    "            \"Room Temperature (15-25°C)\", \"Refrigerated (2-8°C)\", \"Frozen (-20°C)\", \n",
    "            \"Controlled Room Temperature (20-25°C)\", \"Protected from Light\",\n",
    "            \"Protected from Moisture\", \"Controlled Humidity (<60% RH)\"\n",
    "        ],\n",
    "        \"Food\": [\n",
    "            \"Room Temperature\", \"Refrigerated\", \"Frozen\", \"Cool and Dry\", \n",
    "            \"Protected from Light\", \"Protected from Moisture\", \"Ambient\"\n",
    "        ],\n",
    "        \"Packaging\": [\n",
    "            \"Standard Warehouse\", \"Climate Controlled\", \"Protected from Moisture\", \n",
    "            \"Protected from Dust\", \"Away from Chemicals\"\n",
    "        ],\n",
    "        \"Industrial\": [\n",
    "            \"Standard Warehouse\", \"Climate Controlled\", \"Protected from Moisture\", \n",
    "            \"Protected from Corrosives\", \"Hazardous Materials Storage\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define hazard classifications\n",
    "    hazard_classifications = [\n",
    "        \"Non-Hazardous\", \"Flammable\", \"Corrosive\", \"Toxic\", \"Oxidizer\", \n",
    "        \"Explosive\", \"Environmentally Hazardous\", \"Irritant\", \"Pressurized Gas\",\n",
    "        \"Carcinogen\", \"Radioactive\", \"Biohazard\", \"None\"\n",
    "    ]\n",
    "    \n",
    "    # Define data structure\n",
    "    data = {\n",
    "        \"material_id\": [f\"MAT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_materials)],\n",
    "        \"material_name\": [],\n",
    "        \"material_type\": [],\n",
    "        \"description\": [],\n",
    "        \"unit_of_measure\": [],\n",
    "        \"standard_cost\": [],\n",
    "        \"lead_time_days\": [],\n",
    "        \"minimum_order_quantity\": [],\n",
    "        \"approved_suppliers\": [],\n",
    "        \"safety_stock\": [],\n",
    "        \"reorder_point\": [],\n",
    "        \"status\": [],\n",
    "        \"storage_requirements\": [],\n",
    "        \"hazard_classification\": []\n",
    "    }\n",
    "    \n",
    "    # Generate supplier IDs for approved suppliers\n",
    "    supplier_ids = [f\"SUP-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    \n",
    "    # Generate data for each material\n",
    "    for i in range(num_materials):\n",
    "        # Select material type (weighted random)\n",
    "        material_type = random.choices(\n",
    "            list(material_types.keys()), \n",
    "            weights=list(material_types.values())\n",
    "        )[0]\n",
    "        data[\"material_type\"].append(material_type)\n",
    "        \n",
    "        # Select material category\n",
    "        category = random.choice(list(material_categories.keys()))\n",
    "        \n",
    "        # Select material subtype within category\n",
    "        material_subtype = random.choice(material_categories[category])\n",
    "        \n",
    "        # Generate material name\n",
    "        material_grade = random.choice([\"Standard\", \"Premium\", \"Technical\", \"USP\", \"NF\", \"EP\", \"BP\", \"CP\", \"ACS\", \"Ultra\"])\n",
    "        material_number = random.randint(100, 999)\n",
    "        \n",
    "        material_name = f\"{material_subtype} {material_grade} {material_number}\"\n",
    "        data[\"material_name\"].append(material_name)\n",
    "        \n",
    "        # Generate description\n",
    "        descriptions = [\n",
    "            f\"Standard {material_subtype.lower()} for general use\",\n",
    "            f\"{material_grade} grade {material_subtype.lower()} for {material_type.lower()} applications\",\n",
    "            f\"High-quality {material_subtype.lower()} meeting {material_grade} specifications\",\n",
    "            f\"Industrial {material_subtype.lower()} for manufacturing processes\",\n",
    "            f\"{material_grade} certified {material_subtype.lower()}\"\n",
    "        ]\n",
    "        data[\"description\"].append(random.choice(descriptions))\n",
    "        \n",
    "        # Select unit of measure\n",
    "        if material_type in units_of_measure:\n",
    "            unit = random.choice(units_of_measure[material_type])\n",
    "        else:\n",
    "            unit = \"kg\"\n",
    "        data[\"unit_of_measure\"].append(unit)\n",
    "        \n",
    "        # Generate cost\n",
    "        # Different cost ranges for different material types\n",
    "        if material_type == \"Raw Material\":\n",
    "            if material_subtype in [\"API\", \"Catalyst\"]:\n",
    "                # High-value materials\n",
    "                standard_cost = random.uniform(100, 5000)\n",
    "            else:\n",
    "                standard_cost = random.uniform(10, 500)\n",
    "        elif material_type == \"Packaging\":\n",
    "            standard_cost = random.uniform(0.5, 50)\n",
    "        elif material_type in [\"WIP\", \"Intermediate\"]:\n",
    "            standard_cost = random.uniform(20, 1000)\n",
    "        else:  # Consumable\n",
    "            standard_cost = random.uniform(5, 200)\n",
    "        \n",
    "        data[\"standard_cost\"].append(round(standard_cost, 2))\n",
    "        \n",
    "        # Generate lead time\n",
    "        if material_type == \"Raw Material\":\n",
    "            lead_time = random.randint(30, 120)  # 1-4 months\n",
    "        elif material_type == \"Packaging\":\n",
    "            lead_time = random.randint(14, 60)  # 2-8 weeks\n",
    "        else:\n",
    "            lead_time = random.randint(7, 30)  # 1-4 weeks\n",
    "        \n",
    "        data[\"lead_time_days\"].append(lead_time)\n",
    "        \n",
    "        # Generate minimum order quantity\n",
    "        if unit in [\"kg\", \"L\"]:\n",
    "            if standard_cost > 1000:\n",
    "                # Expensive materials have lower MOQs\n",
    "                min_order = random.choice([0.1, 0.25, 0.5, 1, 5])\n",
    "            else:\n",
    "                min_order = random.choice([1, 5, 10, 25, 50, 100])\n",
    "        elif unit in [\"g\", \"ml\"]:\n",
    "            min_order = random.choice([100, 250, 500, 1000, 5000])\n",
    "        else:\n",
    "            min_order = random.choice([10, 25, 50, 100, 500, 1000])\n",
    "        \n",
    "        data[\"minimum_order_quantity\"].append(min_order)\n",
    "        \n",
    "        # Generate approved suppliers\n",
    "        num_suppliers = random.randint(1, 3)\n",
    "        approved_suppliers = random.sample(supplier_ids, num_suppliers)\n",
    "        data[\"approved_suppliers\"].append(str(approved_suppliers))\n",
    "        \n",
    "        # Generate safety stock and reorder point\n",
    "        # Higher for critical materials, lower for consumables\n",
    "        if material_type in [\"Raw Material\", \"Intermediate\"]:\n",
    "            safety_factor = random.uniform(0.5, 2.0)\n",
    "            consumption_rate = random.uniform(1, 10) * min_order\n",
    "            \n",
    "            # Safety stock based on lead time and consumption rate\n",
    "            safety_stock = consumption_rate * lead_time * safety_factor / 30  # Normalized to monthly consumption\n",
    "            reorder_point = safety_stock + (consumption_rate * lead_time / 30)\n",
    "        else:\n",
    "            safety_stock = min_order * random.uniform(1, 3)\n",
    "            reorder_point = min_order * random.uniform(2, 5)\n",
    "        \n",
    "        data[\"safety_stock\"].append(round(safety_stock, 2))\n",
    "        data[\"reorder_point\"].append(round(reorder_point, 2))\n",
    "        \n",
    "        # Set status (mostly active)\n",
    "        statuses = [\"Active\", \"Pending Approval\", \"Obsolete\", \"On Hold\", \"Discontinued\"]\n",
    "        status_weights = [0.8, 0.05, 0.05, 0.05, 0.05]\n",
    "        data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        \n",
    "        # Set storage requirements\n",
    "        if category in storage_requirements:\n",
    "            data[\"storage_requirements\"].append(random.choice(storage_requirements[category]))\n",
    "        else:\n",
    "            data[\"storage_requirements\"].append(\"Standard Storage\")\n",
    "        \n",
    "        # Set hazard classification\n",
    "        if category == \"Chemical\":\n",
    "            # Chemicals are more likely to be hazardous\n",
    "            hazard_weights = [0.1, 0.15, 0.15, 0.15, 0.1, 0.05, 0.1, 0.1, 0.05, 0.05, 0.0, 0.0, 0.0]\n",
    "        elif category == \"Pharmaceutical\":\n",
    "            # Pharmaceuticals can be hazardous but less so\n",
    "            hazard_weights = [0.3, 0.1, 0.05, 0.1, 0.05, 0.0, 0.05, 0.1, 0.0, 0.05, 0.0, 0.05, 0.15]\n",
    "        else:\n",
    "            # Other categories are less likely to be hazardous\n",
    "            hazard_weights = [0.6, 0.05, 0.05, 0.05, 0.0, 0.0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0, 0.15]\n",
    "        \n",
    "        data[\"hazard_classification\"].append(random.choices(hazard_classifications, weights=hazard_weights)[0])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} material records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(products_df, materials_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated products and materials data\n",
    "    \n",
    "    Parameters:\n",
    "    - products_df: DataFrame containing products data\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    \"\"\"\n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"No products data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nProducts Statistics:\")\n",
    "    print(f\"Total products: {len(products_df)}\")\n",
    "    \n",
    "    # Product family distribution\n",
    "    print(\"\\nProduct Family Distribution:\")\n",
    "    family_counts = products_df['product_family'].value_counts()\n",
    "    for family, count in family_counts.items():\n",
    "        print(f\"  {family}: {count} ({count/len(products_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Status distribution\n",
    "    print(\"\\nProduct Status Distribution:\")\n",
    "    status_counts = products_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(products_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Units of measure distribution\n",
    "    print(\"\\nUnits of Measure Distribution:\")\n",
    "    unit_counts = products_df['unit_of_measure'].value_counts().head(10)\n",
    "    for unit, count in unit_counts.items():\n",
    "        print(f\"  {unit}: {count} ({count/len(products_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Price statistics\n",
    "    print(\"\\nPrice Statistics:\")\n",
    "    print(f\"  Average base cost: ${products_df['base_cost'].mean():.2f}\")\n",
    "    print(f\"  Average list price: ${products_df['list_price'].mean():.2f}\")\n",
    "    print(f\"  Average markup: {(products_df['list_price'] / products_df['base_cost']).mean():.2f}x\")\n",
    "    \n",
    "    # Shelf life statistics\n",
    "    print(\"\\nShelf Life Statistics:\")\n",
    "    print(f\"  Average shelf life: {products_df['shelf_life_days'].mean():.1f} days ({products_df['shelf_life_days'].mean()/365:.1f} years)\")\n",
    "    print(f\"  Minimum shelf life: {products_df['shelf_life_days'].min()} days ({products_df['shelf_life_days'].min()/365:.1f} years)\")\n",
    "    print(f\"  Maximum shelf life: {products_df['shelf_life_days'].max()} days ({products_df['shelf_life_days'].max()/365:.1f} years)\")\n",
    "    \n",
    "    # Product age (days since introduction)\n",
    "    products_df['introduction_date'] = pd.to_datetime(products_df['introduction_date'])\n",
    "    products_df['age_days'] = (datetime.now() - products_df['introduction_date']).dt.days\n",
    "    \n",
    "    print(\"\\nProduct Age Statistics:\")\n",
    "    print(f\"  Average product age: {products_df['age_days'].mean():.1f} days ({products_df['age_days'].mean()/365:.1f} years)\")\n",
    "    print(f\"  Newest product: {products_df['age_days'].min()} days old\")\n",
    "    print(f\"  Oldest product: {products_df['age_days'].max()} days old ({products_df['age_days'].max()/365:.1f} years)\")\n",
    "    \n",
    "    # Parent-child relationships\n",
    "    parent_count = products_df['parent_product_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nProducts with parent: {parent_count} ({parent_count/len(products_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Materials statistics (if available)\n",
    "    if materials_df is not None and len(materials_df) > 0:\n",
    "        print(\"\\nMaterials Statistics:\")\n",
    "        print(f\"Total materials: {len(materials_df)}\")\n",
    "        \n",
    "        # Material type distribution\n",
    "        print(\"\\nMaterial Type Distribution:\")\n",
    "        type_counts = materials_df['material_type'].value_counts()\n",
    "        for mat_type, count in type_counts.items():\n",
    "            print(f\"  {mat_type}: {count} ({count/len(materials_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Status distribution\n",
    "        print(\"\\nMaterial Status Distribution:\")\n",
    "        status_counts = materials_df['status'].value_counts()\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"  {status}: {count} ({count/len(materials_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Cost statistics\n",
    "        print(\"\\nCost Statistics:\")\n",
    "        print(f\"  Average standard cost: ${materials_df['standard_cost'].mean():.2f}\")\n",
    "        print(f\"  Minimum standard cost: ${materials_df['standard_cost'].min():.2f}\")\n",
    "        print(f\"  Maximum standard cost: ${materials_df['standard_cost'].max():.2f}\")\n",
    "        \n",
    "        # Lead time statistics\n",
    "        print(\"\\nLead Time Statistics:\")\n",
    "        print(f\"  Average lead time: {materials_df['lead_time_days'].mean():.1f} days\")\n",
    "        print(f\"  Minimum lead time: {materials_df['lead_time_days'].min()} days\")\n",
    "        print(f\"  Maximum lead time: {materials_df['lead_time_days'].max()} days\")\n",
    "        \n",
    "        # Hazard classification distribution\n",
    "        print(\"\\nHazard Classification Distribution:\")\n",
    "        hazard_counts = materials_df['hazard_classification'].value_counts()\n",
    "        for hazard, count in hazard_counts.items():\n",
    "            print(f\"  {hazard}: {count} ({count/len(materials_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Supplier distribution\n",
    "        avg_suppliers = materials_df['approved_suppliers'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0).mean()\n",
    "        print(f\"\\nAverage approved suppliers per material: {avg_suppliers:.1f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Generate products data\n",
    "    products_df = generate_products_data(\n",
    "        num_products=100,  # Generate 100 product records\n",
    "        output_file=\"data/products.csv\"\n",
    "    )\n",
    "    \n",
    "    # Generate materials data\n",
    "    materials_df = generate_materials_data(\n",
    "        products_df,\n",
    "        num_materials=150,  # Generate 150 material records\n",
    "        output_file=\"data/materials.csv\"\n",
    "    )\n",
    "    \n",
    "    # Display statistics\n",
    "    if products_df is not None:\n",
    "        display_statistics(products_df, materials_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample products data (first 5 records):\")\n",
    "        print(products_df.head(5))\n",
    "        \n",
    "        if materials_df is not None:\n",
    "            print(\"\\nSample materials data (first 5 records):\")\n",
    "            print(materials_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55579d3b",
   "metadata": {},
   "source": [
    "Bill Of Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f8a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 680 bill of materials records for 84 products.\n",
      "Data saved to data/bill_of_materials.csv\n",
      "\n",
      "Bill of Materials Statistics:\n",
      "Total BOM records: 680\n",
      "Products with BOMs: 84\n",
      "Materials used in BOMs: 145\n",
      "\n",
      "BOM Level Distribution:\n",
      "  Level 1: 426 (62.6%)\n",
      "  Level 2: 205 (30.1%)\n",
      "  Level 3: 49 (7.2%)\n",
      "\n",
      "Materials Per Product Statistics:\n",
      "  Average materials per product: 8.1\n",
      "  Minimum materials per product: 2\n",
      "  Maximum materials per product: 15\n",
      "\n",
      "Material Type Distribution in BOMs:\n",
      "  Raw Material: 372 (54.7%)\n",
      "  Packaging: 164 (24.1%)\n",
      "  WIP: 64 (9.4%)\n",
      "  Intermediate: 43 (6.3%)\n",
      "  Consumable: 37 (5.4%)\n",
      "\n",
      "Product Family Distribution in BOMs:\n",
      "  Automotive: 19 products (22.6%)\n",
      "  Chemical: 9 products (10.7%)\n",
      "  Consumer Goods: 17 products (20.2%)\n",
      "  Electronics: 14 products (16.7%)\n",
      "  Food & Beverage: 14 products (16.7%)\n",
      "  Pharmaceutical: 11 products (13.1%)\n",
      "\n",
      "Average Materials Per Product by Family:\n",
      "  Automotive: 10.1 materials per product\n",
      "  Chemical: 6.4 materials per product\n",
      "  Consumer Goods: 5.2 materials per product\n",
      "  Electronics: 9.7 materials per product\n",
      "  Food & Beverage: 6.9 materials per product\n",
      "  Pharmaceutical: 9.9 materials per product\n",
      "\n",
      "BOM items with alternative materials: 127 (18.7%)\n",
      "\n",
      "Scrap Factor Statistics:\n",
      "  Average scrap factor: 5.9%\n",
      "  Minimum scrap factor: 1.0%\n",
      "  Maximum scrap factor: 14.9%\n",
      "\n",
      "BOM items with reference designators: 244 (35.9%)\n",
      "\n",
      "Effective Date Distribution:\n",
      "  Oldest effective date: 2015-06-04\n",
      "  Newest effective date: 2025-06-05\n",
      "  Effective dates by year:\n",
      "    2015: 24 (3.5%)\n",
      "    2016: 80 (11.8%)\n",
      "    2017: 11 (1.6%)\n",
      "    2018: 89 (13.1%)\n",
      "    2019: 111 (16.3%)\n",
      "    2020: 93 (13.7%)\n",
      "    2021: 64 (9.4%)\n",
      "    2022: 92 (13.5%)\n",
      "    2023: 47 (6.9%)\n",
      "    2024: 39 (5.7%)\n",
      "    2025: 30 (4.4%)\n",
      "\n",
      "BOM items with obsolete dates: 56 (8.2%)\n",
      "\n",
      "Sample bill of materials data (first 5 records):\n",
      "         bom_id     product_id   material_id  quantity unit  \\\n",
      "0  BOM-C4CDC708  PROD-7ACC7C5D  MAT-ABDCCA2C     83.39   kg   \n",
      "1  BOM-C2F76907  PROD-7ACC7C5D  MAT-A070D1E0     42.86   m³   \n",
      "2  BOM-812BA8EE  PROD-7ACC7C5D  MAT-79E88050     74.59  ton   \n",
      "3  BOM-8E621EAF  PROD-7ACC7C5D  MAT-4D6950B8      8.82  ton   \n",
      "4  BOM-F99CBEB8  PROD-7ACC7C5D  MAT-7DCF01A9      1.83   kg   \n",
      "\n",
      "  reference_designator  bom_level effective_date obsolete_date  \\\n",
      "0               ASY-93          1     2018-02-13                 \n",
      "1              COMP-17          1     2018-02-16                 \n",
      "2              COMP-36          1     2018-02-24                 \n",
      "3               PCB-35          1     2018-03-16                 \n",
      "4              COMP-57          1     2018-02-03    2026-02-02   \n",
      "\n",
      "  alternative_material_ids  scrap_factor material_type product_family  \\\n",
      "0                       []         0.036  Raw Material     Automotive   \n",
      "1                       []         0.029  Raw Material     Automotive   \n",
      "2                       []         0.049  Raw Material     Automotive   \n",
      "3         ['MAT-C2041393']         0.044  Raw Material     Automotive   \n",
      "4                       []         0.043  Raw Material     Automotive   \n",
      "\n",
      "   effective_year  \n",
      "0            2018  \n",
      "1            2018  \n",
      "2            2018  \n",
      "3            2018  \n",
      "4            2018  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_products_data(products_file=\"data/products.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated products data\n",
    "    \n",
    "    Parameters:\n",
    "    - products_file: CSV file containing products data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the products data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(products_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Products data file {products_file} not found.\")\n",
    "        print(\"Please run the products data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_materials_data(materials_file=\"data/materials.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated materials data\n",
    "    \n",
    "    Parameters:\n",
    "    - materials_file: CSV file containing materials data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the materials data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(materials_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Materials data file {materials_file} not found.\")\n",
    "        print(\"Please run the materials data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def generate_bill_of_materials(products_df, materials_df, output_file=\"data/bill_of_materials.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the BillOfMaterials table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - products_df: DataFrame containing products data\n",
    "    - materials_df: DataFrame containing materials data\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated bill of materials data\n",
    "    \"\"\"\n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"Error: No products data available.\")\n",
    "        return None\n",
    "        \n",
    "    if materials_df is None or len(materials_df) == 0:\n",
    "        print(\"Error: No materials data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Define data structure\n",
    "    data = {\n",
    "        \"bom_id\": [],\n",
    "        \"product_id\": [],\n",
    "        \"material_id\": [],\n",
    "        \"quantity\": [],\n",
    "        \"unit\": [],\n",
    "        \"reference_designator\": [],\n",
    "        \"bom_level\": [],\n",
    "        \"effective_date\": [],\n",
    "        \"obsolete_date\": [],\n",
    "        \"alternative_material_ids\": [],\n",
    "        \"scrap_factor\": []\n",
    "    }\n",
    "    \n",
    "    # Track the number of BOM records we'll create\n",
    "    total_bom_records = 0\n",
    "    \n",
    "    # Process each product to create its bill of materials\n",
    "    for _, product in products_df.iterrows():\n",
    "        product_id = product['product_id']\n",
    "        product_family = product['product_family']\n",
    "        product_status = product['status']\n",
    "        \n",
    "        # Skip creating BOMs for obsolete or discontinued products\n",
    "        if product_status in [\"Obsolete\", \"Discontinued\"]:\n",
    "            continue\n",
    "        \n",
    "        # Determine the number of materials to include in this product's BOM\n",
    "        # More complex products have more components\n",
    "        if product_family in [\"Pharmaceutical\", \"Electronics\", \"Automotive\"]:\n",
    "            num_materials = random.randint(5, 15)\n",
    "        elif product_family in [\"Food & Beverage\", \"Chemical\"]:\n",
    "            num_materials = random.randint(3, 10)\n",
    "        else:\n",
    "            num_materials = random.randint(2, 8)\n",
    "        \n",
    "        # Ensure we don't try to select more materials than available\n",
    "        num_materials = min(num_materials, len(materials_df))\n",
    "        \n",
    "        # Select materials for this product's BOM (without replacement for this product)\n",
    "        # Filter materials by type first to ensure logical combinations\n",
    "        raw_materials = materials_df[materials_df['material_type'] == 'Raw Material']\n",
    "        packaging_materials = materials_df[materials_df['material_type'] == 'Packaging']\n",
    "        other_materials = materials_df[~materials_df['material_type'].isin(['Raw Material', 'Packaging'])]\n",
    "        \n",
    "        # Determine mix of material types (usually more raw materials than packaging)\n",
    "        num_raw = int(num_materials * 0.6)\n",
    "        num_packaging = int(num_materials * 0.3)\n",
    "        num_other = num_materials - num_raw - num_packaging\n",
    "        \n",
    "        # Adjust if not enough materials of a certain type\n",
    "        if len(raw_materials) < num_raw:\n",
    "            num_raw = len(raw_materials)\n",
    "            num_other += (num_materials - num_raw - num_packaging)\n",
    "        \n",
    "        if len(packaging_materials) < num_packaging:\n",
    "            num_packaging = len(packaging_materials)\n",
    "            num_other += (num_materials - num_raw - num_packaging)\n",
    "        \n",
    "        if len(other_materials) < num_other:\n",
    "            num_other = len(other_materials)\n",
    "        \n",
    "        # Select materials of each type\n",
    "        selected_raw = raw_materials.sample(num_raw) if num_raw > 0 else pd.DataFrame()\n",
    "        selected_packaging = packaging_materials.sample(num_packaging) if num_packaging > 0 else pd.DataFrame()\n",
    "        selected_other = other_materials.sample(num_other) if num_other > 0 else pd.DataFrame()\n",
    "        \n",
    "        # Combine selected materials\n",
    "        selected_materials = pd.concat([selected_raw, selected_packaging, selected_other])\n",
    "        \n",
    "        # Create BOM records for this product\n",
    "        for level, (_, material) in enumerate(selected_materials.iterrows(), 1):\n",
    "            material_id = material['material_id']\n",
    "            material_type = material['material_type']\n",
    "            \n",
    "            # Generate a unique BOM ID\n",
    "            bom_id = f\"BOM-{uuid.uuid4().hex[:8].upper()}\"\n",
    "            data[\"bom_id\"].append(bom_id)\n",
    "            data[\"product_id\"].append(product_id)\n",
    "            data[\"material_id\"].append(material_id)\n",
    "            \n",
    "            # Set BOM level\n",
    "            if material_type == 'Raw Material':\n",
    "                bom_level = 1  # Raw materials typically at level 1\n",
    "            elif material_type == 'Packaging':\n",
    "                bom_level = 2  # Packaging typically at level 2\n",
    "            else:\n",
    "                bom_level = random.randint(1, 3)  # Other materials at various levels\n",
    "                \n",
    "            data[\"bom_level\"].append(bom_level)\n",
    "            \n",
    "            # Set material quantity based on material type and unit\n",
    "            material_unit = material['unit_of_measure']\n",
    "            data[\"unit\"].append(material_unit)\n",
    "            \n",
    "            if material_type == 'Raw Material':\n",
    "                if material_unit in ['kg', 'L']:\n",
    "                    quantity = random.uniform(0.1, 100)\n",
    "                elif material_unit in ['g', 'ml']:\n",
    "                    quantity = random.uniform(1, 5000)\n",
    "                else:\n",
    "                    quantity = random.uniform(1, 100)\n",
    "            elif material_type == 'Packaging':\n",
    "                if material_unit in ['piece', 'unit']:\n",
    "                    quantity = random.randint(1, 10)\n",
    "                else:\n",
    "                    quantity = random.uniform(0.1, 10)\n",
    "            else:\n",
    "                quantity = random.uniform(0.1, 50)\n",
    "                \n",
    "            # Round to appropriate precision\n",
    "            if material_unit in ['g', 'ml']:\n",
    "                quantity = round(quantity, 0)\n",
    "            else:\n",
    "                quantity = round(quantity, 2)\n",
    "                \n",
    "            data[\"quantity\"].append(quantity)\n",
    "            \n",
    "            # Set reference designator (mainly for assembled products)\n",
    "            if product_family in [\"Electronics\", \"Automotive\"] and material_type != 'Packaging':\n",
    "                reference_designators = [\n",
    "                    f\"POS-{random.randint(1, 100)}\",\n",
    "                    f\"COMP-{random.randint(1, 100)}\",\n",
    "                    f\"ASY-{random.randint(1, 100)}\",\n",
    "                    f\"PCB-{random.randint(1, 100)}\",\n",
    "                    f\"MOD-{random.randint(1, 100)}\"\n",
    "                ]\n",
    "                data[\"reference_designator\"].append(random.choice(reference_designators))\n",
    "            else:\n",
    "                data[\"reference_designator\"].append(\"\")\n",
    "            \n",
    "            # Set effective and obsolete dates\n",
    "            # Effective date is typically before product introduction\n",
    "            if pd.notna(product['introduction_date']):\n",
    "                intro_date = pd.to_datetime(product['introduction_date'])\n",
    "                effective_date = intro_date - timedelta(days=random.randint(30, 180))\n",
    "                data[\"effective_date\"].append(effective_date.strftime(\"%Y-%m-%d\"))\n",
    "            else:\n",
    "                effective_date = datetime.now() - timedelta(days=random.randint(30, 365))\n",
    "                data[\"effective_date\"].append(effective_date.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            # Most BOM items don't have obsolete dates\n",
    "            if random.random() < 0.1:  # 10% chance of having an obsolete date\n",
    "                obsolete_date = datetime.now() + timedelta(days=random.randint(180, 730))\n",
    "                data[\"obsolete_date\"].append(obsolete_date.strftime(\"%Y-%m-%d\"))\n",
    "            else:\n",
    "                data[\"obsolete_date\"].append(\"\")\n",
    "            \n",
    "            # Set alternative materials\n",
    "            # About 20% of materials have alternatives\n",
    "            if random.random() < 0.2:\n",
    "                # Find materials of the same type to use as alternatives\n",
    "                same_type_materials = materials_df[materials_df['material_type'] == material_type]\n",
    "                same_type_materials = same_type_materials[same_type_materials['material_id'] != material_id]\n",
    "                \n",
    "                if len(same_type_materials) > 0:\n",
    "                    num_alternatives = random.randint(1, min(3, len(same_type_materials)))\n",
    "                    alternatives = same_type_materials.sample(num_alternatives)['material_id'].tolist()\n",
    "                    data[\"alternative_material_ids\"].append(str(alternatives))\n",
    "                else:\n",
    "                    data[\"alternative_material_ids\"].append(\"[]\")\n",
    "            else:\n",
    "                data[\"alternative_material_ids\"].append(\"[]\")\n",
    "            \n",
    "            # Set scrap factor (higher for more complex materials)\n",
    "            if material_type == 'Raw Material':\n",
    "                scrap_factor = random.uniform(0.02, 0.1)  # 2-10% scrap\n",
    "            elif material_type == 'Packaging':\n",
    "                scrap_factor = random.uniform(0.01, 0.05)  # 1-5% scrap\n",
    "            else:\n",
    "                scrap_factor = random.uniform(0.03, 0.15)  # 3-15% scrap\n",
    "                \n",
    "            data[\"scrap_factor\"].append(round(scrap_factor, 3))\n",
    "            \n",
    "            total_bom_records += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {total_bom_records} bill of materials records for {df['product_id'].nunique()} products.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(bom_df, products_df=None, materials_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated bill of materials data\n",
    "    \n",
    "    Parameters:\n",
    "    - bom_df: DataFrame containing bill of materials data\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    \"\"\"\n",
    "    if bom_df is None or len(bom_df) == 0:\n",
    "        print(\"No bill of materials data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nBill of Materials Statistics:\")\n",
    "    print(f\"Total BOM records: {len(bom_df)}\")\n",
    "    print(f\"Products with BOMs: {bom_df['product_id'].nunique()}\")\n",
    "    print(f\"Materials used in BOMs: {bom_df['material_id'].nunique()}\")\n",
    "    \n",
    "    # BOM level distribution\n",
    "    print(\"\\nBOM Level Distribution:\")\n",
    "    level_counts = bom_df['bom_level'].value_counts().sort_index()\n",
    "    for level, count in level_counts.items():\n",
    "        print(f\"  Level {level}: {count} ({count/len(bom_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Materials per product statistics\n",
    "    materials_per_product = bom_df.groupby('product_id')['material_id'].count()\n",
    "    \n",
    "    print(\"\\nMaterials Per Product Statistics:\")\n",
    "    print(f\"  Average materials per product: {materials_per_product.mean():.1f}\")\n",
    "    print(f\"  Minimum materials per product: {materials_per_product.min()}\")\n",
    "    print(f\"  Maximum materials per product: {materials_per_product.max()}\")\n",
    "    \n",
    "    # Material type distribution (if materials_df is available)\n",
    "    if materials_df is not None:\n",
    "        # Create a mapping of material_id to material_type\n",
    "        material_type_map = dict(zip(materials_df['material_id'], materials_df['material_type']))\n",
    "        \n",
    "        # Add material type to BOM data\n",
    "        bom_df['material_type'] = bom_df['material_id'].map(material_type_map)\n",
    "        \n",
    "        print(\"\\nMaterial Type Distribution in BOMs:\")\n",
    "        type_counts = bom_df['material_type'].value_counts()\n",
    "        for mat_type, count in type_counts.items():\n",
    "            print(f\"  {mat_type}: {count} ({count/len(bom_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Product family distribution (if products_df is available)\n",
    "    if products_df is not None:\n",
    "        # Create a mapping of product_id to product_family\n",
    "        product_family_map = dict(zip(products_df['product_id'], products_df['product_family']))\n",
    "        \n",
    "        # Add product family to BOM data\n",
    "        bom_df['product_family'] = bom_df['product_id'].map(product_family_map)\n",
    "        \n",
    "        print(\"\\nProduct Family Distribution in BOMs:\")\n",
    "        family_counts = bom_df.groupby('product_family')['product_id'].nunique()\n",
    "        total_products = family_counts.sum()\n",
    "        \n",
    "        for family, count in family_counts.items():\n",
    "            print(f\"  {family}: {count} products ({count/total_products*100:.1f}%)\")\n",
    "        \n",
    "        # Materials per product by family\n",
    "        print(\"\\nAverage Materials Per Product by Family:\")\n",
    "        for family in family_counts.index:\n",
    "            family_boms = bom_df[bom_df['product_family'] == family]\n",
    "            avg_materials = family_boms.groupby('product_id')['material_id'].count().mean()\n",
    "            print(f\"  {family}: {avg_materials:.1f} materials per product\")\n",
    "    \n",
    "    # Alternative materials statistics\n",
    "    has_alternatives = bom_df['alternative_material_ids'].apply(lambda x: x != \"[]\").sum()\n",
    "    print(f\"\\nBOM items with alternative materials: {has_alternatives} ({has_alternatives/len(bom_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Scrap factor statistics\n",
    "    print(\"\\nScrap Factor Statistics:\")\n",
    "    print(f\"  Average scrap factor: {bom_df['scrap_factor'].mean():.1%}\")\n",
    "    print(f\"  Minimum scrap factor: {bom_df['scrap_factor'].min():.1%}\")\n",
    "    print(f\"  Maximum scrap factor: {bom_df['scrap_factor'].max():.1%}\")\n",
    "    \n",
    "    # Reference designator statistics\n",
    "    has_designator = bom_df['reference_designator'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nBOM items with reference designators: {has_designator} ({has_designator/len(bom_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Effective date distribution\n",
    "    bom_df['effective_date'] = pd.to_datetime(bom_df['effective_date'])\n",
    "    \n",
    "    print(\"\\nEffective Date Distribution:\")\n",
    "    oldest = bom_df['effective_date'].min()\n",
    "    newest = bom_df['effective_date'].max()\n",
    "    print(f\"  Oldest effective date: {oldest.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Newest effective date: {newest.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Count by year\n",
    "    bom_df['effective_year'] = bom_df['effective_date'].dt.year\n",
    "    year_counts = bom_df['effective_year'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"  Effective dates by year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"    {year}: {count} ({count/len(bom_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Obsolete date statistics\n",
    "    has_obsolete = bom_df['obsolete_date'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nBOM items with obsolete dates: {has_obsolete} ({has_obsolete/len(bom_df)*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    products_df = load_products_data()\n",
    "    materials_df = load_materials_data()\n",
    "    \n",
    "    if products_df is not None and materials_df is not None:\n",
    "        # Generate bill of materials data\n",
    "        bom_df = generate_bill_of_materials(\n",
    "            products_df,\n",
    "            materials_df,\n",
    "            output_file=\"data/bill_of_materials.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if bom_df is not None:\n",
    "            display_statistics(bom_df, products_df, materials_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample bill of materials data (first 5 records):\")\n",
    "            print(bom_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba502a20",
   "metadata": {},
   "source": [
    "Customer Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c63f9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 customer records.\n",
      "Data saved to data/customers.csv\n",
      "Successfully generated 300 customer order records.\n",
      "Data saved to data/customer_orders.csv\n",
      "Successfully generated 1726 order line records for 300 orders.\n",
      "Data saved to data/order_lines.csv\n",
      "\n",
      "Customers Statistics:\n",
      "Total customers: 100\n",
      "\n",
      "Customer Type Distribution:\n",
      "  Retailer: 28 (28.0%)\n",
      "  Distributor: 25 (25.0%)\n",
      "  Wholesaler: 21 (21.0%)\n",
      "  Contract Manufacturer: 14 (14.0%)\n",
      "  Direct Customer: 12 (12.0%)\n",
      "\n",
      "Industry Distribution (top 10):\n",
      "  Food & Beverage Distribution: 8 (8.0%)\n",
      "  Specialty Store: 8 (8.0%)\n",
      "  Home Improvement: 6 (6.0%)\n",
      "  Pharmaceutical Manufacturing: 5 (5.0%)\n",
      "  Industrial Equipment: 5 (5.0%)\n",
      "  Food Processing: 5 (5.0%)\n",
      "  Building Materials: 5 (5.0%)\n",
      "  Electronics Retail: 5 (5.0%)\n",
      "  Automotive Parts Distribution: 4 (4.0%)\n",
      "  Automotive Manufacturing: 4 (4.0%)\n",
      "\n",
      "Customer Status Distribution:\n",
      "  Active: 77 (77.0%)\n",
      "  New: 8 (8.0%)\n",
      "  On Hold: 5 (5.0%)\n",
      "  Inactive: 5 (5.0%)\n",
      "  Archived: 5 (5.0%)\n",
      "\n",
      "Credit Terms Distribution:\n",
      "  Net 30: 39 (39.0%)\n",
      "  Net 45: 20 (20.0%)\n",
      "  Net 60: 16 (16.0%)\n",
      "  COD: 11 (11.0%)\n",
      "  2/10 Net 30: 8 (8.0%)\n",
      "  Prepaid: 6 (6.0%)\n",
      "\n",
      "Credit Limit Statistics:\n",
      "  Average credit limit: $212945.72\n",
      "  Minimum credit limit: $9151.00\n",
      "  Maximum credit limit: $873169.00\n",
      "\n",
      "Regional Distribution:\n",
      "  Canada: 26 (26.0%)\n",
      "  USA: 21 (21.0%)\n",
      "  Germany: 7 (7.0%)\n",
      "  Italy: 6 (6.0%)\n",
      "  UK: 5 (5.0%)\n",
      "  Australia: 5 (5.0%)\n",
      "  Netherlands: 5 (5.0%)\n",
      "  Argentina: 4 (4.0%)\n",
      "  France: 4 (4.0%)\n",
      "  India: 3 (3.0%)\n",
      "\n",
      "Customer Orders Statistics:\n",
      "Total orders: 300\n",
      "Customers with orders: 88\n",
      "\n",
      "Order Type Distribution:\n",
      "  Standard: 219 (73.0%)\n",
      "  Rush: 29 (9.7%)\n",
      "  Scheduled: 27 (9.0%)\n",
      "  Sample: 14 (4.7%)\n",
      "  Blanket: 11 (3.7%)\n",
      "\n",
      "Order Status Distribution:\n",
      "  Completed: 158 (52.7%)\n",
      "  Cancelled: 46 (15.3%)\n",
      "  On Hold: 40 (13.3%)\n",
      "  Draft: 15 (5.0%)\n",
      "  In Process: 13 (4.3%)\n",
      "  Pending: 11 (3.7%)\n",
      "  Confirmed: 11 (3.7%)\n",
      "  Partially Shipped: 6 (2.0%)\n",
      "\n",
      "Order Priority Distribution:\n",
      "  Priority 1: 41 (13.7%)\n",
      "  Priority 2: 80 (26.7%)\n",
      "  Priority 3: 108 (36.0%)\n",
      "  Priority 4: 43 (14.3%)\n",
      "  Priority 5: 28 (9.3%)\n",
      "\n",
      "Order Value Statistics:\n",
      "  Average order value: $55272.37\n",
      "  Minimum order value: $1089.73\n",
      "  Maximum order value: $391300.60\n",
      "  Total order value: $16581710.62\n",
      "\n",
      "Shipping Method Distribution:\n",
      "  Express: 56 (18.7%)\n",
      "  Truck: 46 (15.3%)\n",
      "  Courier: 44 (14.7%)\n",
      "  Sea: 44 (14.7%)\n",
      "  Air: 43 (14.3%)\n",
      "  Rail: 37 (12.3%)\n",
      "  Customer Pickup: 30 (10.0%)\n",
      "\n",
      "Orders Per Customer Statistics:\n",
      "  Average orders per customer: 3.4\n",
      "  Maximum orders per customer: 8\n",
      "\n",
      "Monthly Order Distribution (last 6 months):\n",
      "  2025-03: 22 orders\n",
      "  2025-04: 26 orders\n",
      "  2025-05: 29 orders\n",
      "  2025-06: 23 orders\n",
      "  2025-07: 30 orders\n",
      "  2025-08: 11 orders\n",
      "\n",
      "Order Lines Statistics:\n",
      "Total order lines: 1726\n",
      "Orders with lines: 300\n",
      "\n",
      "Lines Per Order Statistics:\n",
      "  Average lines per order: 5.8\n",
      "  Minimum lines per order: 1\n",
      "  Maximum lines per order: 10\n",
      "\n",
      "Line Status Distribution:\n",
      "  Shipped: 937 (54.3%)\n",
      "  Cancelled: 234 (13.6%)\n",
      "  On Hold: 223 (12.9%)\n",
      "  Draft: 100 (5.8%)\n",
      "  Confirmed: 84 (4.9%)\n",
      "  Pending: 75 (4.3%)\n",
      "  In Production: 28 (1.6%)\n",
      "  Ready to Ship: 27 (1.6%)\n",
      "  Partially Shipped: 18 (1.0%)\n",
      "\n",
      "Line Value Statistics:\n",
      "  Average line value: $241004.40\n",
      "  Total line value: $415973598.21\n",
      "\n",
      "Quantity Statistics:\n",
      "  Average order quantity: 502.4\n",
      "  Average unit price: $478.69\n",
      "\n",
      "Lines with work order association: 1107 (64.1%)\n",
      "\n",
      "Shipping Statistics:\n",
      "  Shipped lines: 955 (55.3%)\n",
      "  Total shipped quantity: 469001\n",
      "  On-time shipments: 934 (97.8% of shipped lines)\n",
      "\n",
      "Sample customers data (first 5 records):\n",
      "     customer_id        customer_name customer_type  \\\n",
      "0  CUST-8F003517  Walker Distributors   Distributor   \n",
      "1  CUST-0C17D62A     Pro Distributors   Distributor   \n",
      "2  CUST-A3C1F20E   Continental Retail      Retailer   \n",
      "3  CUST-CC1B1413   Northern Wholesale    Wholesaler   \n",
      "4  CUST-615FBCAB        Parker Outlet      Retailer   \n",
      "\n",
      "                      industry  contact_person  \\\n",
      "0  Pharmaceutical Distribution   Cheryl Bryant   \n",
      "1  Consumer Goods Distribution  Steven Roberts   \n",
      "2           Electronics Retail  Raymond Bryant   \n",
      "3           Building Materials   Walter Graham   \n",
      "4              Specialty Store  Melissa Fisher   \n",
      "\n",
      "                                  email              phone  \\\n",
      "0  cheryl.bryant@walkerdistributors.biz  +589 474 754 5445   \n",
      "1    steven.roberts@prodistributors.biz  +271 260 878 8787   \n",
      "2  raymond.bryant@continentalretail.com  +722 448 466 6385   \n",
      "3   walter.graham@northernwholesale.biz  +769 736 973 5671   \n",
      "4       melissa.fisher@parkeroutlet.net  +872 934 850 7472   \n",
      "\n",
      "                                            address credit_terms  \\\n",
      "0              7201 Wood Lane, Walkerfield, Germany       Net 30   \n",
      "1  1475 Alexander Lane, Fort Barnesville, Australia       Net 45   \n",
      "2                   4654 Harris Lane, Patel, Canada       Net 45   \n",
      "3        7026 Lee Avenue, Hayeschester, New Zealand       Net 60   \n",
      "4                       2734 James Lane, Rogers, UK       Net 30   \n",
      "\n",
      "   credit_limit   status account_manager_id      country  \n",
      "0        233485   Active      PERS-4C32DE33      Germany  \n",
      "1        270122   Active      PERS-989A8A5D    Australia  \n",
      "2         54881   Active      PERS-F377223A       Canada  \n",
      "3        262127   Active      PERS-EEF81CF0  New Zealand  \n",
      "4         73821  On Hold      PERS-EEF81CF0           UK  \n",
      "\n",
      "Sample customer orders data (first 5 records):\n",
      "      order_id    customer_id order_date requested_delivery_date  \\\n",
      "0  CO-B262B5F1  CUST-0F042224 2024-07-19              2024-08-24   \n",
      "1  CO-0CBE6E32  CUST-5C305C58 2024-12-04              2025-01-11   \n",
      "2  CO-379457A9  CUST-80C8794C 2025-05-23              2025-06-25   \n",
      "3  CO-087F5BC2  CUST-0C6B2027 2025-05-09              2025-06-02   \n",
      "4  CO-FFCAF8A7  CUST-F25472A3 2024-08-31              2024-10-08   \n",
      "\n",
      "  promised_delivery_date     status order_type  priority  order_value  \\\n",
      "0             2024-08-31  Cancelled   Standard         3     34991.22   \n",
      "1             2025-01-11  Completed   Standard         2    361034.17   \n",
      "2             2025-06-27  Completed  Scheduled         4      6559.35   \n",
      "3             2025-06-06  Completed   Standard         1      3484.40   \n",
      "4             2024-10-07    On Hold   Standard         3     91998.89   \n",
      "\n",
      "  payment_terms  shipping_method   sales_rep_id                        notes  \\\n",
      "0        Net 60          Courier  PERS-BD803BE9                                \n",
      "1        Net 45  Customer Pickup  PERS-BD803BE9                                \n",
      "2   2/10 Net 30          Courier  PERS-C5A0F015  Weekend delivery authorized   \n",
      "3        Net 30          Express  PERS-BD803BE9                                \n",
      "4        Net 30              Air  PERS-BD803BE9                                \n",
      "\n",
      "  order_month  \n",
      "0     2024-07  \n",
      "1     2024-12  \n",
      "2     2025-05  \n",
      "3     2025-05  \n",
      "4     2024-08  \n",
      "\n",
      "Sample order lines data (first 5 records):\n",
      "         line_id     order_id  line_number     product_id  quantity  \\\n",
      "0  LINE-BFC9BB44  CO-B262B5F1            1  PROD-74CC9A91       108   \n",
      "1  LINE-C43B999E  CO-B262B5F1            2  PROD-C192FA6B       675   \n",
      "2  LINE-12143BDF  CO-0CBE6E32            1  PROD-1B34B9C3       742   \n",
      "3  LINE-EDF36298  CO-0CBE6E32            2  PROD-06A99950       162   \n",
      "4  LINE-5B5AF144  CO-0CBE6E32            3  PROD-51EFB58C       258   \n",
      "\n",
      "   unit_price  line_value requested_delivery_date promised_delivery_date  \\\n",
      "0      490.46    52969.21              2024-08-21             2024-09-02   \n",
      "1      540.40   364770.55              2024-08-24             2024-09-01   \n",
      "2       78.75    58434.39              2025-01-12             2025-01-10   \n",
      "3      548.42    88844.68              2025-01-09             2025-01-12   \n",
      "4     1844.34   475840.03              2025-01-11             2025-01-12   \n",
      "\n",
      "      status work_order_id  shipped_quantity shipping_date  \n",
      "0  Cancelled                               0                \n",
      "1  Cancelled                               0                \n",
      "2    Shipped   WO-D0F5FBC6               742    2025-01-06  \n",
      "3    Shipped   WO-A9225AA5               162    2024-12-09  \n",
      "4    Shipped   WO-5510D472               258    2024-12-20  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2065/3239285738.py:882: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  shipped_lines['shipping_date'] = pd.to_datetime(shipped_lines['shipping_date'])\n",
      "/tmp/ipykernel_2065/3239285738.py:883: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  shipped_lines['promised_delivery_date'] = pd.to_datetime(shipped_lines['promised_delivery_date'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_customers_data(num_customers=100, output_file=\"data/customers.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the Customers table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_customers: Number of customer records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated customers data\n",
    "    \"\"\"\n",
    "    # Define customer types and their probabilities\n",
    "    customer_types = {\n",
    "        \"Distributor\": 0.3,\n",
    "        \"Retailer\": 0.25,\n",
    "        \"Wholesaler\": 0.2,\n",
    "        \"Direct Customer\": 0.15,\n",
    "        \"Contract Manufacturer\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Define industries by customer type\n",
    "    industries = {\n",
    "        \"Distributor\": [\n",
    "            \"Pharmaceutical Distribution\", \"Food & Beverage Distribution\", \n",
    "            \"Chemical Distribution\", \"Industrial Supply\", \"Electronics Distribution\",\n",
    "            \"Automotive Parts Distribution\", \"Consumer Goods Distribution\"\n",
    "        ],\n",
    "        \"Retailer\": [\n",
    "            \"Pharmacy\", \"Grocery\", \"Department Store\", \"Specialty Store\", \n",
    "            \"Online Retail\", \"Home Improvement\", \"Electronics Retail\"\n",
    "        ],\n",
    "        \"Wholesaler\": [\n",
    "            \"Pharmaceutical Wholesale\", \"Food Service\", \"Industrial Equipment\", \n",
    "            \"Building Materials\", \"Electronic Components\", \"Automotive Parts\"\n",
    "        ],\n",
    "        \"Direct Customer\": [\n",
    "            \"Hospital\", \"Restaurant Chain\", \"Manufacturing\", \"Government\", \n",
    "            \"Educational Institution\", \"Healthcare Provider\"\n",
    "        ],\n",
    "        \"Contract Manufacturer\": [\n",
    "            \"Pharmaceutical Manufacturing\", \"Food Processing\", \"Electronics Assembly\", \n",
    "            \"Automotive Manufacturing\", \"Medical Device Manufacturing\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define credit terms\n",
    "    credit_terms = [\"Net 30\", \"Net 45\", \"Net 60\", \"2/10 Net 30\", \"COD\", \"Prepaid\"]\n",
    "    credit_terms_weights = [0.4, 0.2, 0.15, 0.1, 0.1, 0.05]  # Probabilities\n",
    "    \n",
    "    # Define regions for address generation\n",
    "    regions = {\n",
    "        \"North America\": [\"USA\", \"Canada\"],\n",
    "        \"Europe\": [\"UK\", \"Germany\", \"France\", \"Italy\", \"Spain\", \"Netherlands\"],\n",
    "        \"Asia\": [\"Japan\", \"China\", \"South Korea\", \"India\", \"Singapore\", \"Taiwan\"],\n",
    "        \"Latin America\": [\"Mexico\", \"Brazil\", \"Colombia\", \"Argentina\", \"Chile\"],\n",
    "        \"Oceania\": [\"Australia\", \"New Zealand\"]\n",
    "    }\n",
    "    \n",
    "    # Region probabilities (adjust as needed for your business model)\n",
    "    region_weights = {\n",
    "        \"North America\": 0.4,\n",
    "        \"Europe\": 0.3,\n",
    "        \"Asia\": 0.15,\n",
    "        \"Latin America\": 0.1,\n",
    "        \"Oceania\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Generate personnel IDs for account managers\n",
    "    account_manager_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(10)]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"customer_id\": [f\"CUST-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_customers)],\n",
    "        \"customer_name\": [],\n",
    "        \"customer_type\": [],\n",
    "        \"industry\": [],\n",
    "        \"contact_person\": [],\n",
    "        \"email\": [],\n",
    "        \"phone\": [],\n",
    "        \"address\": [],\n",
    "        \"credit_terms\": [],\n",
    "        \"credit_limit\": [],\n",
    "        \"status\": [],\n",
    "        \"account_manager_id\": []\n",
    "    }\n",
    "    \n",
    "    # First names for contact generation\n",
    "    first_names = [\n",
    "        \"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\", \n",
    "        \"William\", \"Elizabeth\", \"David\", \"Barbara\", \"Richard\", \"Susan\", \"Joseph\", \"Jessica\", \n",
    "        \"Thomas\", \"Sarah\", \"Charles\", \"Karen\", \"Christopher\", \"Nancy\", \"Daniel\", \"Lisa\", \n",
    "        \"Matthew\", \"Margaret\", \"Anthony\", \"Betty\", \"Mark\", \"Sandra\", \"Donald\", \"Ashley\", \n",
    "        \"Steven\", \"Kimberly\", \"Paul\", \"Emily\", \"Andrew\", \"Donna\", \"Joshua\", \"Michelle\", \n",
    "        \"Kenneth\", \"Dorothy\", \"Kevin\", \"Carol\", \"Brian\", \"Amanda\", \"George\", \"Melissa\", \n",
    "        \"Edward\", \"Deborah\", \"Ronald\", \"Stephanie\", \"Timothy\", \"Rebecca\", \"Jason\", \"Sharon\", \n",
    "        \"Jeffrey\", \"Laura\", \"Ryan\", \"Cynthia\", \"Jacob\", \"Kathleen\", \"Gary\", \"Amy\", \n",
    "        \"Nicholas\", \"Shirley\", \"Eric\", \"Angela\", \"Jonathan\", \"Helen\", \"Stephen\", \"Anna\", \n",
    "        \"Larry\", \"Brenda\", \"Justin\", \"Pamela\", \"Scott\", \"Nicole\", \"Brandon\", \"Emma\", \n",
    "        \"Benjamin\", \"Samantha\", \"Samuel\", \"Katherine\", \"Gregory\", \"Christine\", \"Frank\", \"Debra\", \n",
    "        \"Alexander\", \"Rachel\", \"Raymond\", \"Catherine\", \"Patrick\", \"Carolyn\", \"Jack\", \"Janet\", \n",
    "        \"Dennis\", \"Ruth\", \"Jerry\", \"Maria\", \"Tyler\", \"Heather\", \"Aaron\", \"Diane\", \n",
    "        \"Jose\", \"Virginia\", \"Adam\", \"Julie\", \"Henry\", \"Joyce\", \"Nathan\", \"Victoria\", \n",
    "        \"Douglas\", \"Olivia\", \"Zachary\", \"Kelly\", \"Peter\", \"Christina\", \"Kyle\", \"Lauren\", \n",
    "        \"Walter\", \"Joan\", \"Ethan\", \"Evelyn\", \"Jeremy\", \"Judith\", \"Harold\", \"Megan\", \n",
    "        \"Keith\", \"Cheryl\", \"Christian\", \"Andrea\", \"Roger\", \"Hannah\", \"Noah\", \"Martha\"\n",
    "    ]\n",
    "    \n",
    "    # Last names for contact generation\n",
    "    last_names = [\n",
    "        \"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \n",
    "        \"Rodriguez\", \"Martinez\", \"Hernandez\", \"Lopez\", \"Gonzalez\", \"Wilson\", \"Anderson\", \n",
    "        \"Thomas\", \"Taylor\", \"Moore\", \"Jackson\", \"Martin\", \"Lee\", \"Perez\", \"Thompson\", \n",
    "        \"White\", \"Harris\", \"Sanchez\", \"Clark\", \"Ramirez\", \"Lewis\", \"Robinson\", \"Walker\", \n",
    "        \"Young\", \"Allen\", \"King\", \"Wright\", \"Scott\", \"Torres\", \"Nguyen\", \"Hill\", \"Flores\", \n",
    "        \"Green\", \"Adams\", \"Nelson\", \"Baker\", \"Hall\", \"Rivera\", \"Campbell\", \"Mitchell\", \n",
    "        \"Carter\", \"Roberts\", \"Gomez\", \"Phillips\", \"Evans\", \"Turner\", \"Diaz\", \"Parker\", \n",
    "        \"Cruz\", \"Edwards\", \"Collins\", \"Reyes\", \"Stewart\", \"Morris\", \"Morales\", \"Murphy\", \n",
    "        \"Cook\", \"Rogers\", \"Gutierrez\", \"Ortiz\", \"Morgan\", \"Cooper\", \"Peterson\", \"Bailey\", \n",
    "        \"Reed\", \"Kelly\", \"Howard\", \"Ramos\", \"Kim\", \"Cox\", \"Ward\", \"Richardson\", \"Watson\", \n",
    "        \"Brooks\", \"Chavez\", \"Wood\", \"James\", \"Bennett\", \"Gray\", \"Mendoza\", \"Ruiz\", \"Hughes\", \n",
    "        \"Price\", \"Alvarez\", \"Castillo\", \"Sanders\", \"Patel\", \"Myers\", \"Long\", \"Ross\", \n",
    "        \"Foster\", \"Jimenez\", \"Powell\", \"Jenkins\", \"Perry\", \"Russell\", \"Sullivan\", \"Bell\", \n",
    "        \"Coleman\", \"Butler\", \"Henderson\", \"Barnes\", \"Gonzales\", \"Fisher\", \"Vasquez\", \"Simmons\", \n",
    "        \"Romero\", \"Jordan\", \"Patterson\", \"Alexander\", \"Hamilton\", \"Graham\", \"Reynolds\", \"Griffin\", \n",
    "        \"Wallace\", \"Moreno\", \"West\", \"Cole\", \"Hayes\", \"Bryant\", \"Herrera\", \"Gibson\", \"Ellis\"\n",
    "    ]\n",
    "    \n",
    "    # Company name components for generating realistic company names\n",
    "    company_prefixes = [\n",
    "        \"Advanced\", \"Allied\", \"American\", \"Asian\", \"Atlantic\", \"Best\", \"Better\", \"Blue\", \n",
    "        \"Bright\", \"Central\", \"Century\", \"Consolidated\", \"Continental\", \"Digital\", \"Dynamic\", \n",
    "        \"East\", \"Eastern\", \"Euro\", \"European\", \"Express\", \"First\", \"Global\", \"Golden\", \n",
    "        \"Great\", \"Green\", \"International\", \"Metro\", \"Modern\", \"National\", \"New\", \"North\", \n",
    "        \"Northern\", \"Pacific\", \"Pan\", \"Premier\", \"Prime\", \"Pro\", \"Professional\", \"Quality\", \n",
    "        \"Red\", \"Regional\", \"Royal\", \"Select\", \"South\", \"Southern\", \"Standard\", \"Superior\", \n",
    "        \"Supreme\", \"Trans\", \"United\", \"Universal\", \"West\", \"Western\", \"World\"\n",
    "    ]\n",
    "    \n",
    "    company_types = {\n",
    "        \"Distributor\": [\"Distribution\", \"Distributors\", \"Supply\", \"Supplies\", \"Logistics\"],\n",
    "        \"Retailer\": [\"Retail\", \"Stores\", \"Mart\", \"Market\", \"Shop\", \"Outlet\"],\n",
    "        \"Wholesaler\": [\"Wholesale\", \"Trading\", \"Merchandise\", \"Commerce\"],\n",
    "        \"Direct Customer\": [\"Industries\", \"Group\", \"Corporation\", \"Co.\", \"Inc.\", \"Enterprises\"],\n",
    "        \"Contract Manufacturer\": [\"Manufacturing\", \"Production\", \"Fabrication\", \"Industries\", \"Processors\"]\n",
    "    }\n",
    "    \n",
    "    # Generate data for each customer\n",
    "    for i in range(num_customers):\n",
    "        # Select customer type (weighted random)\n",
    "        customer_type = random.choices(\n",
    "            list(customer_types.keys()), \n",
    "            weights=list(customer_types.values())\n",
    "        )[0]\n",
    "        data[\"customer_type\"].append(customer_type)\n",
    "        \n",
    "        # Generate a realistic company name\n",
    "        if random.random() < 0.7:  # 70% chance of using prefix\n",
    "            prefix = random.choice(company_prefixes)\n",
    "            if customer_type in company_types:\n",
    "                suffix = random.choice(company_types[customer_type])\n",
    "            else:\n",
    "                suffix = random.choice(list(company_types.values())[0])\n",
    "                \n",
    "            company_name = f\"{prefix} {suffix}\"\n",
    "        else:\n",
    "            # Use a last name\n",
    "            last_name = random.choice(last_names)\n",
    "            if customer_type in company_types:\n",
    "                suffix = random.choice(company_types[customer_type])\n",
    "            else:\n",
    "                suffix = random.choice(list(company_types.values())[0])\n",
    "                \n",
    "            company_name = f\"{last_name} {suffix}\"\n",
    "        \n",
    "        data[\"customer_name\"].append(company_name)\n",
    "        \n",
    "        # Select industry based on customer type\n",
    "        if customer_type in industries:\n",
    "            industry = random.choice(industries[customer_type])\n",
    "        else:\n",
    "            # Default to general industry\n",
    "            industry = \"General Manufacturing\"\n",
    "        \n",
    "        data[\"industry\"].append(industry)\n",
    "        \n",
    "        # Generate contact person (random first and last name)\n",
    "        contact_first = random.choice(first_names)\n",
    "        contact_last = random.choice(last_names)\n",
    "        data[\"contact_person\"].append(f\"{contact_first} {contact_last}\")\n",
    "        \n",
    "        # Generate email (company domain based on name)\n",
    "        company_domain = company_name.lower().replace(\" \", \"\").replace(\".\", \"\")\n",
    "        email_domains = [\".com\", \".net\", \".org\", \".co\", \".biz\"]\n",
    "        email_domain = random.choice(email_domains)\n",
    "        data[\"email\"].append(f\"{contact_first.lower()}.{contact_last.lower()}@{company_domain}{email_domain}\")\n",
    "        \n",
    "        # Generate phone\n",
    "        data[\"phone\"].append(f\"+{random.randint(1, 9)}{random.randint(10, 99)} {random.randint(100, 999)} {random.randint(100, 999)} {random.randint(1000, 9999)}\")\n",
    "        \n",
    "        # Generate address based on region probability\n",
    "        region = random.choices(\n",
    "            list(region_weights.keys()), \n",
    "            weights=list(region_weights.values())\n",
    "        )[0]\n",
    "        \n",
    "        country = random.choice(regions[region])\n",
    "        \n",
    "        # Generate a city name (simplified)\n",
    "        city_prefixes = [\"New\", \"Old\", \"East\", \"West\", \"North\", \"South\", \"Central\", \"Upper\", \"Lower\", \"Port\", \"Lake\", \"Mount\", \"Fort\"]\n",
    "        city_suffixes = [\"town\", \"ville\", \"burg\", \"berg\", \"field\", \"ford\", \"port\", \"mouth\", \"stad\", \"furt\", \"chester\", \"cester\", \"bridge\", \"haven\", \"minster\"]\n",
    "        \n",
    "        if random.random() < 0.3:  # 30% chance of using prefix\n",
    "            city = f\"{random.choice(city_prefixes)} {random.choice(last_names)}{random.choice(['', random.choice(city_suffixes)])}\"\n",
    "        else:\n",
    "            city = f\"{random.choice(last_names)}{random.choice(['', random.choice(city_suffixes)])}\"\n",
    "        \n",
    "        # Generate street address\n",
    "        street_number = random.randint(1, 9999)\n",
    "        street_types = [\"Street\", \"Avenue\", \"Boulevard\", \"Road\", \"Lane\", \"Drive\", \"Way\", \"Place\", \"Court\", \"Terrace\"]\n",
    "        street_name = f\"{random.choice(last_names)} {random.choice(street_types)}\"\n",
    "        \n",
    "        address = f\"{street_number} {street_name}, {city}, {country}\"\n",
    "        data[\"address\"].append(address)\n",
    "        \n",
    "        # Set credit terms (weighted random)\n",
    "        credit_term = random.choices(credit_terms, weights=credit_terms_weights)[0]\n",
    "        data[\"credit_terms\"].append(credit_term)\n",
    "        \n",
    "        # Set credit limit based on customer type\n",
    "        if customer_type in [\"Distributor\", \"Wholesaler\"]:\n",
    "            # Larger customers typically have higher credit limits\n",
    "            credit_limit = random.randint(50000, 500000)\n",
    "        elif customer_type == \"Retailer\":\n",
    "            credit_limit = random.randint(10000, 100000)\n",
    "        elif customer_type == \"Contract Manufacturer\":\n",
    "            credit_limit = random.randint(100000, 1000000)\n",
    "        else:\n",
    "            credit_limit = random.randint(5000, 50000)\n",
    "            \n",
    "        data[\"credit_limit\"].append(credit_limit)\n",
    "        \n",
    "        # Set status (mostly active)\n",
    "        statuses = [\"Active\", \"Inactive\", \"On Hold\", \"New\", \"Archived\"]\n",
    "        status_weights = [0.8, 0.05, 0.05, 0.07, 0.03]  # Probabilities\n",
    "        data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        \n",
    "        # Assign account manager\n",
    "        data[\"account_manager_id\"].append(random.choice(account_manager_ids))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} customer records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_customer_orders(customers_df, products_df=None, num_orders=300, \n",
    "                           start_time=None, end_time=None, output_file=\"data/customer_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the CustomerOrders table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - customers_df: DataFrame containing customers data\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - num_orders: Number of customer order records to generate\n",
    "    - start_time: Start time for order dates (defaults to 365 days ago)\n",
    "    - end_time: End time for order dates (defaults to 30 days in the future)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated customer orders data\n",
    "    \"\"\"\n",
    "    if customers_df is None or len(customers_df) == 0:\n",
    "        print(\"Error: No customers data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=365)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now() + timedelta(days=30)\n",
    "    \n",
    "    # Generate product IDs if products_df is not provided\n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"Generating synthetic product IDs...\")\n",
    "        product_ids = [f\"PROD-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    else:\n",
    "        # Use actual product IDs from products_df\n",
    "        product_ids = products_df['product_id'].tolist()\n",
    "    \n",
    "    # Define order types and their probabilities\n",
    "    order_types = {\n",
    "        \"Standard\": 0.7,\n",
    "        \"Rush\": 0.1,\n",
    "        \"Scheduled\": 0.1,\n",
    "        \"Blanket\": 0.05,\n",
    "        \"Sample\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Define priority levels\n",
    "    priority_levels = [1, 2, 3, 4, 5]  # 1 = highest, 5 = lowest\n",
    "    priority_weights = [0.1, 0.2, 0.4, 0.2, 0.1]  # Most orders are medium priority\n",
    "    \n",
    "    # Define payment terms\n",
    "    payment_terms = [\"Net 30\", \"Net 45\", \"Net 60\", \"2/10 Net 30\", \"COD\", \"Prepaid\"]\n",
    "    \n",
    "    # Define shipping methods\n",
    "    shipping_methods = [\"Truck\", \"Air\", \"Sea\", \"Rail\", \"Express\", \"Courier\", \"Customer Pickup\"]\n",
    "    \n",
    "    # Generate sales rep IDs\n",
    "    sales_rep_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(15)]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"order_id\": [f\"CO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_orders)],\n",
    "        \"customer_id\": [],\n",
    "        \"order_date\": [],\n",
    "        \"requested_delivery_date\": [],\n",
    "        \"promised_delivery_date\": [],\n",
    "        \"status\": [],\n",
    "        \"order_type\": [],\n",
    "        \"priority\": [],\n",
    "        \"order_value\": [],\n",
    "        \"payment_terms\": [],\n",
    "        \"shipping_method\": [],\n",
    "        \"sales_rep_id\": [],\n",
    "        \"notes\": []\n",
    "    }\n",
    "    \n",
    "    # Generate data for each order\n",
    "    for i in range(num_orders):\n",
    "        # Select customer (more active customers place more orders)\n",
    "        active_customers = customers_df[customers_df['status'] == 'Active']\n",
    "        \n",
    "        if len(active_customers) > 0:\n",
    "            # Prefer active customers (80% chance)\n",
    "            if random.random() < 0.8:\n",
    "                customer = active_customers.sample(1).iloc[0]\n",
    "            else:\n",
    "                customer = customers_df.sample(1).iloc[0]\n",
    "        else:\n",
    "            customer = customers_df.sample(1).iloc[0]\n",
    "            \n",
    "        data[\"customer_id\"].append(customer['customer_id'])\n",
    "        \n",
    "        # Generate order date\n",
    "        time_range_days = (end_time - start_time).days\n",
    "        days_from_start = random.randint(0, time_range_days)\n",
    "        order_date = start_time + timedelta(days=days_from_start)\n",
    "        data[\"order_date\"].append(order_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Select order type (weighted random)\n",
    "        order_type = random.choices(\n",
    "            list(order_types.keys()), \n",
    "            weights=list(order_types.values())\n",
    "        )[0]\n",
    "        data[\"order_type\"].append(order_type)\n",
    "        \n",
    "        # Generate requested delivery date based on order type\n",
    "        if order_type == \"Rush\":\n",
    "            # Rush orders have shorter delivery windows\n",
    "            delivery_window = random.randint(1, 14)  # 1-14 days\n",
    "        elif order_type == \"Standard\":\n",
    "            delivery_window = random.randint(14, 45)  # 2-6 weeks\n",
    "        elif order_type == \"Scheduled\":\n",
    "            delivery_window = random.randint(30, 90)  # 1-3 months\n",
    "        elif order_type == \"Blanket\":\n",
    "            delivery_window = random.randint(60, 180)  # 2-6 months\n",
    "        else:  # Sample\n",
    "            delivery_window = random.randint(7, 30)  # 1-4 weeks\n",
    "            \n",
    "        requested_delivery_date = order_date + timedelta(days=delivery_window)\n",
    "        data[\"requested_delivery_date\"].append(requested_delivery_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Generate promised delivery date (usually close to requested, but can vary)\n",
    "        promise_variation = random.randint(-5, 10)  # -5 to +10 days from requested\n",
    "        promised_delivery_date = requested_delivery_date + timedelta(days=promise_variation)\n",
    "        \n",
    "        # Ensure promised date is not before order date\n",
    "        if promised_delivery_date <= order_date:\n",
    "            promised_delivery_date = order_date + timedelta(days=1)\n",
    "            \n",
    "        data[\"promised_delivery_date\"].append(promised_delivery_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Determine order status based on dates\n",
    "        current_date = datetime.now()\n",
    "        \n",
    "        if order_date > current_date:\n",
    "            # Future orders are typically in Draft or Pending status\n",
    "            status = random.choice([\"Draft\", \"Pending\"])\n",
    "        elif requested_delivery_date > current_date:\n",
    "            # Current orders are In Process or Confirmed\n",
    "            status = random.choice([\"In Process\", \"Confirmed\", \"Partially Shipped\"])\n",
    "        else:\n",
    "            # Past orders are Completed, Cancelled, or On Hold\n",
    "            status_options = [\"Completed\", \"Completed\", \"Completed\", \"Cancelled\", \"On Hold\"]  # Weighted for more completed\n",
    "            status = random.choice(status_options)\n",
    "            \n",
    "        data[\"status\"].append(status)\n",
    "        \n",
    "        # Set priority (weighted random)\n",
    "        priority = random.choices(priority_levels, weights=priority_weights)[0]\n",
    "        \n",
    "        # Rush orders typically have higher priority\n",
    "        if order_type == \"Rush\" and priority > 2:\n",
    "            priority = random.randint(1, 2)\n",
    "            \n",
    "        data[\"priority\"].append(priority)\n",
    "        \n",
    "        # Generate order value (based on customer credit limit as a rough guide)\n",
    "        if 'credit_limit' in customer:\n",
    "            max_order = customer['credit_limit'] * 0.5  # Typically orders are less than 50% of credit limit\n",
    "        else:\n",
    "            max_order = 50000\n",
    "            \n",
    "        order_value = random.uniform(1000, max_order)\n",
    "        data[\"order_value\"].append(round(order_value, 2))\n",
    "        \n",
    "        # Set payment terms (use customer terms if available)\n",
    "        if 'credit_terms' in customer and pd.notna(customer['credit_terms']):\n",
    "            data[\"payment_terms\"].append(customer['credit_terms'])\n",
    "        else:\n",
    "            data[\"payment_terms\"].append(random.choice(payment_terms))\n",
    "        \n",
    "        # Set shipping method\n",
    "        data[\"shipping_method\"].append(random.choice(shipping_methods))\n",
    "        \n",
    "        # Assign sales rep\n",
    "        data[\"sales_rep_id\"].append(random.choice(sales_rep_ids))\n",
    "        \n",
    "        # Generate notes (mostly empty)\n",
    "        if random.random() < 0.2:  # 20% chance of having notes\n",
    "            notes_options = [\n",
    "                \"Customer requested special packaging\",\n",
    "                \"Delivery must be on exact date\",\n",
    "                \"Call customer before shipping\",\n",
    "                \"Include certificates of analysis\",\n",
    "                \"Partial shipments acceptable\",\n",
    "                \"Do not substitute products\",\n",
    "                f\"Reference PO #{random.randint(10000, 99999)}\",\n",
    "                \"Preferred carrier requested\",\n",
    "                \"Weekend delivery authorized\",\n",
    "                \"Contact warehouse manager upon arrival\"\n",
    "            ]\n",
    "            data[\"notes\"].append(random.choice(notes_options))\n",
    "        else:\n",
    "            data[\"notes\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} customer order records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_order_lines(customer_orders_df, products_df=None, output_file=\"data/order_lines.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the OrderLines table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - customer_orders_df: DataFrame containing customer orders data\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated order lines data\n",
    "    \"\"\"\n",
    "    if customer_orders_df is None or len(customer_orders_df) == 0:\n",
    "        print(\"Error: No customer orders data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Generate product IDs if products_df is not provided\n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"Generating synthetic product IDs...\")\n",
    "        product_ids = [f\"PROD-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "        \n",
    "        # Create synthetic product prices\n",
    "        product_prices = {}\n",
    "        for prod_id in product_ids:\n",
    "            product_prices[prod_id] = random.uniform(10, 1000)\n",
    "    else:\n",
    "        # Use actual product IDs and prices from products_df\n",
    "        product_ids = products_df['product_id'].tolist()\n",
    "        \n",
    "        # Create price mapping\n",
    "        product_prices = {}\n",
    "        for _, product in products_df.iterrows():\n",
    "            if 'list_price' in product and pd.notna(product['list_price']):\n",
    "                product_prices[product['product_id']] = product['list_price']\n",
    "            else:\n",
    "                product_prices[product['product_id']] = random.uniform(10, 1000)\n",
    "    \n",
    "    # Generate work order IDs\n",
    "    work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(100)]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"line_id\": [],\n",
    "        \"order_id\": [],\n",
    "        \"line_number\": [],\n",
    "        \"product_id\": [],\n",
    "        \"quantity\": [],\n",
    "        \"unit_price\": [],\n",
    "        \"line_value\": [],\n",
    "        \"requested_delivery_date\": [],\n",
    "        \"promised_delivery_date\": [],\n",
    "        \"status\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"shipped_quantity\": [],\n",
    "        \"shipping_date\": []\n",
    "    }\n",
    "    \n",
    "    # Process each customer order\n",
    "    for _, order in customer_orders_df.iterrows():\n",
    "        order_id = order['order_id']\n",
    "        order_status = order['status']\n",
    "        \n",
    "        # Determine number of line items for this order\n",
    "        num_lines = random.randint(1, 10)  # 1-10 line items per order\n",
    "        \n",
    "        # Keep track of selected products for this order to avoid duplicates\n",
    "        selected_products = []\n",
    "        \n",
    "        # Get order dates\n",
    "        order_date = pd.to_datetime(order['order_date'])\n",
    "        requested_delivery_date = pd.to_datetime(order['requested_delivery_date'])\n",
    "        promised_delivery_date = pd.to_datetime(order['promised_delivery_date'])\n",
    "        \n",
    "        # Generate line items\n",
    "        for line_num in range(1, num_lines + 1):\n",
    "            # Create unique line ID\n",
    "            line_id = f\"LINE-{uuid.uuid4().hex[:8].upper()}\"\n",
    "            data[\"line_id\"].append(line_id)\n",
    "            data[\"order_id\"].append(order_id)\n",
    "            data[\"line_number\"].append(line_num)\n",
    "            \n",
    "            # Select product (avoid duplicates within same order)\n",
    "            available_products = [p for p in product_ids if p not in selected_products]\n",
    "            \n",
    "            if not available_products:\n",
    "                # If we've used all products, just pick a random one\n",
    "                product_id = random.choice(product_ids)\n",
    "            else:\n",
    "                product_id = random.choice(available_products)\n",
    "                selected_products.append(product_id)\n",
    "                \n",
    "            data[\"product_id\"].append(product_id)\n",
    "            \n",
    "            # Generate quantity\n",
    "            quantity = random.randint(1, 1000)\n",
    "            data[\"quantity\"].append(quantity)\n",
    "            \n",
    "            # Get unit price\n",
    "            if product_id in product_prices:\n",
    "                unit_price = product_prices[product_id]\n",
    "            else:\n",
    "                unit_price = random.uniform(10, 1000)\n",
    "                \n",
    "            # Apply random discount/markup\n",
    "            price_adjustment = random.uniform(0.9, 1.1)  # -10% to +10%\n",
    "            unit_price = unit_price * price_adjustment\n",
    "            \n",
    "            data[\"unit_price\"].append(round(unit_price, 2))\n",
    "            \n",
    "            # Calculate line value\n",
    "            line_value = quantity * unit_price\n",
    "            data[\"line_value\"].append(round(line_value, 2))\n",
    "            \n",
    "            # Set delivery dates (can vary slightly from order dates for individual lines)\n",
    "            line_req_variation = random.randint(-3, 3)  # +/- 3 days\n",
    "            line_requested_date = requested_delivery_date + timedelta(days=line_req_variation)\n",
    "            \n",
    "            line_prom_variation = random.randint(-2, 2)  # +/- 2 days\n",
    "            line_promised_date = promised_delivery_date + timedelta(days=line_prom_variation)\n",
    "            \n",
    "            # Ensure dates make sense\n",
    "            if line_requested_date < order_date:\n",
    "                line_requested_date = order_date + timedelta(days=1)\n",
    "                \n",
    "            if line_promised_date < order_date:\n",
    "                line_promised_date = order_date + timedelta(days=1)\n",
    "                \n",
    "            data[\"requested_delivery_date\"].append(line_requested_date.strftime(\"%Y-%m-%d\"))\n",
    "            data[\"promised_delivery_date\"].append(line_promised_date.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            # Set line status based on order status\n",
    "            if order_status == \"Draft\" or order_status == \"Pending\":\n",
    "                line_status = order_status\n",
    "                data[\"work_order_id\"].append(\"\")\n",
    "                data[\"shipped_quantity\"].append(0)\n",
    "                data[\"shipping_date\"].append(\"\")\n",
    "                \n",
    "            elif order_status == \"Confirmed\":\n",
    "                line_status = \"Confirmed\"\n",
    "                \n",
    "                # Some confirmed orders have work orders\n",
    "                if random.random() < 0.7:  # 70% chance\n",
    "                    data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "                else:\n",
    "                    data[\"work_order_id\"].append(\"\")\n",
    "                    \n",
    "                data[\"shipped_quantity\"].append(0)\n",
    "                data[\"shipping_date\"].append(\"\")\n",
    "                \n",
    "            elif order_status == \"In Process\":\n",
    "                line_statuses = [\"Confirmed\", \"In Production\", \"Ready to Ship\", \"Partially Shipped\"]\n",
    "                line_status = random.choice(line_statuses)\n",
    "                \n",
    "                # Most in-process lines have work orders\n",
    "                if random.random() < 0.9:  # 90% chance\n",
    "                    data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "                else:\n",
    "                    data[\"work_order_id\"].append(\"\")\n",
    "                \n",
    "                # Some lines may be partially shipped\n",
    "                if line_status == \"Partially Shipped\":\n",
    "                    shipped_qty = random.randint(1, quantity - 1)\n",
    "                    data[\"shipped_quantity\"].append(shipped_qty)\n",
    "                    \n",
    "                    # Shipping date is between order date and current date\n",
    "                    days_difference = (datetime.now() - order_date).days\n",
    "                    if days_difference >= 1:\n",
    "                        ship_days = random.randint(1, days_difference)\n",
    "                        shipping_date = order_date + timedelta(days=ship_days)\n",
    "                        data[\"shipping_date\"].append(shipping_date.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        # Handle the case when order_date is today or in the future\n",
    "                        data[\"shipping_date\"].append(\"\")\n",
    "\n",
    "                else:\n",
    "                    data[\"shipped_quantity\"].append(0)\n",
    "                    data[\"shipping_date\"].append(\"\")\n",
    "                \n",
    "            elif order_status == \"Partially Shipped\":\n",
    "                # Mix of shipped and unshipped lines\n",
    "                if random.random() < 0.6:  # 60% chance this line is shipped\n",
    "                    line_status = \"Shipped\"\n",
    "                    data[\"shipped_quantity\"].append(quantity)\n",
    "                    \n",
    "                    # Shipping date is between order date and current date\n",
    "                    days_difference = (datetime.now() - order_date).days\n",
    "                    if days_difference >= 1:\n",
    "                        ship_days = random.randint(1, days_difference)\n",
    "                        shipping_date = order_date + timedelta(days=ship_days)\n",
    "                        data[\"shipping_date\"].append(shipping_date.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        # Handle the case when order_date is today or in the future\n",
    "                        data[\"shipping_date\"].append(\"\")\n",
    "                else:\n",
    "                    line_status = random.choice([\"Confirmed\", \"In Production\", \"Ready to Ship\"])\n",
    "                    data[\"shipped_quantity\"].append(0)\n",
    "                    data[\"shipping_date\"].append(\"\")\n",
    "                \n",
    "                # Most lines have work orders\n",
    "                if random.random() < 0.9:  # 90% chance\n",
    "                    data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "                else:\n",
    "                    data[\"work_order_id\"].append(\"\")\n",
    "                \n",
    "            elif order_status == \"Completed\":\n",
    "                line_status = \"Shipped\"\n",
    "                data[\"shipped_quantity\"].append(quantity)\n",
    "                \n",
    "                # Shipping date is between order date and promised date\n",
    "                ship_days = random.randint(1, (promised_delivery_date - order_date).days)\n",
    "                shipping_date = order_date + timedelta(days=ship_days)\n",
    "                data[\"shipping_date\"].append(shipping_date.strftime(\"%Y-%m-%d\"))\n",
    "                \n",
    "                # Most completed lines have work orders\n",
    "                if random.random() < 0.95:  # 95% chance\n",
    "                    data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "                else:\n",
    "                    data[\"work_order_id\"].append(\"\")\n",
    "                \n",
    "            elif order_status == \"Cancelled\":\n",
    "                line_status = \"Cancelled\"\n",
    "                data[\"work_order_id\"].append(\"\")\n",
    "                data[\"shipped_quantity\"].append(0)\n",
    "                data[\"shipping_date\"].append(\"\")\n",
    "                \n",
    "            else:  # On Hold\n",
    "                line_status = \"On Hold\"\n",
    "                \n",
    "                # Some on-hold orders have work orders\n",
    "                if random.random() < 0.4:  # 40% chance\n",
    "                    data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "                else:\n",
    "                    data[\"work_order_id\"].append(\"\")\n",
    "                    \n",
    "                data[\"shipped_quantity\"].append(0)\n",
    "                data[\"shipping_date\"].append(\"\")\n",
    "            \n",
    "            data[\"status\"].append(line_status)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} order line records for {df['order_id'].nunique()} orders.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(customers_df, customer_orders_df=None, order_lines_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated customers, orders, and order lines data\n",
    "    \n",
    "    Parameters:\n",
    "    - customers_df: DataFrame containing customers data\n",
    "    - customer_orders_df: DataFrame containing customer orders data (optional)\n",
    "    - order_lines_df: DataFrame containing order lines data (optional)\n",
    "    \"\"\"\n",
    "    if customers_df is None or len(customers_df) == 0:\n",
    "        print(\"No customers data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nCustomers Statistics:\")\n",
    "    print(f\"Total customers: {len(customers_df)}\")\n",
    "    \n",
    "    # Customer type distribution\n",
    "    print(\"\\nCustomer Type Distribution:\")\n",
    "    type_counts = customers_df['customer_type'].value_counts()\n",
    "    for cust_type, count in type_counts.items():\n",
    "        print(f\"  {cust_type}: {count} ({count/len(customers_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Industry distribution\n",
    "    print(\"\\nIndustry Distribution (top 10):\")\n",
    "    industry_counts = customers_df['industry'].value_counts().head(10)\n",
    "    for industry, count in industry_counts.items():\n",
    "        print(f\"  {industry}: {count} ({count/len(customers_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Status distribution\n",
    "    print(\"\\nCustomer Status Distribution:\")\n",
    "    status_counts = customers_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(customers_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Credit terms distribution\n",
    "    print(\"\\nCredit Terms Distribution:\")\n",
    "    terms_counts = customers_df['credit_terms'].value_counts()\n",
    "    for terms, count in terms_counts.items():\n",
    "        print(f\"  {terms}: {count} ({count/len(customers_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Credit limit statistics\n",
    "    print(\"\\nCredit Limit Statistics:\")\n",
    "    print(f\"  Average credit limit: ${customers_df['credit_limit'].mean():.2f}\")\n",
    "    print(f\"  Minimum credit limit: ${customers_df['credit_limit'].min():.2f}\")\n",
    "    print(f\"  Maximum credit limit: ${customers_df['credit_limit'].max():.2f}\")\n",
    "    \n",
    "    # Regional distribution (extracted from address)\n",
    "    if 'address' in customers_df.columns:\n",
    "        print(\"\\nRegional Distribution:\")\n",
    "        \n",
    "        # Extract country from address\n",
    "        customers_df['country'] = customers_df['address'].apply(lambda x: x.split(',')[-1].strip() if isinstance(x, str) else \"Unknown\")\n",
    "        \n",
    "        country_counts = customers_df['country'].value_counts().head(10)\n",
    "        for country, count in country_counts.items():\n",
    "            print(f\"  {country}: {count} ({count/len(customers_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Customer orders statistics (if available)\n",
    "    if customer_orders_df is not None and len(customer_orders_df) > 0:\n",
    "        print(\"\\nCustomer Orders Statistics:\")\n",
    "        print(f\"Total orders: {len(customer_orders_df)}\")\n",
    "        print(f\"Customers with orders: {customer_orders_df['customer_id'].nunique()}\")\n",
    "        \n",
    "        # Order type distribution\n",
    "        print(\"\\nOrder Type Distribution:\")\n",
    "        order_type_counts = customer_orders_df['order_type'].value_counts()\n",
    "        for order_type, count in order_type_counts.items():\n",
    "            print(f\"  {order_type}: {count} ({count/len(customer_orders_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Order status distribution\n",
    "        print(\"\\nOrder Status Distribution:\")\n",
    "        order_status_counts = customer_orders_df['status'].value_counts()\n",
    "        for status, count in order_status_counts.items():\n",
    "            print(f\"  {status}: {count} ({count/len(customer_orders_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Priority distribution\n",
    "        print(\"\\nOrder Priority Distribution:\")\n",
    "        priority_counts = customer_orders_df['priority'].value_counts().sort_index()\n",
    "        for priority, count in priority_counts.items():\n",
    "            print(f\"  Priority {priority}: {count} ({count/len(customer_orders_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Order value statistics\n",
    "        print(\"\\nOrder Value Statistics:\")\n",
    "        print(f\"  Average order value: ${customer_orders_df['order_value'].mean():.2f}\")\n",
    "        print(f\"  Minimum order value: ${customer_orders_df['order_value'].min():.2f}\")\n",
    "        print(f\"  Maximum order value: ${customer_orders_df['order_value'].max():.2f}\")\n",
    "        print(f\"  Total order value: ${customer_orders_df['order_value'].sum():.2f}\")\n",
    "        \n",
    "        # Shipping method distribution\n",
    "        print(\"\\nShipping Method Distribution:\")\n",
    "        shipping_counts = customer_orders_df['shipping_method'].value_counts()\n",
    "        for method, count in shipping_counts.items():\n",
    "            print(f\"  {method}: {count} ({count/len(customer_orders_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Orders per customer\n",
    "        orders_per_customer = customer_orders_df.groupby('customer_id').size()\n",
    "        print(\"\\nOrders Per Customer Statistics:\")\n",
    "        print(f\"  Average orders per customer: {orders_per_customer.mean():.1f}\")\n",
    "        print(f\"  Maximum orders per customer: {orders_per_customer.max()}\")\n",
    "        \n",
    "        # Date-based analysis\n",
    "        customer_orders_df['order_date'] = pd.to_datetime(customer_orders_df['order_date'])\n",
    "        \n",
    "        # Orders by month\n",
    "        customer_orders_df['order_month'] = customer_orders_df['order_date'].dt.to_period('M')\n",
    "        monthly_orders = customer_orders_df.groupby('order_month').size().sort_index()\n",
    "        \n",
    "        print(\"\\nMonthly Order Distribution (last 6 months):\")\n",
    "        for month, count in monthly_orders.tail(6).items():\n",
    "            print(f\"  {month}: {count} orders\")\n",
    "    \n",
    "    # Order lines statistics (if available)\n",
    "    if order_lines_df is not None and len(order_lines_df) > 0:\n",
    "        print(\"\\nOrder Lines Statistics:\")\n",
    "        print(f\"Total order lines: {len(order_lines_df)}\")\n",
    "        print(f\"Orders with lines: {order_lines_df['order_id'].nunique()}\")\n",
    "        \n",
    "        # Lines per order\n",
    "        lines_per_order = order_lines_df.groupby('order_id').size()\n",
    "        print(\"\\nLines Per Order Statistics:\")\n",
    "        print(f\"  Average lines per order: {lines_per_order.mean():.1f}\")\n",
    "        print(f\"  Minimum lines per order: {lines_per_order.min()}\")\n",
    "        print(f\"  Maximum lines per order: {lines_per_order.max()}\")\n",
    "        \n",
    "        # Line status distribution\n",
    "        print(\"\\nLine Status Distribution:\")\n",
    "        line_status_counts = order_lines_df['status'].value_counts()\n",
    "        for status, count in line_status_counts.items():\n",
    "            print(f\"  {status}: {count} ({count/len(order_lines_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Line value statistics\n",
    "        print(\"\\nLine Value Statistics:\")\n",
    "        print(f\"  Average line value: ${order_lines_df['line_value'].mean():.2f}\")\n",
    "        print(f\"  Total line value: ${order_lines_df['line_value'].sum():.2f}\")\n",
    "        \n",
    "        # Quantity statistics\n",
    "        print(\"\\nQuantity Statistics:\")\n",
    "        print(f\"  Average order quantity: {order_lines_df['quantity'].mean():.1f}\")\n",
    "        print(f\"  Average unit price: ${order_lines_df['unit_price'].mean():.2f}\")\n",
    "        \n",
    "        # Work order association\n",
    "        has_wo = order_lines_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "        print(f\"\\nLines with work order association: {has_wo} ({has_wo/len(order_lines_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Shipping statistics\n",
    "        shipped_lines = order_lines_df[order_lines_df['shipped_quantity'] > 0]\n",
    "        if len(shipped_lines) > 0:\n",
    "            print(\"\\nShipping Statistics:\")\n",
    "            print(f\"  Shipped lines: {len(shipped_lines)} ({len(shipped_lines)/len(order_lines_df)*100:.1f}%)\")\n",
    "            print(f\"  Total shipped quantity: {shipped_lines['shipped_quantity'].sum()}\")\n",
    "            \n",
    "            # Calculate ship-to-promise ratio\n",
    "            shipped_lines['shipping_date'] = pd.to_datetime(shipped_lines['shipping_date'])\n",
    "            shipped_lines['promised_delivery_date'] = pd.to_datetime(shipped_lines['promised_delivery_date'])\n",
    "            \n",
    "            # On-time shipments (shipped on or before promised date)\n",
    "            on_time = shipped_lines[shipped_lines['shipping_date'] <= shipped_lines['promised_delivery_date']]\n",
    "            on_time_pct = len(on_time) / len(shipped_lines) * 100\n",
    "            \n",
    "            print(f\"  On-time shipments: {len(on_time)} ({on_time_pct:.1f}% of shipped lines)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Try to load products data if available\n",
    "    try:\n",
    "        products_df = pd.read_csv(\"data/products.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Note: Products data file not found.\")\n",
    "        products_df = None\n",
    "    \n",
    "    # Generate customers data\n",
    "    customers_df = generate_customers_data(\n",
    "        num_customers=100,  # Generate 100 customer records\n",
    "        output_file=\"data/customers.csv\"\n",
    "    )\n",
    "    \n",
    "    if customers_df is not None:\n",
    "        # Generate customer orders data\n",
    "        customer_orders_df = generate_customer_orders(\n",
    "            customers_df,\n",
    "            products_df,\n",
    "            num_orders=300,  # Generate 300 order records\n",
    "            output_file=\"data/customer_orders.csv\"\n",
    "        )\n",
    "        \n",
    "        if customer_orders_df is not None:\n",
    "            # Generate order lines data\n",
    "            order_lines_df = generate_order_lines(\n",
    "                customer_orders_df,\n",
    "                products_df,\n",
    "                output_file=\"data/order_lines.csv\"\n",
    "            )\n",
    "        else:\n",
    "            order_lines_df = None\n",
    "    else:\n",
    "        customer_orders_df = None\n",
    "        order_lines_df = None\n",
    "    \n",
    "    # Display statistics\n",
    "    if customers_df is not None:\n",
    "        display_statistics(customers_df, customer_orders_df, order_lines_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample customers data (first 5 records):\")\n",
    "        print(customers_df.head(5))\n",
    "        \n",
    "        if customer_orders_df is not None:\n",
    "            print(\"\\nSample customer orders data (first 5 records):\")\n",
    "            print(customer_orders_df.head(5))\n",
    "            \n",
    "            if order_lines_df is not None:\n",
    "                print(\"\\nSample order lines data (first 5 records):\")\n",
    "                print(order_lines_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0a5cc",
   "metadata": {},
   "source": [
    "Suppliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6642e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 50 supplier records.\n",
      "Data saved to data/suppliers.csv\n",
      "\n",
      "Sample suppliers data (first 5 records):\n",
      "    supplier_id           supplier_name     supplier_type   contact_person  \\\n",
      "0  SUP-50348391       Robinson Services  Service Provider    Betty Johnson   \n",
      "1  SUP-7A0426D3  Hernandez Distributors       Distributor  Jennifer Taylor   \n",
      "2  SUP-5CF5DECA       Central Logistics       Distributor   Michael Walker   \n",
      "3  SUP-6330D222             Great Group  Service Provider     Daniel Lopez   \n",
      "4  SUP-9EF5B17E         Best Industries      Manufacturer       Lisa Perez   \n",
      "\n",
      "                                       email              phone  \\\n",
      "0         betty.johnson@robinsonservices.org  +157 975 903 1133   \n",
      "1  jennifer.taylor@hernandezdistributors.biz  +981 290 886 5916   \n",
      "2        michael.walker@centrallogistics.net  +125 108 159 4947   \n",
      "3                 daniel.lopez@greatgroup.co  +466 556 749 1046   \n",
      "4              lisa.perez@bestindustries.org  +213 931 721 4937   \n",
      "\n",
      "                                        address payment_terms  lead_time_days  \\\n",
      "0      9060 Davis Terrace, Martinezville, China        Net 30              16   \n",
      "1           396 Taylor Avenue, Clarkford, Spain        Net 45              21   \n",
      "2             1956 Jones Lane, Moorebridge, USA           COD              27   \n",
      "3     7588 Anderson Drive, Williamshaven, Japan        Net 30              54   \n",
      "4  984 Harris Drive, Lower Thompsontown, France   2/10 Net 30             127   \n",
      "\n",
      "   quality_rating  status                                  primary_materials  \\\n",
      "0             2.8  Active  ['Testing Laboratory', 'Cleaning Service', 'Ca...   \n",
      "1             1.8  Active            ['MRO Supplier', 'Laboratory Supplies']   \n",
      "2             4.1  Active                                ['Bottle Supplier']   \n",
      "3             4.6  Active  ['Cleaning Service', 'Engineering Consultant',...   \n",
      "4             2.3  Active                             ['Machinery Supplier']   \n",
      "\n",
      "  notes  \n",
      "0        \n",
      "1        \n",
      "2        \n",
      "3        \n",
      "4        \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_suppliers_data(num_suppliers=50, output_file=\"data/suppliers.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the Suppliers table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_suppliers: Number of supplier records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated suppliers data\n",
    "    \"\"\"\n",
    "    # Define supplier types and their probabilities\n",
    "    supplier_types = {\n",
    "        \"Manufacturer\": 0.4,\n",
    "        \"Distributor\": 0.25,\n",
    "        \"Wholesaler\": 0.15,\n",
    "        \"Service Provider\": 0.1,\n",
    "        \"Contractor\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Define supplier categories by material type\n",
    "    supplier_categories = {\n",
    "        \"Raw Material\": [\"Chemical Supplier\", \"Industrial Raw Material\", \"Commodity Supplier\", \n",
    "                      \"Mining Company\", \"Agricultural Supplier\", \"Petroleum Supplier\"],\n",
    "        \"Packaging\": [\"Packaging Manufacturer\", \"Container Supplier\", \"Label Supplier\", \n",
    "                   \"Film Supplier\", \"Box Manufacturer\", \"Bottle Supplier\"],\n",
    "        \"Equipment\": [\"Equipment Manufacturer\", \"Machinery Supplier\", \"Tool Vendor\", \n",
    "                   \"Instrumentation Supplier\", \"Automation Provider\", \"Parts Supplier\"],\n",
    "        \"Service\": [\"Maintenance Service\", \"Calibration Service\", \"Cleaning Service\", \n",
    "                 \"Engineering Consultant\", \"Testing Laboratory\", \"Transportation Provider\"],\n",
    "        \"Consumable\": [\"Laboratory Supplies\", \"MRO Supplier\", \"Office Supplies\", \n",
    "                    \"Safety Equipment\", \"Utility Provider\", \"IT Service Provider\"]\n",
    "    }\n",
    "    \n",
    "    # Define payment terms\n",
    "    payment_terms = [\"Net 30\", \"Net 45\", \"Net 60\", \"2/10 Net 30\", \"COD\", \"Net 15\"]\n",
    "    payment_terms_weights = [0.4, 0.2, 0.15, 0.1, 0.1, 0.05]  # Probabilities\n",
    "    \n",
    "    # Define regions for address generation\n",
    "    regions = {\n",
    "        \"North America\": [\"USA\", \"Canada\", \"Mexico\"],\n",
    "        \"Europe\": [\"UK\", \"Germany\", \"France\", \"Italy\", \"Spain\", \"Netherlands\"],\n",
    "        \"Asia\": [\"Japan\", \"China\", \"South Korea\", \"India\", \"Singapore\", \"Taiwan\"],\n",
    "        \"Latin America\": [\"Brazil\", \"Colombia\", \"Argentina\", \"Chile\", \"Peru\"],\n",
    "        \"Oceania\": [\"Australia\", \"New Zealand\"]\n",
    "    }\n",
    "    \n",
    "    # Region probabilities (adjust as needed for your supply chain model)\n",
    "    region_weights = {\n",
    "        \"North America\": 0.35,\n",
    "        \"Europe\": 0.25,\n",
    "        \"Asia\": 0.25,\n",
    "        \"Latin America\": 0.1,\n",
    "        \"Oceania\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"supplier_id\": [f\"SUP-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_suppliers)],\n",
    "        \"supplier_name\": [],\n",
    "        \"supplier_type\": [],\n",
    "        \"contact_person\": [],\n",
    "        \"email\": [],\n",
    "        \"phone\": [],\n",
    "        \"address\": [],\n",
    "        \"payment_terms\": [],\n",
    "        \"lead_time_days\": [],\n",
    "        \"quality_rating\": [],\n",
    "        \"status\": [],\n",
    "        \"primary_materials\": [],\n",
    "        \"notes\": []\n",
    "    }\n",
    "    \n",
    "    # First names for contact generation\n",
    "    first_names = [\n",
    "        \"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\", \n",
    "        \"William\", \"Elizabeth\", \"David\", \"Barbara\", \"Richard\", \"Susan\", \"Joseph\", \"Jessica\", \n",
    "        \"Thomas\", \"Sarah\", \"Charles\", \"Karen\", \"Christopher\", \"Nancy\", \"Daniel\", \"Lisa\", \n",
    "        \"Matthew\", \"Margaret\", \"Anthony\", \"Betty\", \"Mark\", \"Sandra\", \"Donald\", \"Ashley\", \n",
    "        \"Steven\", \"Kimberly\", \"Paul\", \"Emily\", \"Andrew\", \"Donna\", \"Joshua\", \"Michelle\"\n",
    "    ]\n",
    "    \n",
    "    # Last names for contact generation\n",
    "    last_names = [\n",
    "        \"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \n",
    "        \"Rodriguez\", \"Martinez\", \"Hernandez\", \"Lopez\", \"Gonzalez\", \"Wilson\", \"Anderson\", \n",
    "        \"Thomas\", \"Taylor\", \"Moore\", \"Jackson\", \"Martin\", \"Lee\", \"Perez\", \"Thompson\", \n",
    "        \"White\", \"Harris\", \"Sanchez\", \"Clark\", \"Ramirez\", \"Lewis\", \"Robinson\", \"Walker\"\n",
    "    ]\n",
    "    \n",
    "    # Company name components for generating realistic company names\n",
    "    company_prefixes = [\n",
    "        \"Advanced\", \"Allied\", \"American\", \"Asian\", \"Atlantic\", \"Best\", \"Better\", \"Blue\", \n",
    "        \"Bright\", \"Central\", \"Century\", \"Consolidated\", \"Continental\", \"Digital\", \"Dynamic\", \n",
    "        \"East\", \"Eastern\", \"Euro\", \"European\", \"Express\", \"First\", \"Global\", \"Golden\", \n",
    "        \"Great\", \"Green\", \"International\", \"Metro\", \"Modern\", \"National\", \"New\", \"North\"\n",
    "    ]\n",
    "    \n",
    "    company_types = {\n",
    "        \"Manufacturer\": [\"Manufacturing\", \"Industries\", \"Fabrication\", \"Production\", \"Processors\"],\n",
    "        \"Distributor\": [\"Distribution\", \"Distributors\", \"Supply\", \"Supplies\", \"Logistics\"],\n",
    "        \"Wholesaler\": [\"Wholesale\", \"Trading\", \"Merchandise\", \"Commerce\", \"Exchange\"],\n",
    "        \"Service Provider\": [\"Services\", \"Solutions\", \"Consulting\", \"Associates\", \"Group\"],\n",
    "        \"Contractor\": [\"Contracting\", \"Construction\", \"Engineering\", \"Installations\", \"Systems\"]\n",
    "    }\n",
    "    \n",
    "    # Generate data for each supplier\n",
    "    for i in range(num_suppliers):\n",
    "        # Select supplier type (weighted random)\n",
    "        supplier_type = random.choices(\n",
    "            list(supplier_types.keys()), \n",
    "            weights=list(supplier_types.values())\n",
    "        )[0]\n",
    "        data[\"supplier_type\"].append(supplier_type)\n",
    "        \n",
    "        # Generate a realistic company name\n",
    "        if random.random() < 0.7:  # 70% chance of using prefix\n",
    "            prefix = random.choice(company_prefixes)\n",
    "            if supplier_type in company_types:\n",
    "                suffix = random.choice(company_types[supplier_type])\n",
    "            else:\n",
    "                suffix = random.choice(list(company_types.values())[0])\n",
    "                \n",
    "            company_name = f\"{prefix} {suffix}\"\n",
    "        else:\n",
    "            # Use a last name\n",
    "            last_name = random.choice(last_names)\n",
    "            if supplier_type in company_types:\n",
    "                suffix = random.choice(company_types[supplier_type])\n",
    "            else:\n",
    "                suffix = random.choice(list(company_types.values())[0])\n",
    "                \n",
    "            company_name = f\"{last_name} {suffix}\"\n",
    "        \n",
    "        data[\"supplier_name\"].append(company_name)\n",
    "        \n",
    "        # Generate contact person (random first and last name)\n",
    "        contact_first = random.choice(first_names)\n",
    "        contact_last = random.choice(last_names)\n",
    "        data[\"contact_person\"].append(f\"{contact_first} {contact_last}\")\n",
    "        \n",
    "        # Generate email (company domain based on name)\n",
    "        company_domain = company_name.lower().replace(\" \", \"\").replace(\".\", \"\")\n",
    "        email_domains = [\".com\", \".net\", \".org\", \".co\", \".biz\"]\n",
    "        email_domain = random.choice(email_domains)\n",
    "        data[\"email\"].append(f\"{contact_first.lower()}.{contact_last.lower()}@{company_domain}{email_domain}\")\n",
    "        \n",
    "        # Generate phone\n",
    "        data[\"phone\"].append(f\"+{random.randint(1, 9)}{random.randint(10, 99)} {random.randint(100, 999)} {random.randint(100, 999)} {random.randint(1000, 9999)}\")\n",
    "        \n",
    "        # Generate address based on region probability\n",
    "        region = random.choices(\n",
    "            list(region_weights.keys()), \n",
    "            weights=list(region_weights.values())\n",
    "        )[0]\n",
    "        \n",
    "        country = random.choice(regions[region])\n",
    "        \n",
    "        # Generate a city name (simplified)\n",
    "        city_prefixes = [\"New\", \"Old\", \"East\", \"West\", \"North\", \"South\", \"Central\", \"Upper\", \"Lower\", \"Port\", \"Lake\", \"Mount\", \"Fort\"]\n",
    "        city_suffixes = [\"town\", \"ville\", \"burg\", \"berg\", \"field\", \"ford\", \"port\", \"mouth\", \"stad\", \"furt\", \"chester\", \"cester\", \"bridge\", \"haven\", \"minster\"]\n",
    "        \n",
    "        if random.random() < 0.3:  # 30% chance of using prefix\n",
    "            city = f\"{random.choice(city_prefixes)} {random.choice(last_names)}{random.choice(['', random.choice(city_suffixes)])}\"\n",
    "        else:\n",
    "            city = f\"{random.choice(last_names)}{random.choice(['', random.choice(city_suffixes)])}\"\n",
    "        \n",
    "        # Generate street address\n",
    "        street_number = random.randint(1, 9999)\n",
    "        street_types = [\"Street\", \"Avenue\", \"Boulevard\", \"Road\", \"Lane\", \"Drive\", \"Way\", \"Place\", \"Court\", \"Terrace\"]\n",
    "        street_name = f\"{random.choice(last_names)} {random.choice(street_types)}\"\n",
    "        \n",
    "        address = f\"{street_number} {street_name}, {city}, {country}\"\n",
    "        data[\"address\"].append(address)\n",
    "        \n",
    "        # Set payment terms (weighted random)\n",
    "        payment_term = random.choices(payment_terms, weights=payment_terms_weights)[0]\n",
    "        data[\"payment_terms\"].append(payment_term)\n",
    "        \n",
    "        # Set lead time based on supplier type and region\n",
    "        if supplier_type in [\"Manufacturer\", \"Contractor\"]:\n",
    "            base_lead_time = random.randint(30, 90)  # Longer lead times for manufacturers\n",
    "        else:\n",
    "            base_lead_time = random.randint(7, 45)   # Shorter for distributors\n",
    "            \n",
    "        # Adjust for region (international suppliers have longer lead times)\n",
    "        if region == \"North America\":\n",
    "            lead_time_multiplier = 1.0\n",
    "        elif region == \"Europe\" or region == \"Oceania\":\n",
    "            lead_time_multiplier = 1.5\n",
    "        elif region == \"Asia\":\n",
    "            lead_time_multiplier = 1.8\n",
    "        else:\n",
    "            lead_time_multiplier = 1.3\n",
    "            \n",
    "        lead_time = int(base_lead_time * lead_time_multiplier)\n",
    "        data[\"lead_time_days\"].append(lead_time)\n",
    "        \n",
    "        # Set quality rating (1-5 scale, 5 being best)\n",
    "        # Most suppliers should be good (3-5) with fewer poor suppliers\n",
    "        quality_weights = [0.05, 0.15, 0.30, 0.35, 0.15]  # Weights for ratings 1-5\n",
    "        quality_rating = round(random.choices([1, 2, 3, 4, 5], weights=quality_weights)[0], 1)\n",
    "        \n",
    "        # Add some random decimal to make it more realistic\n",
    "        if quality_rating < 5:\n",
    "            quality_rating += round(random.uniform(0, 0.9), 1)\n",
    "            \n",
    "        data[\"quality_rating\"].append(quality_rating)\n",
    "        \n",
    "        # Set status (mostly active)\n",
    "        statuses = [\"Active\", \"Inactive\", \"On Hold\", \"New\", \"Disqualified\"]\n",
    "        status_weights = [0.8, 0.05, 0.05, 0.07, 0.03]  # Probabilities\n",
    "        data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        \n",
    "        # Set primary materials/categories\n",
    "        num_categories = random.randint(1, 3)  # 1-3 primary categories per supplier\n",
    "        categories = []\n",
    "        \n",
    "        # Select based on supplier type\n",
    "        if supplier_type == \"Manufacturer\":\n",
    "            category_pool = list(supplier_categories[\"Raw Material\"]) + list(supplier_categories[\"Equipment\"])\n",
    "        elif supplier_type == \"Distributor\":\n",
    "            category_pool = list(supplier_categories[\"Raw Material\"]) + list(supplier_categories[\"Packaging\"]) + list(supplier_categories[\"Consumable\"])\n",
    "        elif supplier_type == \"Service Provider\":\n",
    "            category_pool = list(supplier_categories[\"Service\"])\n",
    "        else:\n",
    "            # Mix of all categories\n",
    "            category_pool = []\n",
    "            for cat_list in supplier_categories.values():\n",
    "                category_pool.extend(cat_list)\n",
    "                \n",
    "        categories = random.sample(category_pool, min(num_categories, len(category_pool)))\n",
    "        data[\"primary_materials\"].append(str(categories))\n",
    "        \n",
    "        # Generate notes (mostly empty)\n",
    "        if random.random() < 0.3:  # 30% chance of having notes\n",
    "            notes_options = [\n",
    "                \"Preferred supplier for critical materials\",\n",
    "                \"Requires minimum order quantities\",\n",
    "                \"ISO 9001 certified\",\n",
    "                \"Long-term contract in place\",\n",
    "                f\"Annual review scheduled for Q{random.randint(1, 4)}\",\n",
    "                \"Sustainability certified\",\n",
    "                \"Offers volume discounts\",\n",
    "                \"Approved for regulated materials\",\n",
    "                \"Subject to import restrictions\",\n",
    "                \"Can provide rush delivery\"\n",
    "            ]\n",
    "            data[\"notes\"].append(random.choice(notes_options))\n",
    "        else:\n",
    "            data[\"notes\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} supplier records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Generate suppliers data\n",
    "    suppliers_df = generate_suppliers_data(\n",
    "        num_suppliers=50,  # Generate 50 supplier records\n",
    "        output_file=\"data/suppliers.csv\"\n",
    "    )\n",
    "    \n",
    "    # Display sample data\n",
    "    if suppliers_df is not None:\n",
    "        print(\"\\nSample suppliers data (first 5 records):\")\n",
    "        print(suppliers_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52f264",
   "metadata": {},
   "source": [
    "Purchase Orders & Purchase Order Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2047ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Purchase orders will be generated with synthetic buyer IDs.\n",
      "Generating synthetic buyer IDs...\n",
      "Successfully generated 200 purchase order records.\n",
      "Data saved to data/purchase_orders.csv\n",
      "Successfully generated 1005 purchase order line records for 200 purchase orders.\n",
      "Data saved to data/purchase_order_lines.csv\n",
      "\n",
      "Purchase Orders Statistics:\n",
      "Total purchase orders: 200\n",
      "\n",
      "Purchase Order Status Distribution:\n",
      "  Completed: 85 (42.5%)\n",
      "  Closed: 37 (18.5%)\n",
      "  Cancelled: 34 (17.0%)\n",
      "  In Process: 12 (6.0%)\n",
      "  Partially Received: 11 (5.5%)\n",
      "  Pending Approval: 8 (4.0%)\n",
      "  Draft: 8 (4.0%)\n",
      "  Approved: 5 (2.5%)\n",
      "\n",
      "Approval Status Distribution:\n",
      "  Approved: 150 (75.0%)\n",
      "  Rejected: 34 (17.0%)\n",
      "  Pending Approval: 8 (4.0%)\n",
      "  Draft: 8 (4.0%)\n",
      "\n",
      "Order Value Statistics:\n",
      "  Average order value: $24663.36\n",
      "  Minimum order value: $1044.11\n",
      "  Maximum order value: $49739.18\n",
      "  Total order value: $4932671.10\n",
      "\n",
      "Shipping Method Distribution:\n",
      "  Sea: 38 (19.0%)\n",
      "  Truck: 32 (16.0%)\n",
      "  Courier: 29 (14.5%)\n",
      "  Express: 28 (14.0%)\n",
      "  Air: 27 (13.5%)\n",
      "  Supplier Delivery: 27 (13.5%)\n",
      "  Rail: 19 (9.5%)\n",
      "\n",
      "Payment Terms Distribution:\n",
      "  Net 30: 90 (45.0%)\n",
      "  Net 45: 35 (17.5%)\n",
      "  Net 60: 29 (14.5%)\n",
      "  2/10 Net 30: 21 (10.5%)\n",
      "  COD: 18 (9.0%)\n",
      "  Net 15: 7 (3.5%)\n",
      "\n",
      "Lead Time Statistics:\n",
      "  Average lead time: 61.2 days\n",
      "  Minimum lead time: 10 days\n",
      "  Maximum lead time: 168 days\n",
      "\n",
      "Monthly Order Distribution (last 6 months):\n",
      "  2025-03: 15 orders\n",
      "  2025-04: 19 orders\n",
      "  2025-05: 13 orders\n",
      "  2025-06: 16 orders\n",
      "  2025-07: 11 orders\n",
      "  2025-08: 10 orders\n",
      "\n",
      "Top 10 Suppliers by Order Count:\n",
      "  SUP-8D1078FD: 9 orders (4.5%)\n",
      "  SUP-4098B3F1: 9 orders (4.5%)\n",
      "  SUP-6FD510F0: 8 orders (4.0%)\n",
      "  SUP-15E82874: 7 orders (3.5%)\n",
      "  SUP-7EF565B1: 7 orders (3.5%)\n",
      "  SUP-7A0426D3: 7 orders (3.5%)\n",
      "  SUP-A8B566C7: 6 orders (3.0%)\n",
      "  SUP-42156347: 6 orders (3.0%)\n",
      "  SUP-CE246628: 6 orders (3.0%)\n",
      "  SUP-60592F62: 6 orders (3.0%)\n",
      "\n",
      "Purchase Order Lines Statistics:\n",
      "Total purchase order lines: 1005\n",
      "Purchase orders with lines: 200\n",
      "\n",
      "Lines Per Order Statistics:\n",
      "  Average lines per order: 5.0\n",
      "  Minimum lines per order: 1\n",
      "  Maximum lines per order: 10\n",
      "\n",
      "Line Status Distribution:\n",
      "  Received: 444 (44.2%)\n",
      "  Cancelled: 176 (17.5%)\n",
      "  Closed: 165 (16.4%)\n",
      "  In Process: 97 (9.7%)\n",
      "  Draft: 55 (5.5%)\n",
      "  Pending Approval: 44 (4.4%)\n",
      "  Approved: 24 (2.4%)\n",
      "\n",
      "Line Value Statistics:\n",
      "  Average line value: $154620.84\n",
      "  Total line value: $155393941.82\n",
      "\n",
      "Quantity Statistics:\n",
      "  Average order quantity: 512.3\n",
      "  Average unit price: $299.87\n",
      "\n",
      "Receipt Statistics:\n",
      "  Received lines: 609 (60.6%)\n",
      "  Total received quantity: 304820\n",
      "  On-time deliveries: 590 (96.9% of received lines)\n",
      "\n",
      "Top 10 Materials by Order Frequency:\n",
      "  MAT-AE0F160E: 13 order lines (1.3%)\n",
      "  MAT-9B6CA482: 12 order lines (1.2%)\n",
      "  MAT-2D6834DA: 12 order lines (1.2%)\n",
      "  MAT-79E88050: 11 order lines (1.1%)\n",
      "  MAT-6D45A015: 11 order lines (1.1%)\n",
      "  MAT-AEAEF3FE: 11 order lines (1.1%)\n",
      "  MAT-E9D582B3: 11 order lines (1.1%)\n",
      "  MAT-91344FF5: 11 order lines (1.1%)\n",
      "  MAT-BC4C1BBA: 11 order lines (1.1%)\n",
      "  MAT-8BBB035F: 11 order lines (1.1%)\n",
      "\n",
      "Sample purchase orders data (first 5 records):\n",
      "         po_id   supplier_id order_date expected_delivery_date  \\\n",
      "0  PO-2F154CC6  SUP-6330D222 2025-01-08             2025-03-08   \n",
      "1  PO-D54B2B4F  SUP-66416915 2025-06-30             2025-08-01   \n",
      "2  PO-05A76A15  SUP-9EF5B17E 2025-04-20             2025-08-14   \n",
      "3  PO-2C6A4517  SUP-4098B3F1 2025-04-03             2025-06-24   \n",
      "4  PO-8A9591A3  SUP-60592F62 2025-07-23             2025-12-04   \n",
      "\n",
      "             status  total_value payment_terms shipping_method       buyer_id  \\\n",
      "0         Cancelled     40335.60        Net 30             Sea  PERS-C110F2BF   \n",
      "1          Approved     29360.97        Net 30             Sea  PERS-BE932AA1   \n",
      "2        In Process     34381.58   2/10 Net 30             Air  PERS-0447DBFB   \n",
      "3         Completed     29732.75        Net 60         Express  PERS-893326C0   \n",
      "4  Pending Approval      9030.27        Net 30         Courier  PERS-0447DBFB   \n",
      "\n",
      "    approval_status notes  lead_time_days order_month  \n",
      "0          Rejected                    59     2025-01  \n",
      "1          Approved                    32     2025-06  \n",
      "2          Approved                   116     2025-04  \n",
      "3          Approved                    82     2025-04  \n",
      "4  Pending Approval                   134     2025-07  \n",
      "\n",
      "Sample purchase order lines data (first 5 records):\n",
      "           line_id        po_id  line_number   material_id  quantity  \\\n",
      "0  POLINE-2114A636  PO-2F154CC6            1  MAT-E79C885F        57   \n",
      "1  POLINE-DE0165A6  PO-2F154CC6            2  MAT-DF3FB384       767   \n",
      "2  POLINE-12F304A9  PO-2F154CC6            3  MAT-A3D59F52       872   \n",
      "3  POLINE-2BF5A532  PO-2F154CC6            4  MAT-BC4C1BBA       968   \n",
      "4  POLINE-1B23EDB7  PO-2F154CC6            5  MAT-82DB5CD5       873   \n",
      "\n",
      "   unit_price  line_value expected_delivery_date  received_quantity  \\\n",
      "0      252.36    14384.66             2025-03-03                  0   \n",
      "1      265.49   203632.52             2025-03-06                  0   \n",
      "2      171.09   149186.77             2025-03-04                  0   \n",
      "3       19.68    19054.65             2025-03-10                  0   \n",
      "4        6.94     6058.47             2025-03-03                  0   \n",
      "\n",
      "  receipt_date     status lot_id  \n",
      "0               Cancelled         \n",
      "1               Cancelled         \n",
      "2               Cancelled         \n",
      "3               Cancelled         \n",
      "4               Cancelled         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2065/3304263831.py:556: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  received_lines['receipt_date'] = pd.to_datetime(received_lines['receipt_date'])\n",
      "/tmp/ipykernel_2065/3304263831.py:557: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  received_lines['expected_delivery_date'] = pd.to_datetime(received_lines['expected_delivery_date'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_suppliers_data(suppliers_file=\"data/suppliers.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated suppliers data\n",
    "    \n",
    "    Parameters:\n",
    "    - suppliers_file: CSV file containing suppliers data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the suppliers data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(suppliers_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Suppliers data file {suppliers_file} not found.\")\n",
    "        print(\"Please run the suppliers data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_materials_data(materials_file=\"data/materials.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated materials data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - materials_file: CSV file containing materials data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the materials data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(materials_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Materials data file {materials_file} not found.\")\n",
    "        print(\"Purchase orders will be generated with synthetic material IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Purchase orders will be generated with synthetic buyer IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_purchase_orders(suppliers_df, materials_df=None, personnel_df=None, \n",
    "                             num_orders=200, start_time=None, end_time=None,\n",
    "                             output_file=\"data/purchase_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the PurchaseOrders table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - suppliers_df: DataFrame containing suppliers data\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_orders: Number of purchase order records to generate\n",
    "    - start_time: Start time for order dates (defaults to 365 days ago)\n",
    "    - end_time: End time for order dates (defaults to 30 days in the future)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated purchase orders data\n",
    "    \"\"\"\n",
    "    if suppliers_df is None or len(suppliers_df) == 0:\n",
    "        print(\"Error: No suppliers data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=365)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now() + timedelta(days=30)\n",
    "    \n",
    "    # Generate buyer IDs if personnel_df is not provided\n",
    "    if personnel_df is None or len(personnel_df) == 0:\n",
    "        print(\"Generating synthetic buyer IDs...\")\n",
    "        buyer_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(10)]\n",
    "    else:\n",
    "        # Use actual personnel IDs from personnel_df\n",
    "        # Typically use personnel with purchasing roles\n",
    "        buyer_ids = personnel_df['personnel_id'].sample(min(10, len(personnel_df))).tolist()\n",
    "    \n",
    "    # Define payment terms (use supplier terms or defaults)\n",
    "    payment_terms = [\"Net 30\", \"Net 45\", \"Net 60\", \"2/10 Net 30\", \"COD\", \"Prepaid\"]\n",
    "    \n",
    "    # Define shipping methods\n",
    "    shipping_methods = [\"Truck\", \"Air\", \"Sea\", \"Rail\", \"Express\", \"Courier\", \"Supplier Delivery\"]\n",
    "    \n",
    "    # Define approval statuses\n",
    "    approval_statuses = [\"Draft\", \"Pending Approval\", \"Approved\", \"Rejected\", \"On Hold\"]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"po_id\": [f\"PO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_orders)],\n",
    "        \"supplier_id\": [],\n",
    "        \"order_date\": [],\n",
    "        \"expected_delivery_date\": [],\n",
    "        \"status\": [],\n",
    "        \"total_value\": [],\n",
    "        \"payment_terms\": [],\n",
    "        \"shipping_method\": [],\n",
    "        \"buyer_id\": [],\n",
    "        \"approval_status\": [],\n",
    "        \"notes\": []\n",
    "    }\n",
    "    \n",
    "    # Generate data for each purchase order\n",
    "    for i in range(num_orders):\n",
    "        # Select supplier (more active suppliers get more orders)\n",
    "        active_suppliers = suppliers_df[suppliers_df['status'] == 'Active']\n",
    "        \n",
    "        if len(active_suppliers) > 0:\n",
    "            # Prefer active suppliers (80% chance)\n",
    "            if random.random() < 0.8:\n",
    "                supplier = active_suppliers.sample(1).iloc[0]\n",
    "            else:\n",
    "                supplier = suppliers_df.sample(1).iloc[0]\n",
    "        else:\n",
    "            supplier = suppliers_df.sample(1).iloc[0]\n",
    "            \n",
    "        supplier_id = supplier['supplier_id']\n",
    "        data[\"supplier_id\"].append(supplier_id)\n",
    "        \n",
    "        # Generate order date\n",
    "        time_range_days = (end_time - start_time).days\n",
    "        days_from_start = random.randint(0, time_range_days)\n",
    "        order_date = start_time + timedelta(days=days_from_start)\n",
    "        data[\"order_date\"].append(order_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Generate expected delivery date based on supplier lead time\n",
    "        if 'lead_time_days' in supplier and pd.notna(supplier['lead_time_days']):\n",
    "            lead_time = supplier['lead_time_days']\n",
    "        else:\n",
    "            # Default lead time if not available\n",
    "            lead_time = random.randint(14, 60)\n",
    "            \n",
    "        # Add some variation to the lead time\n",
    "        lead_time_variation = random.uniform(0.8, 1.2)  # +/- 20%\n",
    "        adjusted_lead_time = int(lead_time * lead_time_variation)\n",
    "        \n",
    "        expected_delivery_date = order_date + timedelta(days=adjusted_lead_time)\n",
    "        data[\"expected_delivery_date\"].append(expected_delivery_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Determine PO status based on dates\n",
    "        current_date = datetime.now()\n",
    "        \n",
    "        if order_date > current_date:\n",
    "            # Future POs are typically in Draft or Pending status\n",
    "            status = random.choice([\"Draft\", \"Pending Approval\"])\n",
    "        elif expected_delivery_date > current_date:\n",
    "            # Current POs are Approved or In Process\n",
    "            status = random.choice([\"Approved\", \"In Process\", \"Partially Received\"])\n",
    "        else:\n",
    "            # Past POs are Completed, Cancelled, or Closed\n",
    "            status_options = [\"Completed\", \"Completed\", \"Completed\", \"Cancelled\", \"Closed\"]  # Weighted for more completed\n",
    "            status = random.choice(status_options)\n",
    "            \n",
    "        data[\"status\"].append(status)\n",
    "        \n",
    "        # Generate order value (based on a reasonable range for purchase orders)\n",
    "        order_value = random.uniform(1000, 50000)\n",
    "        data[\"total_value\"].append(round(order_value, 2))\n",
    "        \n",
    "        # Set payment terms (use supplier terms if available)\n",
    "        if 'payment_terms' in supplier and pd.notna(supplier['payment_terms']):\n",
    "            data[\"payment_terms\"].append(supplier['payment_terms'])\n",
    "        else:\n",
    "            data[\"payment_terms\"].append(random.choice(payment_terms))\n",
    "        \n",
    "        # Set shipping method\n",
    "        data[\"shipping_method\"].append(random.choice(shipping_methods))\n",
    "        \n",
    "        # Assign buyer\n",
    "        data[\"buyer_id\"].append(random.choice(buyer_ids))\n",
    "        \n",
    "        # Set approval status based on PO status\n",
    "        if status == \"Draft\":\n",
    "            approval_status = \"Draft\"\n",
    "        elif status == \"Pending Approval\":\n",
    "            approval_status = \"Pending Approval\"\n",
    "        elif status in [\"Cancelled\", \"Rejected\"]:\n",
    "            approval_status = \"Rejected\"\n",
    "        elif status == \"On Hold\":\n",
    "            approval_status = \"On Hold\"\n",
    "        else:\n",
    "            approval_status = \"Approved\"\n",
    "            \n",
    "        data[\"approval_status\"].append(approval_status)\n",
    "        \n",
    "        # Generate notes (mostly empty)\n",
    "        if random.random() < 0.2:  # 20% chance of having notes\n",
    "            notes_options = [\n",
    "                \"Rush order, critical materials\",\n",
    "                \"Partial shipments acceptable\",\n",
    "                \"Quality certificates required\",\n",
    "                \"Special packaging instructions included\",\n",
    "                \"Price negotiated below standard\",\n",
    "                \"Consolidated order for multiple projects\",\n",
    "                f\"Reference requisition #{random.randint(10000, 99999)}\",\n",
    "                \"Schedule delivery with warehouse manager\",\n",
    "                \"New supplier, additional quality checks\",\n",
    "                \"Replacement for PO cancelled last month\"\n",
    "            ]\n",
    "            data[\"notes\"].append(random.choice(notes_options))\n",
    "        else:\n",
    "            data[\"notes\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} purchase order records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_purchase_order_lines(purchase_orders_df, materials_df=None, \n",
    "                                output_file=\"data/purchase_order_lines.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the PurchaseOrderLines table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - purchase_orders_df: DataFrame containing purchase orders data\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated purchase order lines data\n",
    "    \"\"\"\n",
    "    if purchase_orders_df is None or len(purchase_orders_df) == 0:\n",
    "        print(\"Error: No purchase orders data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Generate material IDs if materials_df is not provided\n",
    "    if materials_df is None or len(materials_df) == 0:\n",
    "        print(\"Generating synthetic material IDs...\")\n",
    "        material_ids = [f\"MAT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "        \n",
    "        # Create synthetic material prices\n",
    "        material_prices = {}\n",
    "        for mat_id in material_ids:\n",
    "            material_prices[mat_id] = random.uniform(10, 1000)\n",
    "    else:\n",
    "        # Use actual material IDs from materials_df\n",
    "        material_ids = materials_df['material_id'].tolist()\n",
    "        \n",
    "        # Create price mapping from standard cost\n",
    "        material_prices = {}\n",
    "        for _, material in materials_df.iterrows():\n",
    "            if 'standard_cost' in material and pd.notna(material['standard_cost']):\n",
    "                material_prices[material['material_id']] = material['standard_cost']\n",
    "            else:\n",
    "                material_prices[material['material_id']] = random.uniform(10, 1000)\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"line_id\": [],\n",
    "        \"po_id\": [],\n",
    "        \"line_number\": [],\n",
    "        \"material_id\": [],\n",
    "        \"quantity\": [],\n",
    "        \"unit_price\": [],\n",
    "        \"line_value\": [],\n",
    "        \"expected_delivery_date\": [],\n",
    "        \"received_quantity\": [],\n",
    "        \"receipt_date\": [],\n",
    "        \"status\": [],\n",
    "        \"lot_id\": []\n",
    "    }\n",
    "    \n",
    "    # Process each purchase order\n",
    "    for _, po in purchase_orders_df.iterrows():\n",
    "        po_id = po['po_id']\n",
    "        po_status = po['status']\n",
    "        \n",
    "        # Determine number of line items for this PO\n",
    "        num_lines = random.randint(1, 10)  # 1-10 line items per PO\n",
    "        \n",
    "        # Keep track of selected materials for this PO to avoid duplicates\n",
    "        selected_materials = []\n",
    "        \n",
    "        # Get PO dates\n",
    "        order_date = pd.to_datetime(po['order_date'])\n",
    "        expected_delivery_date = pd.to_datetime(po['expected_delivery_date'])\n",
    "        \n",
    "        # Generate line items\n",
    "        for line_num in range(1, num_lines + 1):\n",
    "            # Create unique line ID\n",
    "            line_id = f\"POLINE-{uuid.uuid4().hex[:8].upper()}\"\n",
    "            data[\"line_id\"].append(line_id)\n",
    "            data[\"po_id\"].append(po_id)\n",
    "            data[\"line_number\"].append(line_num)\n",
    "            \n",
    "            # Select material (avoid duplicates within same PO)\n",
    "            available_materials = [m for m in material_ids if m not in selected_materials]\n",
    "            \n",
    "            if not available_materials:\n",
    "                # If we've used all materials, just pick a random one\n",
    "                material_id = random.choice(material_ids)\n",
    "            else:\n",
    "                material_id = random.choice(available_materials)\n",
    "                selected_materials.append(material_id)\n",
    "                \n",
    "            data[\"material_id\"].append(material_id)\n",
    "            \n",
    "            # Generate quantity based on material (would depend on unit of measure)\n",
    "            # For simplicity we'll use generic quantities\n",
    "            quantity = random.randint(1, 1000)\n",
    "            data[\"quantity\"].append(quantity)\n",
    "            \n",
    "            # Get unit price\n",
    "            if material_id in material_prices:\n",
    "                unit_price = material_prices[material_id]\n",
    "            else:\n",
    "                unit_price = random.uniform(10, 1000)\n",
    "                \n",
    "            # Apply random variation (supplier-specific pricing)\n",
    "            price_variation = random.uniform(0.9, 1.1)  # +/- 10%\n",
    "            unit_price = unit_price * price_variation\n",
    "            \n",
    "            data[\"unit_price\"].append(round(unit_price, 2))\n",
    "            \n",
    "            # Calculate line value\n",
    "            line_value = quantity * unit_price\n",
    "            data[\"line_value\"].append(round(line_value, 2))\n",
    "            \n",
    "            # Set expected delivery date (can vary slightly from PO date)\n",
    "            line_delivery_variation = random.randint(-5, 5)  # +/- 5 days\n",
    "            line_delivery_date = expected_delivery_date + timedelta(days=line_delivery_variation)\n",
    "            \n",
    "            # Ensure date makes sense\n",
    "            if line_delivery_date < order_date:\n",
    "                line_delivery_date = order_date + timedelta(days=1)\n",
    "                \n",
    "            data[\"expected_delivery_date\"].append(line_delivery_date.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            # Set line status and receipt info based on PO status\n",
    "            if po_status in [\"Draft\", \"Pending Approval\"]:\n",
    "                line_status = po_status\n",
    "                data[\"received_quantity\"].append(0)\n",
    "                data[\"receipt_date\"].append(\"\")\n",
    "                data[\"lot_id\"].append(\"\")\n",
    "                \n",
    "            elif po_status == \"Approved\":\n",
    "                line_status = \"Approved\"\n",
    "                data[\"received_quantity\"].append(0)\n",
    "                data[\"receipt_date\"].append(\"\")\n",
    "                data[\"lot_id\"].append(\"\")\n",
    "                \n",
    "            elif po_status == \"In Process\":\n",
    "                line_status = \"In Process\"\n",
    "                data[\"received_quantity\"].append(0)\n",
    "                data[\"receipt_date\"].append(\"\")\n",
    "                data[\"lot_id\"].append(\"\")\n",
    "                \n",
    "            elif po_status == \"Partially Received\":\n",
    "                # Mix of received and unreceived lines\n",
    "                if random.random() < 0.6:  # 60% chance this line is received\n",
    "                    line_status = \"Received\"\n",
    "                    received_qty = quantity\n",
    "                    \n",
    "                    # Receipt date is between order date and current date\n",
    "                    days_difference = (datetime.now() - order_date).days\n",
    "                    if days_difference >= 1:\n",
    "                        receipt_days = random.randint(1, days_difference)\n",
    "                        receipt_date = order_date + timedelta(days=receipt_days)\n",
    "                        data[\"receipt_date\"].append(receipt_date.strftime(\"%Y-%m-%d\"))\n",
    "                    else:\n",
    "                        data[\"receipt_date\"].append(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "                    \n",
    "                    # Generate a lot ID for the received material\n",
    "                    data[\"lot_id\"].append(f\"LOT-{uuid.uuid4().hex[:8].upper()}\")\n",
    "                else:\n",
    "                    line_status = \"In Process\"\n",
    "                    received_qty = 0\n",
    "                    data[\"receipt_date\"].append(\"\")\n",
    "                    data[\"lot_id\"].append(\"\")\n",
    "                    \n",
    "                data[\"received_quantity\"].append(received_qty)\n",
    "                \n",
    "            elif po_status == \"Completed\":\n",
    "                line_status = \"Received\"\n",
    "                data[\"received_quantity\"].append(quantity)\n",
    "                \n",
    "                # Receipt date is between order date and expected delivery date\n",
    "                receipt_days = random.randint(1, max(1, (expected_delivery_date - order_date).days))\n",
    "                receipt_date = order_date + timedelta(days=receipt_days)\n",
    "                data[\"receipt_date\"].append(receipt_date.strftime(\"%Y-%m-%d\"))\n",
    "                \n",
    "                # Generate a lot ID for the received material\n",
    "                data[\"lot_id\"].append(f\"LOT-{uuid.uuid4().hex[:8].upper()}\")\n",
    "                \n",
    "            elif po_status == \"Cancelled\":\n",
    "                line_status = \"Cancelled\"\n",
    "                data[\"received_quantity\"].append(0)\n",
    "                data[\"receipt_date\"].append(\"\")\n",
    "                data[\"lot_id\"].append(\"\")\n",
    "                \n",
    "            else:  # Closed or other status\n",
    "                line_status = po_status\n",
    "                data[\"received_quantity\"].append(quantity)\n",
    "                \n",
    "                # Receipt date is between order date and expected delivery date\n",
    "                receipt_days = random.randint(1, max(1, (expected_delivery_date - order_date).days))\n",
    "                receipt_date = order_date + timedelta(days=receipt_days)\n",
    "                data[\"receipt_date\"].append(receipt_date.strftime(\"%Y-%m-%d\"))\n",
    "                \n",
    "                # Generate a lot ID for the received material\n",
    "                data[\"lot_id\"].append(f\"LOT-{uuid.uuid4().hex[:8].upper()}\")\n",
    "            \n",
    "            data[\"status\"].append(line_status)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} purchase order line records for {df['po_id'].nunique()} purchase orders.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(purchase_orders_df, purchase_order_lines_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated purchase orders and lines data\n",
    "    \n",
    "    Parameters:\n",
    "    - purchase_orders_df: DataFrame containing purchase orders data\n",
    "    - purchase_order_lines_df: DataFrame containing purchase order lines data (optional)\n",
    "    \"\"\"\n",
    "    if purchase_orders_df is None or len(purchase_orders_df) == 0:\n",
    "        print(\"No purchase orders data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nPurchase Orders Statistics:\")\n",
    "    print(f\"Total purchase orders: {len(purchase_orders_df)}\")\n",
    "    \n",
    "    # Order status distribution\n",
    "    print(\"\\nPurchase Order Status Distribution:\")\n",
    "    status_counts = purchase_orders_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(purchase_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Approval status distribution\n",
    "    print(\"\\nApproval Status Distribution:\")\n",
    "    approval_counts = purchase_orders_df['approval_status'].value_counts()\n",
    "    for status, count in approval_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(purchase_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Order value statistics\n",
    "    print(\"\\nOrder Value Statistics:\")\n",
    "    print(f\"  Average order value: ${purchase_orders_df['total_value'].mean():.2f}\")\n",
    "    print(f\"  Minimum order value: ${purchase_orders_df['total_value'].min():.2f}\")\n",
    "    print(f\"  Maximum order value: ${purchase_orders_df['total_value'].max():.2f}\")\n",
    "    print(f\"  Total order value: ${purchase_orders_df['total_value'].sum():.2f}\")\n",
    "    \n",
    "    # Shipping method distribution\n",
    "    print(\"\\nShipping Method Distribution:\")\n",
    "    shipping_counts = purchase_orders_df['shipping_method'].value_counts()\n",
    "    for method, count in shipping_counts.items():\n",
    "        print(f\"  {method}: {count} ({count/len(purchase_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Payment terms distribution\n",
    "    print(\"\\nPayment Terms Distribution:\")\n",
    "    terms_counts = purchase_orders_df['payment_terms'].value_counts()\n",
    "    for terms, count in terms_counts.items():\n",
    "        print(f\"  {terms}: {count} ({count/len(purchase_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Date-based analysis\n",
    "    purchase_orders_df['order_date'] = pd.to_datetime(purchase_orders_df['order_date'])\n",
    "    purchase_orders_df['expected_delivery_date'] = pd.to_datetime(purchase_orders_df['expected_delivery_date'])\n",
    "    \n",
    "    # Lead time analysis\n",
    "    purchase_orders_df['lead_time_days'] = (purchase_orders_df['expected_delivery_date'] - \n",
    "                                         purchase_orders_df['order_date']).dt.days\n",
    "    \n",
    "    print(\"\\nLead Time Statistics:\")\n",
    "    print(f\"  Average lead time: {purchase_orders_df['lead_time_days'].mean():.1f} days\")\n",
    "    print(f\"  Minimum lead time: {purchase_orders_df['lead_time_days'].min()} days\")\n",
    "    print(f\"  Maximum lead time: {purchase_orders_df['lead_time_days'].max()} days\")\n",
    "    \n",
    "    # Orders by month\n",
    "    purchase_orders_df['order_month'] = purchase_orders_df['order_date'].dt.to_period('M')\n",
    "    monthly_orders = purchase_orders_df.groupby('order_month').size().sort_index()\n",
    "    \n",
    "    print(\"\\nMonthly Order Distribution (last 6 months):\")\n",
    "    for month, count in monthly_orders.tail(6).items():\n",
    "        print(f\"  {month}: {count} orders\")\n",
    "    \n",
    "    # Supplier distribution\n",
    "    supplier_counts = purchase_orders_df.groupby('supplier_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Suppliers by Order Count:\")\n",
    "    for supplier, count in supplier_counts.head(10).items():\n",
    "        print(f\"  {supplier}: {count} orders ({count/len(purchase_orders_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Purchase order lines statistics (if available)\n",
    "    if purchase_order_lines_df is not None and len(purchase_order_lines_df) > 0:\n",
    "        print(\"\\nPurchase Order Lines Statistics:\")\n",
    "        print(f\"Total purchase order lines: {len(purchase_order_lines_df)}\")\n",
    "        print(f\"Purchase orders with lines: {purchase_order_lines_df['po_id'].nunique()}\")\n",
    "        \n",
    "        # Lines per order\n",
    "        lines_per_order = purchase_order_lines_df.groupby('po_id').size()\n",
    "        print(\"\\nLines Per Order Statistics:\")\n",
    "        print(f\"  Average lines per order: {lines_per_order.mean():.1f}\")\n",
    "        print(f\"  Minimum lines per order: {lines_per_order.min()}\")\n",
    "        print(f\"  Maximum lines per order: {lines_per_order.max()}\")\n",
    "        \n",
    "        # Line status distribution\n",
    "        print(\"\\nLine Status Distribution:\")\n",
    "        line_status_counts = purchase_order_lines_df['status'].value_counts()\n",
    "        for status, count in line_status_counts.items():\n",
    "            print(f\"  {status}: {count} ({count/len(purchase_order_lines_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Line value statistics\n",
    "        print(\"\\nLine Value Statistics:\")\n",
    "        print(f\"  Average line value: ${purchase_order_lines_df['line_value'].mean():.2f}\")\n",
    "        print(f\"  Total line value: ${purchase_order_lines_df['line_value'].sum():.2f}\")\n",
    "        \n",
    "        # Quantity statistics\n",
    "        print(\"\\nQuantity Statistics:\")\n",
    "        print(f\"  Average order quantity: {purchase_order_lines_df['quantity'].mean():.1f}\")\n",
    "        print(f\"  Average unit price: ${purchase_order_lines_df['unit_price'].mean():.2f}\")\n",
    "        \n",
    "        # Receipt statistics\n",
    "        received_lines = purchase_order_lines_df[purchase_order_lines_df['received_quantity'] > 0]\n",
    "        if len(received_lines) > 0:\n",
    "            print(\"\\nReceipt Statistics:\")\n",
    "            print(f\"  Received lines: {len(received_lines)} ({len(received_lines)/len(purchase_order_lines_df)*100:.1f}%)\")\n",
    "            print(f\"  Total received quantity: {received_lines['received_quantity'].sum()}\")\n",
    "            \n",
    "            # Calculate on-time delivery\n",
    "            received_lines['receipt_date'] = pd.to_datetime(received_lines['receipt_date'])\n",
    "            received_lines['expected_delivery_date'] = pd.to_datetime(received_lines['expected_delivery_date'])\n",
    "            \n",
    "            # On-time deliveries (received on or before expected date)\n",
    "            on_time = received_lines[received_lines['receipt_date'] <= received_lines['expected_delivery_date']]\n",
    "            on_time_pct = len(on_time) / len(received_lines) * 100\n",
    "            \n",
    "            print(f\"  On-time deliveries: {len(on_time)} ({on_time_pct:.1f}% of received lines)\")\n",
    "            \n",
    "            # Material distribution\n",
    "            material_counts = purchase_order_lines_df.groupby('material_id').size().sort_values(ascending=False)\n",
    "            \n",
    "            print(\"\\nTop 10 Materials by Order Frequency:\")\n",
    "            for material, count in material_counts.head(10).items():\n",
    "                print(f\"  {material}: {count} order lines ({count/len(purchase_order_lines_df)*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    suppliers_df = load_suppliers_data()\n",
    "    materials_df = load_materials_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    \n",
    "    if suppliers_df is not None:\n",
    "        # Generate purchase orders data\n",
    "        purchase_orders_df = generate_purchase_orders(\n",
    "            suppliers_df,\n",
    "            materials_df,\n",
    "            personnel_df,\n",
    "            num_orders=200,  # Generate 200 purchase order records\n",
    "            output_file=\"data/purchase_orders.csv\"\n",
    "        )\n",
    "        \n",
    "        if purchase_orders_df is not None:\n",
    "            # Generate purchase order lines data\n",
    "            purchase_order_lines_df = generate_purchase_order_lines(\n",
    "                purchase_orders_df,\n",
    "                materials_df,\n",
    "                output_file=\"data/purchase_order_lines.csv\"\n",
    "            )\n",
    "        else:\n",
    "            purchase_order_lines_df = None\n",
    "    else:\n",
    "        purchase_orders_df = None\n",
    "        purchase_order_lines_df = None\n",
    "    \n",
    "    # Display statistics\n",
    "    if purchase_orders_df is not None:\n",
    "        display_statistics(purchase_orders_df, purchase_order_lines_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample purchase orders data (first 5 records):\")\n",
    "        print(purchase_orders_df.head(5))\n",
    "        \n",
    "        if purchase_order_lines_df is not None:\n",
    "            print(\"\\nSample purchase order lines data (first 5 records):\")\n",
    "            print(purchase_order_lines_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93d653",
   "metadata": {},
   "source": [
    "Production Schedules & Scheduled Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37edeffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Production schedules will be generated with synthetic creator IDs.\n",
      "Generating synthetic creator IDs...\n",
      "Successfully generated 20 production schedule records.\n",
      "Data saved to data/production_schedules.csv\n",
      "Successfully generated 264 scheduled production records for 20 schedules.\n",
      "Data saved to data/scheduled_production.csv\n",
      "\n",
      "Production Schedules Statistics:\n",
      "Total production schedules: 20\n",
      "\n",
      "Schedule Type Distribution:\n",
      "  Master: 5 (25.0%)\n",
      "  Quarterly: 4 (20.0%)\n",
      "  Weekly: 3 (15.0%)\n",
      "  Campaign: 3 (15.0%)\n",
      "  Production: 3 (15.0%)\n",
      "  Detailed: 1 (5.0%)\n",
      "  Monthly: 1 (5.0%)\n",
      "\n",
      "Schedule Status Distribution:\n",
      "  Approved: 7 (35.0%)\n",
      "  Draft: 5 (25.0%)\n",
      "  In Progress: 5 (25.0%)\n",
      "  Superseded: 1 (5.0%)\n",
      "  Completed: 1 (5.0%)\n",
      "  Cancelled: 1 (5.0%)\n",
      "\n",
      "Schedule Duration Statistics:\n",
      "  Average duration: 77.7 days\n",
      "  Minimum duration: 7 days\n",
      "  Maximum duration: 287 days\n",
      "\n",
      "Freeze Horizon Statistics:\n",
      "  Average freeze horizon: 27.1 days\n",
      "  Minimum freeze horizon: 4 days\n",
      "  Maximum freeze horizon: 57 days\n",
      "\n",
      "Schedules by Facility:\n",
      "  FAC-2B0BE767: 2 schedules\n",
      "  FAC-2CBAAD9D: 6 schedules\n",
      "  FAC-80A0687B: 3 schedules\n",
      "  FAC-BEDC9B8B: 4 schedules\n",
      "  FAC-E91E49C1: 5 schedules\n",
      "\n",
      "Monthly Schedule Distribution:\n",
      "  2025-03: 3 schedules\n",
      "  2025-06: 3 schedules\n",
      "  2025-07: 2 schedules\n",
      "  2025-08: 2 schedules\n",
      "  2025-09: 1 schedules\n",
      "  2025-10: 2 schedules\n",
      "  2025-11: 6 schedules\n",
      "  2025-12: 1 schedules\n",
      "\n",
      "Scheduled Production Statistics:\n",
      "Total scheduled production items: 264\n",
      "Items per schedule: 13.2\n",
      "\n",
      "Item Status Distribution:\n",
      "  Scheduled: 127 (48.1%)\n",
      "  Planned: 70 (26.5%)\n",
      "  Completed: 44 (16.7%)\n",
      "  Cancelled: 17 (6.4%)\n",
      "  Canceled: 3 (1.1%)\n",
      "  Released: 2 (0.8%)\n",
      "  Held: 1 (0.4%)\n",
      "\n",
      "Priority Distribution:\n",
      "  Priority 1: 54 (20.5%)\n",
      "  Priority 2: 43 (16.3%)\n",
      "  Priority 3: 48 (18.2%)\n",
      "  Priority 4: 56 (21.2%)\n",
      "  Priority 5: 63 (23.9%)\n",
      "\n",
      "Items with work order association: 101 (38.3%)\n",
      "Items with customer order association: 185 (70.1%)\n",
      "\n",
      "Quantity Statistics:\n",
      "  Average scheduled quantity: 5088.9\n",
      "  Total scheduled quantity: 1343482\n",
      "\n",
      "Item Duration Statistics:\n",
      "  Average duration: 4.7 days\n",
      "  Minimum duration: 1 days\n",
      "  Maximum duration: 14 days\n",
      "\n",
      "Top 10 Products in Production Schedules:\n",
      "  PROD-74CC9A91: 8 scheduled items (3.0%)\n",
      "  PROD-FB56C194: 7 scheduled items (2.7%)\n",
      "  PROD-25EAA2CF: 7 scheduled items (2.7%)\n",
      "  PROD-5F70A643: 7 scheduled items (2.7%)\n",
      "  PROD-E0ED1A01: 6 scheduled items (2.3%)\n",
      "  PROD-AAF9CC5C: 5 scheduled items (1.9%)\n",
      "  PROD-A0B59AF7: 5 scheduled items (1.9%)\n",
      "  PROD-A04A7F31: 5 scheduled items (1.9%)\n",
      "  PROD-9B402796: 5 scheduled items (1.9%)\n",
      "  PROD-EAE10C58: 5 scheduled items (1.9%)\n",
      "\n",
      "Sample production schedules data (first 5 records):\n",
      "   schedule_id                         schedule_name schedule_type  \\\n",
      "0  PS-D4D5B539       Weekly Schedule - Week 31, 2024        Weekly   \n",
      "1  PS-7A0E370B         Mid-term Master Schedule 2025        Master   \n",
      "2  PS-34F60343  Catalyst Pro 689 Production Campaign      Campaign   \n",
      "3  PS-A4B0544E               Q1 2024 Production Plan     Quarterly   \n",
      "4  PS-BE17DE8D       Weekly Schedule - Week 26, 2026        Weekly   \n",
      "\n",
      "    facility_id start_date   end_date creation_date     created_by  \\\n",
      "0  FAC-80A0687B 2025-03-04 2025-03-11    2025-01-19  PERS-3F1E1F2A   \n",
      "1  FAC-BEDC9B8B 2025-08-01 2026-01-12    2025-07-25  PERS-56DE268A   \n",
      "2  FAC-E91E49C1 2025-11-10 2025-11-28    2025-10-06  PERS-3F1E1F2A   \n",
      "3  FAC-E91E49C1 2025-09-06 2025-12-05    2025-07-28  PERS-0DF08593   \n",
      "4  FAC-2CBAAD9D 2025-11-28 2025-12-05    2025-10-30  PERS-73D14741   \n",
      "\n",
      "       status  revision  freeze_horizon_days  \\\n",
      "0  Superseded         9                    9   \n",
      "1       Draft         2                   47   \n",
      "2    Approved         2                   26   \n",
      "3       Draft         2                   57   \n",
      "4       Draft         3                   11   \n",
      "\n",
      "                                   notes  duration_days month_year  \n",
      "0  Consolidated for resource utilization              7    2025-03  \n",
      "1                                                   164    2025-08  \n",
      "2                                                    18    2025-11  \n",
      "3         Balanced for labor utilization             90    2025-09  \n",
      "4                                                     7    2025-11  \n",
      "\n",
      "Sample scheduled production data (first 5 records):\n",
      "  scheduled_id  schedule_id     product_id work_order_id  scheduled_quantity  \\\n",
      "0  SP-E4097BE0  PS-D4D5B539  PROD-10126E26                              4151   \n",
      "1  SP-0B704B7F  PS-D4D5B539  PROD-AAF9CC5C                              4037   \n",
      "2  SP-582EF199  PS-D4D5B539  PROD-3153BC90                              3084   \n",
      "3  SP-01154CAA  PS-D4D5B539  PROD-925C6B31                              8080   \n",
      "4  SP-F8BCD289  PS-D4D5B539  PROD-F58D15D9                              5031   \n",
      "\n",
      "  start_date   end_date equipment_id  priority     status     order_id  \\\n",
      "0 2025-03-06 2025-03-10  EQ-DEB311BD         4  Completed  CO-9C82CB5D   \n",
      "1 2025-03-04 2025-03-07  EQ-277C7BA4         2  Completed  CO-D2A1363D   \n",
      "2 2025-03-07 2025-03-11  EQ-6809A01E         5  Completed  CO-AB91861E   \n",
      "3 2025-03-05 2025-03-06  EQ-988D072B         3  Completed  CO-5A376C82   \n",
      "4 2025-03-08 2025-03-11  EQ-DA1AB602         3  Completed                \n",
      "\n",
      "   duration_days  \n",
      "0              4  \n",
      "1              3  \n",
      "2              4  \n",
      "3              1  \n",
      "4              3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_products_data(products_file=\"data/products.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated products data\n",
    "    \n",
    "    Parameters:\n",
    "    - products_file: CSV file containing products data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the products data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(products_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Products data file {products_file} not found.\")\n",
    "        print(\"Please run the products data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_customer_orders_data(customer_orders_file=\"data/customer_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated customer orders data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - customer_orders_file: CSV file containing customer orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the customer orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(customer_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Customer orders data file {customer_orders_file} not found.\")\n",
    "        print(\"Production schedules will be generated without customer order associations.\")\n",
    "        return None\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Production schedules will be generated without equipment associations.\")\n",
    "        return None\n",
    "\n",
    "def load_facilities_data(facilities_file=\"data/facilities.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated facilities data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - facilities_file: CSV file containing facilities data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the facilities data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(facilities_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Facilities data file {facilities_file} not found.\")\n",
    "        print(\"Production schedules will be generated with synthetic facility IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Production schedules will be generated with synthetic creator IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_production_schedules(products_df, customer_orders_df=None, equipment_df=None, \n",
    "                                facilities_df=None, personnel_df=None, num_schedules=20,\n",
    "                                start_time=None, end_time=None, \n",
    "                                output_file=\"data/production_schedules.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the ProductionSchedules table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - products_df: DataFrame containing products data\n",
    "    - customer_orders_df: DataFrame containing customer orders data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    - facilities_df: DataFrame containing facilities data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_schedules: Number of production schedule records to generate\n",
    "    - start_time: Start time for schedule dates (defaults to 180 days ago)\n",
    "    - end_time: End time for schedule dates (defaults to 180 days in the future)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated production schedules data\n",
    "    \"\"\"\n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"Error: No products data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=180)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now() + timedelta(days=180)\n",
    "    \n",
    "    # Generate facility IDs if facilities_df is not provided\n",
    "    if facilities_df is None or len(facilities_df) == 0:\n",
    "        print(\"Generating synthetic facility IDs...\")\n",
    "        facility_ids = [f\"FAC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(5)]\n",
    "    else:\n",
    "        facility_ids = facilities_df['facility_id'].tolist()\n",
    "    \n",
    "    # Generate personnel IDs for creators if personnel_df is not provided\n",
    "    if personnel_df is None or len(personnel_df) == 0:\n",
    "        print(\"Generating synthetic creator IDs...\")\n",
    "        creator_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(10)]\n",
    "    else:\n",
    "        creator_ids = personnel_df['personnel_id'].tolist()\n",
    "    \n",
    "    # Define schedule types\n",
    "    schedule_types = [\"Master\", \"Detailed\", \"Production\", \"Campaign\", \"Weekly\", \"Monthly\", \"Quarterly\"]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"schedule_id\": [f\"PS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_schedules)],\n",
    "        \"schedule_name\": [],\n",
    "        \"schedule_type\": [],\n",
    "        \"facility_id\": [],\n",
    "        \"start_date\": [],\n",
    "        \"end_date\": [],\n",
    "        \"creation_date\": [],\n",
    "        \"created_by\": [],\n",
    "        \"status\": [],\n",
    "        \"revision\": [],\n",
    "        \"freeze_horizon_days\": [],\n",
    "        \"notes\": []\n",
    "    }\n",
    "    \n",
    "    # Define schedule statuses\n",
    "    statuses = [\"Draft\", \"Approved\", \"In Progress\", \"Completed\", \"Cancelled\", \"Superseded\"]\n",
    "    \n",
    "    # Generate data for each production schedule\n",
    "    for i in range(num_schedules):\n",
    "        # Generate schedule name (descriptive)\n",
    "        schedule_type = random.choice(schedule_types)\n",
    "        \n",
    "        # Create a date component for the name\n",
    "        if schedule_type in [\"Weekly\", \"Production\"]:\n",
    "            # Weekly schedules - named by week number\n",
    "            week_num = random.randint(1, 52)\n",
    "            year = random.randint(datetime.now().year - 1, datetime.now().year + 1)\n",
    "            schedule_name = f\"{schedule_type} Schedule - Week {week_num}, {year}\"\n",
    "        elif schedule_type == \"Monthly\":\n",
    "            # Monthly schedules - named by month\n",
    "            month_names = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n",
    "                          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "            month = random.choice(month_names)\n",
    "            year = random.randint(datetime.now().year - 1, datetime.now().year + 1)\n",
    "            schedule_name = f\"{month} {year} Production Schedule\"\n",
    "        elif schedule_type == \"Quarterly\":\n",
    "            # Quarterly schedules\n",
    "            quarter = random.randint(1, 4)\n",
    "            year = random.randint(datetime.now().year - 1, datetime.now().year + 1)\n",
    "            schedule_name = f\"Q{quarter} {year} Production Plan\"\n",
    "        elif schedule_type == \"Campaign\":\n",
    "            # Campaign schedules - named by product or campaign\n",
    "            product_name = random.choice(products_df['product_name'].tolist())\n",
    "            schedule_name = f\"{product_name} Production Campaign\"\n",
    "        else:\n",
    "            # Other schedule types\n",
    "            time_period = random.choice([\"Short-term\", \"Mid-term\", \"Long-term\"])\n",
    "            year = random.randint(datetime.now().year - 1, datetime.now().year + 1)\n",
    "            schedule_name = f\"{time_period} {schedule_type} Schedule {year}\"\n",
    "        \n",
    "        data[\"schedule_name\"].append(schedule_name)\n",
    "        data[\"schedule_type\"].append(schedule_type)\n",
    "        \n",
    "        # Assign facility\n",
    "        data[\"facility_id\"].append(random.choice(facility_ids))\n",
    "        \n",
    "        # Generate schedule dates based on type\n",
    "        time_range_days = (end_time - start_time).days\n",
    "        start_offset = random.randint(0, time_range_days - 1)\n",
    "        schedule_start = start_time + timedelta(days=start_offset)\n",
    "        \n",
    "        # Schedule duration depends on type\n",
    "        if schedule_type == \"Weekly\":\n",
    "            duration_days = 7\n",
    "        elif schedule_type == \"Monthly\":\n",
    "            duration_days = 30\n",
    "        elif schedule_type == \"Quarterly\":\n",
    "            duration_days = 90\n",
    "        elif schedule_type == \"Campaign\":\n",
    "            duration_days = random.randint(14, 60)  # 2-8 weeks\n",
    "        elif schedule_type == \"Master\":\n",
    "            duration_days = random.randint(180, 365)  # 6-12 months\n",
    "        else:\n",
    "            duration_days = random.randint(30, 120)  # 1-4 months\n",
    "            \n",
    "        schedule_end = schedule_start + timedelta(days=duration_days)\n",
    "        \n",
    "        # Ensure end date is within range\n",
    "        if schedule_end > end_time:\n",
    "            schedule_end = end_time\n",
    "            \n",
    "        data[\"start_date\"].append(schedule_start.strftime(\"%Y-%m-%d\"))\n",
    "        data[\"end_date\"].append(schedule_end.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Creation date is typically before start date\n",
    "        creation_offset = random.randint(7, 60)  # 1-8 weeks before\n",
    "        creation_date = schedule_start - timedelta(days=creation_offset)\n",
    "        \n",
    "        # Ensure creation date is not before data range start\n",
    "        if creation_date < start_time:\n",
    "            creation_date = start_time\n",
    "            \n",
    "        data[\"creation_date\"].append(creation_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Assign creator\n",
    "        data[\"created_by\"].append(random.choice(creator_ids))\n",
    "        \n",
    "        # Determine status based on dates\n",
    "        current_date = datetime.now()\n",
    "        \n",
    "        if schedule_start > current_date:\n",
    "            # Future schedules are typically Draft or Approved\n",
    "            status = random.choice([\"Draft\", \"Approved\"])\n",
    "        elif schedule_end < current_date:\n",
    "            # Past schedules are Completed or Superseded\n",
    "            status = random.choice([\"Completed\", \"Completed\", \"Superseded\"])  # Weight toward completed\n",
    "        else:\n",
    "            # Current schedules are In Progress\n",
    "            status = \"In Progress\"\n",
    "            \n",
    "        # Some schedules might be cancelled\n",
    "        if random.random() < 0.05:  # 5% chance\n",
    "            status = \"Cancelled\"\n",
    "            \n",
    "        data[\"status\"].append(status)\n",
    "        \n",
    "        # Set revision\n",
    "        if status in [\"Draft\", \"Approved\"]:\n",
    "            revision = random.randint(1, 3)  # Newer revisions\n",
    "        elif status == \"In Progress\":\n",
    "            revision = random.randint(1, 5)  # Might have several revisions\n",
    "        else:\n",
    "            revision = random.randint(1, 10)  # Completed schedules might have many revisions\n",
    "            \n",
    "        data[\"revision\"].append(revision)\n",
    "        \n",
    "        # Set freeze horizon (period during which schedule cannot be changed)\n",
    "        if schedule_type in [\"Master\", \"Quarterly\"]:\n",
    "            freeze_horizon = random.randint(30, 60)  # 1-2 months\n",
    "        elif schedule_type in [\"Monthly\", \"Campaign\"]:\n",
    "            freeze_horizon = random.randint(14, 30)  # 2-4 weeks\n",
    "        else:\n",
    "            freeze_horizon = random.randint(3, 14)  # 3-14 days\n",
    "            \n",
    "        data[\"freeze_horizon_days\"].append(freeze_horizon)\n",
    "        \n",
    "        # Generate notes (mostly empty)\n",
    "        if random.random() < 0.3:  # 30% chance of having notes\n",
    "            notes_options = [\n",
    "                \"Adjusted for material availability\",\n",
    "                \"Optimized for equipment efficiency\",\n",
    "                \"Consolidated for resource utilization\",\n",
    "                \"Modified to accommodate rush orders\",\n",
    "                \"Updated based on inventory levels\",\n",
    "                \"Revised to match supplier deliveries\",\n",
    "                \"Balanced for labor utilization\",\n",
    "                \"Coordinated with maintenance schedule\",\n",
    "                \"Aligned with quality testing capacity\",\n",
    "                \"Considering seasonal demand factors\"\n",
    "            ]\n",
    "            data[\"notes\"].append(random.choice(notes_options))\n",
    "        else:\n",
    "            data[\"notes\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} production schedule records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_scheduled_production(production_schedules_df, products_df, customer_orders_df=None, \n",
    "                                 equipment_df=None, work_orders_df=None,\n",
    "                                 output_file=\"data/scheduled_production.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the ScheduledProduction table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - production_schedules_df: DataFrame containing production schedules data\n",
    "    - products_df: DataFrame containing products data\n",
    "    - customer_orders_df: DataFrame containing customer orders data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated scheduled production data\n",
    "    \"\"\"\n",
    "    if production_schedules_df is None or len(production_schedules_df) == 0:\n",
    "        print(\"Error: No production schedules data available.\")\n",
    "        return None\n",
    "        \n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"Error: No products data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Load work order IDs if available\n",
    "    if work_orders_df is not None and len(work_orders_df) > 0:\n",
    "        work_order_ids = work_orders_df['work_order_id'].tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(100)]\n",
    "    \n",
    "    # Load equipment IDs if available\n",
    "    if equipment_df is not None and len(equipment_df) > 0:\n",
    "        equipment_ids = equipment_df['equipment_id'].tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic equipment IDs...\")\n",
    "        equipment_ids = [f\"EQ-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    \n",
    "    # Load customer order IDs if available\n",
    "    if customer_orders_df is not None and len(customer_orders_df) > 0:\n",
    "        order_ids = customer_orders_df['order_id'].tolist()\n",
    "    else:\n",
    "        print(\"Generating synthetic customer order IDs...\")\n",
    "        order_ids = [f\"CO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"scheduled_id\": [],\n",
    "        \"schedule_id\": [],\n",
    "        \"product_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"scheduled_quantity\": [],\n",
    "        \"start_date\": [],\n",
    "        \"end_date\": [],\n",
    "        \"equipment_id\": [],\n",
    "        \"priority\": [],\n",
    "        \"status\": [],\n",
    "        \"order_id\": []\n",
    "    }\n",
    "    \n",
    "    # Generate scheduled production items for each production schedule\n",
    "    for _, schedule in production_schedules_df.iterrows():\n",
    "        schedule_id = schedule['schedule_id']\n",
    "        schedule_start = pd.to_datetime(schedule['start_date'])\n",
    "        schedule_end = pd.to_datetime(schedule['end_date'])\n",
    "        schedule_status = schedule['status']\n",
    "        \n",
    "        # Determine number of production items for this schedule\n",
    "        # More detailed schedules have more items\n",
    "        if schedule['schedule_type'] in [\"Detailed\", \"Weekly\"]:\n",
    "            num_items = random.randint(10, 30)\n",
    "        elif schedule['schedule_type'] == \"Campaign\":\n",
    "            num_items = random.randint(5, 15)\n",
    "        else:\n",
    "            num_items = random.randint(5, 20)\n",
    "        \n",
    "        # Generate production items\n",
    "        for _ in range(num_items):\n",
    "            # Create unique scheduled production ID\n",
    "            scheduled_id = f\"SP-{uuid.uuid4().hex[:8].upper()}\"\n",
    "            data[\"scheduled_id\"].append(scheduled_id)\n",
    "            data[\"schedule_id\"].append(schedule_id)\n",
    "            \n",
    "            # Select product\n",
    "            product = products_df.sample(1).iloc[0]\n",
    "            data[\"product_id\"].append(product['product_id'])\n",
    "            \n",
    "            # Assign work order (some items might not have work orders yet)\n",
    "            if schedule_status in [\"In Progress\", \"Completed\"] and random.random() < 0.9:\n",
    "                # Most in-progress or completed schedule items have work orders\n",
    "                data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "            elif schedule_status == \"Approved\" and random.random() < 0.5:\n",
    "                # Some approved schedule items have work orders\n",
    "                data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "            else:\n",
    "                data[\"work_order_id\"].append(\"\")\n",
    "            \n",
    "            # Generate scheduled quantity\n",
    "            scheduled_quantity = random.randint(100, 10000)\n",
    "            data[\"scheduled_quantity\"].append(scheduled_quantity)\n",
    "            \n",
    "            # Generate start and end dates\n",
    "            # Distribute items across the schedule period\n",
    "            schedule_days = (schedule_end - schedule_start).days\n",
    "            if schedule_days <= 0:\n",
    "                schedule_days = 1  # Ensure at least 1 day\n",
    "                \n",
    "            item_start_offset = random.randint(0, max(0, schedule_days - 1))\n",
    "            item_start = schedule_start + timedelta(days=item_start_offset)\n",
    "            \n",
    "            # Item duration depends on quantity and complexity\n",
    "            if 'product_family' in product and product['product_family'] in [\"Pharmaceutical\", \"Chemical\"]:\n",
    "                # Complex products take longer\n",
    "                item_duration = random.randint(3, 14)  # 3-14 days\n",
    "            else:\n",
    "                item_duration = random.randint(1, 7)  # 1-7 days\n",
    "                \n",
    "            item_end = item_start + timedelta(days=item_duration)\n",
    "            \n",
    "            # Ensure end date is within schedule\n",
    "            if item_end > schedule_end:\n",
    "                item_end = schedule_end\n",
    "                \n",
    "            data[\"start_date\"].append(item_start.strftime(\"%Y-%m-%d\"))\n",
    "            data[\"end_date\"].append(item_end.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            # Assign equipment\n",
    "            data[\"equipment_id\"].append(random.choice(equipment_ids))\n",
    "            \n",
    "            # Set priority\n",
    "            data[\"priority\"].append(random.randint(1, 5))  # 1=highest, 5=lowest\n",
    "            \n",
    "            # Set status based on schedule status and dates\n",
    "            current_date = datetime.now()\n",
    "            \n",
    "            if schedule_status == \"Cancelled\":\n",
    "                item_status = \"Cancelled\"\n",
    "            elif schedule_status == \"Draft\":\n",
    "                item_status = \"Planned\"\n",
    "            elif schedule_status == \"Approved\":\n",
    "                if item_start > current_date:\n",
    "                    item_status = \"Scheduled\"\n",
    "                else:\n",
    "                    item_status = random.choice([\"Scheduled\", \"Released\"])\n",
    "            elif schedule_status == \"In Progress\":\n",
    "                if item_start > current_date:\n",
    "                    item_status = \"Scheduled\"\n",
    "                elif item_end < current_date:\n",
    "                    item_status = random.choice([\"Completed\", \"Completed\", \"Canceled\"])\n",
    "                else:\n",
    "                    item_status = random.choice([\"Released\", \"In Progress\", \"Held\"])\n",
    "            elif schedule_status in [\"Completed\", \"Superseded\"]:\n",
    "                if random.random() < 0.9:  # 90% chance\n",
    "                    item_status = \"Completed\"\n",
    "                else:\n",
    "                    item_status = random.choice([\"Canceled\", \"Partially Completed\"])\n",
    "            else:\n",
    "                item_status = \"Planned\"\n",
    "                \n",
    "            data[\"status\"].append(item_status)\n",
    "            \n",
    "            # Link to customer order (if applicable)\n",
    "            if random.random() < 0.7:  # 70% linked to order\n",
    "                data[\"order_id\"].append(random.choice(order_ids))\n",
    "            else:\n",
    "                data[\"order_id\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} scheduled production records for {len(production_schedules_df)} schedules.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(production_schedules_df, scheduled_production_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated production schedules and scheduled production data\n",
    "    \n",
    "    Parameters:\n",
    "    - production_schedules_df: DataFrame containing production schedules data\n",
    "    - scheduled_production_df: DataFrame containing scheduled production data (optional)\n",
    "    \"\"\"\n",
    "    if production_schedules_df is None or len(production_schedules_df) == 0:\n",
    "        print(\"No production schedules data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nProduction Schedules Statistics:\")\n",
    "    print(f\"Total production schedules: {len(production_schedules_df)}\")\n",
    "    \n",
    "    # Schedule type distribution\n",
    "    print(\"\\nSchedule Type Distribution:\")\n",
    "    type_counts = production_schedules_df['schedule_type'].value_counts()\n",
    "    for schedule_type, count in type_counts.items():\n",
    "        print(f\"  {schedule_type}: {count} ({count/len(production_schedules_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Status distribution\n",
    "    print(\"\\nSchedule Status Distribution:\")\n",
    "    status_counts = production_schedules_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(production_schedules_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Duration statistics\n",
    "    production_schedules_df['start_date'] = pd.to_datetime(production_schedules_df['start_date'])\n",
    "    production_schedules_df['end_date'] = pd.to_datetime(production_schedules_df['end_date'])\n",
    "    \n",
    "    production_schedules_df['duration_days'] = (production_schedules_df['end_date'] - \n",
    "                                            production_schedules_df['start_date']).dt.days\n",
    "    \n",
    "    print(\"\\nSchedule Duration Statistics:\")\n",
    "    print(f\"  Average duration: {production_schedules_df['duration_days'].mean():.1f} days\")\n",
    "    print(f\"  Minimum duration: {production_schedules_df['duration_days'].min()} days\")\n",
    "    print(f\"  Maximum duration: {production_schedules_df['duration_days'].max()} days\")\n",
    "    \n",
    "    # Freeze horizon statistics\n",
    "    print(\"\\nFreeze Horizon Statistics:\")\n",
    "    print(f\"  Average freeze horizon: {production_schedules_df['freeze_horizon_days'].mean():.1f} days\")\n",
    "    print(f\"  Minimum freeze horizon: {production_schedules_df['freeze_horizon_days'].min()} days\")\n",
    "    print(f\"  Maximum freeze horizon: {production_schedules_df['freeze_horizon_days'].max()} days\")\n",
    "    \n",
    "    # Schedule by facility\n",
    "    facility_counts = production_schedules_df.groupby('facility_id').size()\n",
    "    \n",
    "    print(\"\\nSchedules by Facility:\")\n",
    "    for facility, count in facility_counts.items():\n",
    "        print(f\"  {facility}: {count} schedules\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    production_schedules_df['month_year'] = production_schedules_df['start_date'].dt.to_period('M')\n",
    "    monthly_schedules = production_schedules_df.groupby('month_year').size().sort_index()\n",
    "    \n",
    "    print(\"\\nMonthly Schedule Distribution:\")\n",
    "    for month, count in monthly_schedules.items():\n",
    "        print(f\"  {month}: {count} schedules\")\n",
    "    \n",
    "    # Scheduled production statistics (if available)\n",
    "    if scheduled_production_df is not None and len(scheduled_production_df) > 0:\n",
    "        print(\"\\nScheduled Production Statistics:\")\n",
    "        print(f\"Total scheduled production items: {len(scheduled_production_df)}\")\n",
    "        print(f\"Items per schedule: {len(scheduled_production_df)/len(production_schedules_df):.1f}\")\n",
    "        \n",
    "        # Status distribution\n",
    "        print(\"\\nItem Status Distribution:\")\n",
    "        item_status_counts = scheduled_production_df['status'].value_counts()\n",
    "        for status, count in item_status_counts.items():\n",
    "            print(f\"  {status}: {count} ({count/len(scheduled_production_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Priority distribution\n",
    "        print(\"\\nPriority Distribution:\")\n",
    "        priority_counts = scheduled_production_df['priority'].value_counts().sort_index()\n",
    "        for priority, count in priority_counts.items():\n",
    "            print(f\"  Priority {priority}: {count} ({count/len(scheduled_production_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Work order association\n",
    "        has_wo = scheduled_production_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "        print(f\"\\nItems with work order association: {has_wo} ({has_wo/len(scheduled_production_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Customer order association\n",
    "        has_co = scheduled_production_df['order_id'].apply(lambda x: x != \"\").sum()\n",
    "        print(f\"Items with customer order association: {has_co} ({has_co/len(scheduled_production_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Quantity statistics\n",
    "        print(\"\\nQuantity Statistics:\")\n",
    "        print(f\"  Average scheduled quantity: {scheduled_production_df['scheduled_quantity'].mean():.1f}\")\n",
    "        print(f\"  Total scheduled quantity: {scheduled_production_df['scheduled_quantity'].sum()}\")\n",
    "        \n",
    "        # Duration statistics\n",
    "        scheduled_production_df['start_date'] = pd.to_datetime(scheduled_production_df['start_date'])\n",
    "        scheduled_production_df['end_date'] = pd.to_datetime(scheduled_production_df['end_date'])\n",
    "        \n",
    "        scheduled_production_df['duration_days'] = (scheduled_production_df['end_date'] - \n",
    "                                                 scheduled_production_df['start_date']).dt.days\n",
    "        \n",
    "        print(\"\\nItem Duration Statistics:\")\n",
    "        print(f\"  Average duration: {scheduled_production_df['duration_days'].mean():.1f} days\")\n",
    "        print(f\"  Minimum duration: {scheduled_production_df['duration_days'].min()} days\")\n",
    "        print(f\"  Maximum duration: {scheduled_production_df['duration_days'].max()} days\")\n",
    "        \n",
    "        # Product distribution\n",
    "        product_counts = scheduled_production_df.groupby('product_id').size().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Products in Production Schedules:\")\n",
    "        for product, count in product_counts.head(10).items():\n",
    "            print(f\"  {product}: {count} scheduled items ({count/len(scheduled_production_df)*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    products_df = load_products_data()\n",
    "    customer_orders_df = load_customer_orders_data()\n",
    "    equipment_df = load_equipment_data()\n",
    "    facilities_df = load_facilities_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    \n",
    "    try:\n",
    "        work_orders_df = pd.read_csv(\"data/work_orders.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Note: Work orders data file not found.\")\n",
    "        work_orders_df = None\n",
    "    \n",
    "    if products_df is not None:\n",
    "        # Generate production schedules data\n",
    "        production_schedules_df = generate_production_schedules(\n",
    "            products_df,\n",
    "            customer_orders_df,\n",
    "            equipment_df,\n",
    "            facilities_df,\n",
    "            personnel_df,\n",
    "            num_schedules=20,  # Generate 20 production schedule records\n",
    "            output_file=\"data/production_schedules.csv\"\n",
    "        )\n",
    "        \n",
    "        if production_schedules_df is not None:\n",
    "            # Generate scheduled production data\n",
    "            scheduled_production_df = generate_scheduled_production(\n",
    "                production_schedules_df,\n",
    "                products_df,\n",
    "                customer_orders_df,\n",
    "                equipment_df,\n",
    "                work_orders_df,\n",
    "                output_file=\"data/scheduled_production.csv\"\n",
    "            )\n",
    "        else:\n",
    "            scheduled_production_df = None\n",
    "    else:\n",
    "        production_schedules_df = None\n",
    "        scheduled_production_df = None\n",
    "    \n",
    "    # Display statistics\n",
    "    if production_schedules_df is not None:\n",
    "        display_statistics(production_schedules_df, scheduled_production_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample production schedules data (first 5 records):\")\n",
    "        print(production_schedules_df.head(5))\n",
    "        \n",
    "        if scheduled_production_df is not None:\n",
    "            print(\"\\nSample scheduled production data (first 5 records):\")\n",
    "            print(scheduled_production_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a9215",
   "metadata": {},
   "source": [
    "Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b086d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file not found.\n",
      "Generating synthetic manager IDs...\n",
      "Successfully generated 15 facility records.\n",
      "Data saved to data/facilities.csv\n",
      "\n",
      "Facilities Statistics:\n",
      "Total facilities: 15\n",
      "\n",
      "Facility Type Distribution:\n",
      "  Distribution Center: 5 (33.3%)\n",
      "  Warehouse: 4 (26.7%)\n",
      "  R&D Center: 2 (13.3%)\n",
      "  Administrative Office: 2 (13.3%)\n",
      "  Manufacturing Plant: 2 (13.3%)\n",
      "\n",
      "Status Distribution:\n",
      "  Active: 13 (86.7%)\n",
      "  Under Construction: 2 (13.3%)\n",
      "\n",
      "Operating Hours Distribution:\n",
      "  Mon-Fri: 8AM-5PM: 5 (33.3%)\n",
      "  Mon-Sat: 6AM-10PM: 5 (33.3%)\n",
      "  24/7: 3 (20.0%)\n",
      "  Mon-Fri: 7AM-7PM, Sat: 8AM-12PM: 2 (13.3%)\n",
      "\n",
      "Facilities with parent: 9 (60.0%)\n",
      "\n",
      "Regional Distribution:\n",
      "  Canada: 2 (13.3%)\n",
      "  Singapore: 2 (13.3%)\n",
      "  Italy: 2 (13.3%)\n",
      "  India: 1 (6.7%)\n",
      "  Brazil: 1 (6.7%)\n",
      "  New Zealand: 1 (6.7%)\n",
      "  Australia: 1 (6.7%)\n",
      "  Netherlands: 1 (6.7%)\n",
      "  Colombia: 1 (6.7%)\n",
      "  China: 1 (6.7%)\n",
      "  USA: 1 (6.7%)\n",
      "  UK: 1 (6.7%)\n",
      "\n",
      "Parent Facilities:\n",
      "  Latin America Distribution Warehouse (Warehouse): 2 child facilities\n",
      "  Jones Distribution Center (Distribution Center): 3 child facilities\n",
      "  Europe Distribution Warehouse (Warehouse): 4 child facilities\n",
      "\n",
      "Sample facilities data (first 5 records):\n",
      "    facility_id                         facility_name          facility_type  \\\n",
      "0  FAC-13B225D9                      R&D Center Jones             R&D Center   \n",
      "1  FAC-DE4EBAD4  West Martinfield Distribution Center    Distribution Center   \n",
      "2  FAC-C75B878C  Latin America Distribution Warehouse              Warehouse   \n",
      "3  FAC-91AA1B91         Oceania Administrative Office  Administrative Office   \n",
      "4  FAC-C618CC2F         Australia Distribution Center    Distribution Center   \n",
      "\n",
      "                                             address     manager_id  \\\n",
      "0                 8618 Williams Street, Jones, India  PERS-ACB23370   \n",
      "1    5024 Hernandez Street, West Martinfield, Canada  PERS-971EC4B1   \n",
      "2                1379 Johnson Lane, Martinez, Brazil  PERS-F9269DA0   \n",
      "3            5289 Martin Avenue, Thomas, New Zealand  PERS-E8829BF4   \n",
      "4  4228 Martin Street, South Williamshaven, Austr...  PERS-03787CA8   \n",
      "\n",
      "                   operating_hours              status parent_facility_id  \\\n",
      "0                 Mon-Fri: 8AM-5PM              Active                      \n",
      "1                Mon-Sat: 6AM-10PM              Active       FAC-C75B878C   \n",
      "2                             24/7              Active                      \n",
      "3                 Mon-Fri: 8AM-5PM  Under Construction       FAC-E0B9C395   \n",
      "4  Mon-Fri: 7AM-7PM, Sat: 8AM-12PM  Under Construction       FAC-E0B9C395   \n",
      "\n",
      "       country  \n",
      "0        India  \n",
      "1       Canada  \n",
      "2       Brazil  \n",
      "3  New Zealand  \n",
      "4    Australia  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Facilities will be generated with synthetic manager IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_facilities_data(personnel_df=None, num_facilities=15, output_file=\"data/facilities.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the Facilities table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_facilities: Number of facility records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated facilities data\n",
    "    \"\"\"\n",
    "    # Define facility types and their probabilities\n",
    "    facility_types = {\n",
    "        \"Manufacturing Plant\": 0.5,\n",
    "        \"Warehouse\": 0.2,\n",
    "        \"Distribution Center\": 0.15,\n",
    "        \"R&D Center\": 0.1,\n",
    "        \"Administrative Office\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Define regions for address generation\n",
    "    regions = {\n",
    "        \"North America\": [\"USA\", \"Canada\", \"Mexico\"],\n",
    "        \"Europe\": [\"UK\", \"Germany\", \"France\", \"Italy\", \"Spain\", \"Netherlands\"],\n",
    "        \"Asia\": [\"Japan\", \"China\", \"South Korea\", \"India\", \"Singapore\", \"Taiwan\"],\n",
    "        \"Latin America\": [\"Brazil\", \"Colombia\", \"Argentina\", \"Chile\", \"Peru\"],\n",
    "        \"Oceania\": [\"Australia\", \"New Zealand\"]\n",
    "    }\n",
    "    \n",
    "    # Region probabilities\n",
    "    region_weights = {\n",
    "        \"North America\": 0.4,\n",
    "        \"Europe\": 0.25,\n",
    "        \"Asia\": 0.2,\n",
    "        \"Latin America\": 0.1,\n",
    "        \"Oceania\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Generate manager IDs if personnel_df is not provided\n",
    "    if personnel_df is None or len(personnel_df) == 0:\n",
    "        print(\"Generating synthetic manager IDs...\")\n",
    "        manager_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_facilities)]\n",
    "    else:\n",
    "        # Use actual personnel IDs, preferably managers if role information is available\n",
    "        if 'job_title' in personnel_df.columns:\n",
    "            managers = personnel_df[personnel_df['job_title'].str.contains('Manager|Director|Supervisor', case=False, na=False)]\n",
    "            if len(managers) >= num_facilities:\n",
    "                manager_ids = managers['personnel_id'].sample(num_facilities).tolist()\n",
    "            else:\n",
    "                # If not enough managers, fill in with other personnel\n",
    "                manager_ids = managers['personnel_id'].tolist()\n",
    "                additional_ids = personnel_df[~personnel_df['personnel_id'].isin(manager_ids)]['personnel_id'].sample(num_facilities - len(manager_ids)).tolist()\n",
    "                manager_ids.extend(additional_ids)\n",
    "        else:\n",
    "            # If no job title info, just sample random personnel\n",
    "            manager_ids = personnel_df['personnel_id'].sample(min(num_facilities, len(personnel_df))).tolist()\n",
    "            \n",
    "            # If we don't have enough personnel, generate synthetic IDs for the rest\n",
    "            if len(manager_ids) < num_facilities:\n",
    "                additional_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_facilities - len(manager_ids))]\n",
    "                manager_ids.extend(additional_ids)\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"facility_id\": [f\"FAC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_facilities)],\n",
    "        \"facility_name\": [],\n",
    "        \"facility_type\": [],\n",
    "        \"address\": [],\n",
    "        \"manager_id\": [],\n",
    "        \"operating_hours\": [],\n",
    "        \"status\": [],\n",
    "        \"parent_facility_id\": []\n",
    "    }\n",
    "    \n",
    "    # Last names for city generation\n",
    "    last_names = [\n",
    "        \"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \n",
    "        \"Rodriguez\", \"Martinez\", \"Hernandez\", \"Lopez\", \"Gonzalez\", \"Wilson\", \"Anderson\", \n",
    "        \"Thomas\", \"Taylor\", \"Moore\", \"Jackson\", \"Martin\", \"Lee\", \"Perez\", \"Thompson\"\n",
    "    ]\n",
    "    \n",
    "    # Generate a list of facility IDs for potential parent relationships\n",
    "    all_facility_ids = data[\"facility_id\"].copy()\n",
    "    \n",
    "    # Select some facilities to be headquarters or main facilities (no parent)\n",
    "    num_main_facilities = min(3, num_facilities)  # Up to 3 main facilities or less if fewer total facilities\n",
    "    main_facility_indices = random.sample(range(num_facilities), num_main_facilities)\n",
    "    potential_parent_ids = [all_facility_ids[i] for i in main_facility_indices]\n",
    "    \n",
    "    # Generate data for each facility\n",
    "    for i in range(num_facilities):\n",
    "        # Select facility type (weighted random)\n",
    "        facility_type = random.choices(\n",
    "            list(facility_types.keys()), \n",
    "            weights=list(facility_types.values())\n",
    "        )[0]\n",
    "        data[\"facility_type\"].append(facility_type)\n",
    "        \n",
    "        # Generate facility name\n",
    "        # For manufacturing plants, use format like \"City Manufacturing Plant\"\n",
    "        # For warehouses, use format like \"Regional Distribution Center\"\n",
    "        # For offices, use format like \"Corporate Headquarters\"\n",
    "        \n",
    "        # Generate a region and country\n",
    "        region = random.choices(\n",
    "            list(region_weights.keys()), \n",
    "            weights=list(region_weights.values())\n",
    "        )[0]\n",
    "        country = random.choice(regions[region])\n",
    "        \n",
    "        # Generate a city name (simplified)\n",
    "        city_prefixes = [\"New\", \"East\", \"West\", \"North\", \"South\", \"Central\", \"Upper\", \"Lower\", \"Port\", \"Lake\", \"Mount\"]\n",
    "        city_suffixes = [\"ville\", \"burg\", \"town\", \"field\", \"ford\", \"port\", \"bridge\", \"haven\", \"city\"]\n",
    "        \n",
    "        if random.random() < 0.3:  # 30% chance of using prefix\n",
    "            city = f\"{random.choice(city_prefixes)} {random.choice(last_names)}{random.choice(['', random.choice(city_suffixes)])}\"\n",
    "        else:\n",
    "            city = f\"{random.choice(last_names)}{random.choice(['', random.choice(city_suffixes)])}\"\n",
    "        \n",
    "        # Create facility name based on type\n",
    "        if facility_type == \"Manufacturing Plant\":\n",
    "            facility_name = f\"{city} Manufacturing Plant\"\n",
    "            if random.random() < 0.3:  # 30% chance of adding specialty\n",
    "                specialties = [\"Pharmaceutical\", \"Food\", \"Electronics\", \"Automotive\", \"Chemical\", \"Consumer Goods\"]\n",
    "                facility_name = f\"{city} {random.choice(specialties)} Manufacturing Plant\"\n",
    "        elif facility_type == \"Warehouse\":\n",
    "            facility_name = f\"{region} Distribution Warehouse\"\n",
    "            if random.random() < 0.5:  # 50% chance of adding location\n",
    "                facility_name = f\"{city} Distribution Warehouse\"\n",
    "        elif facility_type == \"Distribution Center\":\n",
    "            facility_name = f\"{country} Distribution Center\"\n",
    "            if random.random() < 0.5:  # 50% chance of adding location\n",
    "                facility_name = f\"{city} Distribution Center\"\n",
    "        elif facility_type == \"R&D Center\":\n",
    "            facility_name = f\"R&D Center {city}\"\n",
    "            if random.random() < 0.3:  # 30% chance of adding specialty\n",
    "                specialties = [\"Pharmaceutical\", \"Formulation\", \"Process Development\", \"Analytical\", \"Innovation\"]\n",
    "                facility_name = f\"{random.choice(specialties)} R&D Center\"\n",
    "        else:  # Administrative Office\n",
    "            if i in main_facility_indices:  # If this is a main facility\n",
    "                facility_name = f\"Corporate Headquarters - {city}\"\n",
    "            else:\n",
    "                facility_name = f\"{region} Administrative Office\"\n",
    "                if random.random() < 0.5:  # 50% chance of adding location\n",
    "                    facility_name = f\"{city} Administrative Office\"\n",
    "        \n",
    "        data[\"facility_name\"].append(facility_name)\n",
    "        \n",
    "        # Generate street address\n",
    "        street_number = random.randint(1, 9999)\n",
    "        street_types = [\"Street\", \"Avenue\", \"Boulevard\", \"Road\", \"Lane\", \"Drive\", \"Way\", \"Place\", \"Court\"]\n",
    "        street_name = f\"{random.choice(last_names)} {random.choice(street_types)}\"\n",
    "        \n",
    "        address = f\"{street_number} {street_name}, {city}, {country}\"\n",
    "        data[\"address\"].append(address)\n",
    "        \n",
    "        # Assign manager\n",
    "        data[\"manager_id\"].append(manager_ids[i % len(manager_ids)])  # Cycle through available managers\n",
    "        \n",
    "        # Set operating hours\n",
    "        operating_hour_options = [\n",
    "            \"24/7\", \n",
    "            \"Mon-Fri: 8AM-5PM\", \n",
    "            \"Mon-Fri: 7AM-7PM, Sat: 8AM-12PM\",\n",
    "            \"Mon-Sat: 6AM-10PM\",\n",
    "            \"Mon-Fri: 6AM-6PM, Weekends: On-call\"\n",
    "        ]\n",
    "        \n",
    "        # Manufacturing plants and warehouses are more likely to have extended hours\n",
    "        if facility_type in [\"Manufacturing Plant\", \"Warehouse\", \"Distribution Center\"]:\n",
    "            hours_weights = [0.4, 0.2, 0.2, 0.15, 0.05]  # More 24/7 operations\n",
    "        else:\n",
    "            hours_weights = [0.05, 0.6, 0.2, 0.1, 0.05]  # More standard business hours\n",
    "            \n",
    "        data[\"operating_hours\"].append(random.choices(operating_hour_options, weights=hours_weights)[0])\n",
    "        \n",
    "        # Set status\n",
    "        statuses = [\"Active\", \"Inactive\", \"Under Construction\", \"Under Renovation\", \"Planned\"]\n",
    "        status_weights = [0.8, 0.05, 0.05, 0.05, 0.05]  # Mostly active facilities\n",
    "        data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        \n",
    "        # Set parent facility ID (if applicable)\n",
    "        if i in main_facility_indices:\n",
    "            # Main facilities have no parent\n",
    "            data[\"parent_facility_id\"].append(\"\")\n",
    "        else:\n",
    "            # Other facilities have a parent with 80% probability\n",
    "            if random.random() < 0.8:\n",
    "                data[\"parent_facility_id\"].append(random.choice(potential_parent_ids))\n",
    "            else:\n",
    "                data[\"parent_facility_id\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} facility records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(facilities_df):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated facilities data\n",
    "    \n",
    "    Parameters:\n",
    "    - facilities_df: DataFrame containing facilities data\n",
    "    \"\"\"\n",
    "    if facilities_df is None or len(facilities_df) == 0:\n",
    "        print(\"No facilities data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nFacilities Statistics:\")\n",
    "    print(f\"Total facilities: {len(facilities_df)}\")\n",
    "    \n",
    "    # Facility type distribution\n",
    "    print(\"\\nFacility Type Distribution:\")\n",
    "    type_counts = facilities_df['facility_type'].value_counts()\n",
    "    for facility_type, count in type_counts.items():\n",
    "        print(f\"  {facility_type}: {count} ({count/len(facilities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Status distribution\n",
    "    print(\"\\nStatus Distribution:\")\n",
    "    status_counts = facilities_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(facilities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Operating hours distribution\n",
    "    print(\"\\nOperating Hours Distribution:\")\n",
    "    hours_counts = facilities_df['operating_hours'].value_counts()\n",
    "    for hours, count in hours_counts.items():\n",
    "        print(f\"  {hours}: {count} ({count/len(facilities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Parent-child relationships\n",
    "    has_parent = facilities_df['parent_facility_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nFacilities with parent: {has_parent} ({has_parent/len(facilities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Facilities by region (extracted from address)\n",
    "    if 'address' in facilities_df.columns:\n",
    "        print(\"\\nRegional Distribution:\")\n",
    "        \n",
    "        # Extract country from address\n",
    "        facilities_df['country'] = facilities_df['address'].apply(lambda x: x.split(',')[-1].strip() if isinstance(x, str) else \"Unknown\")\n",
    "        \n",
    "        country_counts = facilities_df['country'].value_counts()\n",
    "        for country, count in country_counts.items():\n",
    "            print(f\"  {country}: {count} ({count/len(facilities_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Parent facilities analysis\n",
    "    parent_ids = [pid for pid in facilities_df['parent_facility_id'].unique() if pid != \"\"]\n",
    "    if parent_ids:\n",
    "        parent_facilities = facilities_df[facilities_df['facility_id'].isin(parent_ids)]\n",
    "        \n",
    "        print(\"\\nParent Facilities:\")\n",
    "        for _, facility in parent_facilities.iterrows():\n",
    "            child_count = sum(facilities_df['parent_facility_id'] == facility['facility_id'])\n",
    "            print(f\"  {facility['facility_name']} ({facility['facility_type']}): {child_count} child facilities\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Try to load personnel data if available\n",
    "    try:\n",
    "        personnel_df = pd.read_csv(\"data/personnel.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Note: Personnel data file not found.\")\n",
    "        personnel_df = None\n",
    "    \n",
    "    # Generate facilities data\n",
    "    facilities_df = generate_facilities_data(\n",
    "        personnel_df,\n",
    "        num_facilities=15,  # Generate 15 facility records\n",
    "        output_file=\"data/facilities.csv\"\n",
    "    )\n",
    "    \n",
    "    # Display statistics\n",
    "    if facilities_df is not None:\n",
    "        display_statistics(facilities_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample facilities data (first 5 records):\")\n",
    "        print(facilities_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bf997",
   "metadata": {},
   "source": [
    "Storage Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e382c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 444 storage location records across 15 facilities.\n",
      "Data saved to data/storage_locations.csv\n",
      "\n",
      "Storage Locations Statistics:\n",
      "Total storage locations: 444\n",
      "\n",
      "Location Type Distribution (top 10):\n",
      "  Hazardous Material: 13 (2.9%)\n",
      "  Bin: 10 (2.3%)\n",
      "  Freezer: 9 (2.0%)\n",
      "  Shelf: 8 (1.8%)\n",
      "  Sub-Location 29: 8 (1.8%)\n",
      "  Sub-Location 19: 8 (1.8%)\n",
      "  Pallet Rack: 8 (1.8%)\n",
      "  Sub-Location 73: 7 (1.6%)\n",
      "  Sub-Location 99: 7 (1.6%)\n",
      "  Damaged Goods: 6 (1.4%)\n",
      "\n",
      "Status Distribution:\n",
      "  Active: 315 (70.9%)\n",
      "  Inactive: 47 (10.6%)\n",
      "  Full: 31 (7.0%)\n",
      "  Maintenance: 30 (6.8%)\n",
      "  Reserved: 21 (4.7%)\n",
      "\n",
      "Storage Conditions Distribution (top 10):\n",
      "  Room Temperature (15-25°C): 48 (10.8%)\n",
      "  Dry: 42 (9.5%)\n",
      "  Ambient: 41 (9.2%)\n",
      "  Nitrogen Atmosphere: 34 (7.7%)\n",
      "  ESD Protected: 33 (7.4%)\n",
      "  Clean Room ISO Class 7: 31 (7.0%)\n",
      "  Explosion Proof: 30 (6.8%)\n",
      "  Freezer (-20°C): 22 (5.0%)\n",
      "  Humidity Controlled (40-60% RH): 22 (5.0%)\n",
      "  Standard Warehouse Conditions: 22 (5.0%)\n",
      "\n",
      "Locations with parent: 355 (80.0%)\n",
      "\n",
      "Storage Locations by Facility:\n",
      "  Jones Distribution Center: 46 locations (10.4%)\n",
      "  Perezburg Distribution Center: 39 locations (8.8%)\n",
      "  Johnson Distribution Warehouse: 36 locations (8.1%)\n",
      "  Europe Distribution Warehouse: 34 locations (7.7%)\n",
      "  Australia Distribution Center: 34 locations (7.7%)\n",
      "  Latin America Distribution Warehouse: 34 locations (7.7%)\n",
      "  R&D Center Jones: 32 locations (7.2%)\n",
      "  Miller Manufacturing Plant: 30 locations (6.8%)\n",
      "  Smithhaven Food Manufacturing Plant: 28 locations (6.3%)\n",
      "  North America Distribution Warehouse: 25 locations (5.6%)\n",
      "  Oceania Administrative Office: 24 locations (5.4%)\n",
      "  UK Distribution Center: 22 locations (5.0%)\n",
      "  West Martinfield Distribution Center: 22 locations (5.0%)\n",
      "  R&D Center Brownburg: 21 locations (4.7%)\n",
      "  Wilsontown Administrative Office: 17 locations (3.8%)\n",
      "\n",
      "Capacity Statistics:\n",
      "  Total maximum capacity: 218,898\n",
      "  Average maximum capacity: 493.0\n",
      "  Average utilization: 59.2%\n",
      "\n",
      "Locations at or above 90% capacity: 22 (5.0%)\n",
      "Locations at or below 30% capacity: 51 (11.5%)\n",
      "\n",
      "Location Hierarchy Analysis:\n",
      "  Level 1: 89 locations (20.0%)\n",
      "  Level 2: 143 locations (32.2%)\n",
      "  Level 3: 111 locations (25.0%)\n",
      "  Level 4: 59 locations (13.3%)\n",
      "  Level 5: 36 locations (8.1%)\n",
      "  Level 6: 6 locations (1.4%)\n",
      "  Maximum hierarchy depth: 6\n",
      "\n",
      "Sample storage locations data (first 5 records):\n",
      "    location_id                  location_name   facility_id  \\\n",
      "0  LOC-AB4F97D5    Zone A - Hazardous Material  FAC-B0E9A423   \n",
      "1  LOC-8E4244A7          Wing 2 - Cold Storage  FAC-42A69F6B   \n",
      "2  LOC-1436C413                  Block C - Bin  FAC-C75B878C   \n",
      "3  LOC-682375C5  Area 4 - Controlled Substance  FAC-C5BA67D1   \n",
      "4  LOC-C9226733                 Archive Area A  FAC-91AA1B91   \n",
      "\n",
      "          location_type               storage_conditions  maximum_capacity  \\\n",
      "0    Hazardous Material              Nitrogen Atmosphere              1229   \n",
      "1          Cold Storage                  Freezer (-20°C)              3829   \n",
      "2                   Bin  Humidity Controlled (40-60% RH)               708   \n",
      "3  Controlled Substance           Clean Room ISO Class 7              3699   \n",
      "4               Archive                      Oxygen-Free              1146   \n",
      "\n",
      "   current_utilization  status parent_location_id  hierarchy_level  \n",
      "0                 83.8  Active                                   1  \n",
      "1                 68.0  Active                                   1  \n",
      "2                 36.5  Active                                   1  \n",
      "3                 49.4  Active                                   1  \n",
      "4                 61.9  Active                                   1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_facilities_data(facilities_file=\"data/facilities.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated facilities data\n",
    "    \n",
    "    Parameters:\n",
    "    - facilities_file: CSV file containing facilities data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the facilities data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(facilities_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Facilities data file {facilities_file} not found.\")\n",
    "        print(\"Please run the facilities data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def generate_storage_locations(facilities_df, num_locations=None, output_file=\"data/storage_locations.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the StorageLocations table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - facilities_df: DataFrame containing facilities data\n",
    "    - num_locations: Number of storage location records to generate (default: auto-calculate based on facilities)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated storage locations data\n",
    "    \"\"\"\n",
    "    if facilities_df is None or len(facilities_df) == 0:\n",
    "        print(\"Error: No facilities data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Auto-calculate number of locations if not specified\n",
    "    if num_locations is None:\n",
    "        # Generate more storage locations for manufacturing plants and warehouses\n",
    "        # and fewer for offices and R&D centers\n",
    "        num_locations = 0\n",
    "        for _, facility in facilities_df.iterrows():\n",
    "            if 'facility_type' in facility:\n",
    "                if facility['facility_type'] == 'Manufacturing Plant':\n",
    "                    num_locations += random.randint(10, 30)\n",
    "                elif facility['facility_type'] in ['Warehouse', 'Distribution Center']:\n",
    "                    num_locations += random.randint(30, 50)\n",
    "                elif facility['facility_type'] == 'R&D Center':\n",
    "                    num_locations += random.randint(5, 15)\n",
    "                else:  # Administrative Office\n",
    "                    num_locations += random.randint(1, 5)\n",
    "            else:\n",
    "                num_locations += random.randint(10, 20)  # Default if type not available\n",
    "    \n",
    "    # Define storage location types\n",
    "    location_types = {\n",
    "        \"Warehouse\": [\"Bulk Storage\", \"Pallet Rack\", \"Shelf\", \"Bin\", \"Cold Storage\", \"Freezer\", \"Controlled Substance\", \"Hazardous Material\"],\n",
    "        \"Production\": [\"Raw Material Staging\", \"Work In Progress\", \"Finished Goods\", \"Line-Side\", \"Temporary Holding\", \"Quality Control Hold\"],\n",
    "        \"Shipping\": [\"Shipping Dock\", \"Receiving Dock\", \"Staging Area\", \"Cross-Dock\", \"Outbound Queue\", \"Returns Processing\"],\n",
    "        \"Special\": [\"Sample Storage\", \"Archive\", \"Quarantine\", \"QA Lab\", \"Damaged Goods\", \"Rejected Material\", \"Maintenance Supplies\"]\n",
    "    }\n",
    "    \n",
    "    # Define storage conditions\n",
    "    storage_conditions = {\n",
    "        \"Standard\": [\"Ambient\", \"Room Temperature (15-25°C)\", \"Dry\", \"Standard Warehouse Conditions\"],\n",
    "        \"Temperature Controlled\": [\"Refrigerated (2-8°C)\", \"Cold Room (8-15°C)\", \"Freezer (-20°C)\", \"Deep Freeze (-80°C)\", \"Heated (25-40°C)\"],\n",
    "        \"Environmental Control\": [\"Humidity Controlled (<40% RH)\", \"Humidity Controlled (40-60% RH)\", \"Clean Room ISO Class 8\", \"Clean Room ISO Class 7\", \"Clean Room ISO Class 6\"],\n",
    "        \"Special Conditions\": [\"Explosion Proof\", \"Fire Resistant\", \"ESD Protected\", \"Light Protected\", \"Nitrogen Atmosphere\", \"Oxygen-Free\"]\n",
    "    }\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"location_id\": [f\"LOC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_locations)],\n",
    "        \"location_name\": [],\n",
    "        \"facility_id\": [],\n",
    "        \"location_type\": [],\n",
    "        \"storage_conditions\": [],\n",
    "        \"maximum_capacity\": [],\n",
    "        \"current_utilization\": [],\n",
    "        \"status\": [],\n",
    "        \"parent_location_id\": []\n",
    "    }\n",
    "    \n",
    "    # Map to keep track of locations by facility\n",
    "    locations_by_facility = {facility_id: [] for facility_id in facilities_df['facility_id']}\n",
    "    \n",
    "    # Generate root-level storage locations first (ones with no parent)\n",
    "    facility_ids = facilities_df['facility_id'].tolist()\n",
    "    root_locations_count = int(num_locations * 0.2)  # 20% of locations are root level\n",
    "    child_locations_count = num_locations - root_locations_count\n",
    "    \n",
    "    # Keep track of all location IDs\n",
    "    all_location_ids = data[\"location_id\"].copy()\n",
    "    root_location_ids = all_location_ids[:root_locations_count]\n",
    "    \n",
    "    # Generate root locations first\n",
    "    for i in range(root_locations_count):\n",
    "        location_id = all_location_ids[i]\n",
    "        \n",
    "        # Select a facility (weighted toward warehouses and manufacturing plants)\n",
    "        if 'facility_type' in facilities_df.columns:\n",
    "            facility_weights = facilities_df['facility_type'].apply(\n",
    "                lambda x: 3 if x in ['Warehouse', 'Distribution Center'] else \n",
    "                         (2 if x == 'Manufacturing Plant' else 1)\n",
    "            )\n",
    "            facility = facilities_df.sample(weights=facility_weights).iloc[0]\n",
    "        else:\n",
    "            facility = facilities_df.sample(1).iloc[0]\n",
    "            \n",
    "        facility_id = facility['facility_id']\n",
    "        locations_by_facility[facility_id].append(location_id)\n",
    "        data[\"facility_id\"].append(facility_id)\n",
    "        \n",
    "        # Determine location type based on facility type\n",
    "        if 'facility_type' in facility:\n",
    "            if facility['facility_type'] in ['Warehouse', 'Distribution Center']:\n",
    "                location_category = random.choices(\n",
    "                    [\"Warehouse\", \"Shipping\", \"Special\"],\n",
    "                    weights=[0.7, 0.2, 0.1]\n",
    "                )[0]\n",
    "            elif facility['facility_type'] == 'Manufacturing Plant':\n",
    "                location_category = random.choices(\n",
    "                    [\"Warehouse\", \"Production\", \"Shipping\", \"Special\"],\n",
    "                    weights=[0.3, 0.4, 0.2, 0.1]\n",
    "                )[0]\n",
    "            elif facility['facility_type'] == 'R&D Center':\n",
    "                location_category = random.choices(\n",
    "                    [\"Warehouse\", \"Special\"],\n",
    "                    weights=[0.3, 0.7]\n",
    "                )[0]\n",
    "            else:  # Administrative Office\n",
    "                location_category = \"Special\"\n",
    "        else:\n",
    "            location_category = random.choice(list(location_types.keys()))\n",
    "            \n",
    "        location_type = random.choice(location_types[location_category])\n",
    "        data[\"location_type\"].append(location_type)\n",
    "        \n",
    "        # Generate location name\n",
    "        building = random.choice([\"Building\", \"Block\", \"Wing\", \"Area\", \"Zone\"])\n",
    "        building_num = random.choice([\"A\", \"B\", \"C\", \"D\", \"1\", \"2\", \"3\", \"4\"])\n",
    "        \n",
    "        # Format location name based on type\n",
    "        if location_category == \"Warehouse\":\n",
    "            location_name = f\"{building} {building_num} - {location_type}\"\n",
    "        elif location_category == \"Production\":\n",
    "            location_name = f\"Production {building} {building_num} - {location_type}\"\n",
    "        elif location_category == \"Shipping\":\n",
    "            location_name = f\"{location_type} Area {building_num}\"\n",
    "        else:  # Special\n",
    "            location_name = f\"{location_type} {building} {building_num}\"\n",
    "            \n",
    "        data[\"location_name\"].append(location_name)\n",
    "        \n",
    "        # Set storage conditions based on location type\n",
    "        if location_type in [\"Cold Storage\", \"Freezer\", \"Refrigerated\"]:\n",
    "            condition_category = \"Temperature Controlled\"\n",
    "        elif location_type in [\"Clean Room\", \"QA Lab\", \"Controlled Substance\"]:\n",
    "            condition_category = \"Environmental Control\"\n",
    "        elif location_type in [\"Hazardous Material\", \"Quarantine\"]:\n",
    "            condition_category = \"Special Conditions\"\n",
    "        else:\n",
    "            condition_category = random.choices(\n",
    "                list(storage_conditions.keys()),\n",
    "                weights=[0.7, 0.1, 0.1, 0.1]\n",
    "            )[0]\n",
    "            \n",
    "        data[\"storage_conditions\"].append(random.choice(storage_conditions[condition_category]))\n",
    "        \n",
    "        # Set capacity based on location type\n",
    "        if location_type in [\"Bulk Storage\", \"Pallet Rack\", \"Warehouse\"]:\n",
    "            max_capacity = random.randint(1000, 10000)\n",
    "        elif location_type in [\"Shelf\", \"Bin\", \"Line-Side\"]:\n",
    "            max_capacity = random.randint(100, 1000)\n",
    "        elif location_type in [\"Cold Storage\", \"Freezer\", \"Controlled Substance\"]:\n",
    "            max_capacity = random.randint(500, 5000)\n",
    "        else:\n",
    "            max_capacity = random.randint(200, 3000)\n",
    "            \n",
    "        data[\"maximum_capacity\"].append(max_capacity)\n",
    "        \n",
    "        # Set current utilization (as a percentage)\n",
    "        utilization = round(random.uniform(0.3, 0.9) * 100, 1)  # 30-90% utilized\n",
    "        data[\"current_utilization\"].append(utilization)\n",
    "        \n",
    "        # Set status\n",
    "        statuses = [\"Active\", \"Inactive\", \"Maintenance\", \"Full\", \"Reserved\"]\n",
    "        status_weights = [0.8, 0.05, 0.05, 0.05, 0.05]  # Mostly active\n",
    "        data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        \n",
    "        # Root locations have no parent\n",
    "        data[\"parent_location_id\"].append(\"\")\n",
    "    \n",
    "    # Generate child locations\n",
    "    for i in range(child_locations_count):\n",
    "        location_id = all_location_ids[root_locations_count + i]\n",
    "        \n",
    "        # Select a facility and a potential parent location in that facility\n",
    "        facility_id = random.choice(facility_ids)\n",
    "        \n",
    "        # If the facility has root locations, use one as parent\n",
    "        if locations_by_facility[facility_id]:\n",
    "            parent_location_id = random.choice(locations_by_facility[facility_id])\n",
    "            parent_index = all_location_ids.index(parent_location_id)\n",
    "            parent_location_type = data[\"location_type\"][parent_index]\n",
    "            \n",
    "            # Store the facility and parent relationship\n",
    "            data[\"facility_id\"].append(facility_id)\n",
    "            data[\"parent_location_id\"].append(parent_location_id)\n",
    "            \n",
    "            # Child location types are derived from parent types\n",
    "            if parent_location_type in [\"Bulk Storage\", \"Pallet Rack\", \"Warehouse\"]:\n",
    "                child_types = [\"Aisle\", \"Bay\", \"Rack\", \"Section\", \"Zone\"]\n",
    "                location_type = f\"{random.choice(child_types)} {random.randint(1, 99):02d}\"\n",
    "            elif parent_location_type in [\"Shelf\", \"Bin\"]:\n",
    "                location_type = f\"{parent_location_type} {random.randint(1, 999):03d}\"\n",
    "            elif parent_location_type in [\"Cold Storage\", \"Freezer\"]:\n",
    "                location_type = f\"{parent_location_type} Section {random.randint(1, 20):02d}\"\n",
    "            elif parent_location_type in [\"Raw Material Staging\", \"Finished Goods\"]:\n",
    "                location_type = f\"{parent_location_type} Area {random.choice(['A', 'B', 'C', 'D'])}{random.randint(1, 20):02d}\"\n",
    "            else:\n",
    "                # Default child naming for other parent types\n",
    "                location_type = f\"Sub-Location {random.randint(1, 99):02d}\"\n",
    "        else:\n",
    "            # If no root locations in this facility, create as a root location\n",
    "            data[\"facility_id\"].append(facility_id)\n",
    "            data[\"parent_location_id\"].append(\"\")\n",
    "            \n",
    "            # Pick a random location type\n",
    "            location_category = random.choice(list(location_types.keys()))\n",
    "            location_type = random.choice(location_types[location_category])\n",
    "        \n",
    "        data[\"location_type\"].append(location_type)\n",
    "        \n",
    "        # Generate location name\n",
    "        if data[\"parent_location_id\"][-1]:  # If has parent\n",
    "            parent_index = all_location_ids.index(data[\"parent_location_id\"][-1])\n",
    "            parent_name = data[\"location_name\"][parent_index]\n",
    "            location_name = f\"{parent_name} - {location_type}\"\n",
    "        else:\n",
    "            building = random.choice([\"Building\", \"Block\", \"Wing\", \"Area\", \"Zone\"])\n",
    "            building_num = random.choice([\"A\", \"B\", \"C\", \"D\", \"1\", \"2\", \"3\", \"4\"])\n",
    "            location_name = f\"{building} {building_num} - {location_type}\"\n",
    "            \n",
    "        data[\"location_name\"].append(location_name)\n",
    "        \n",
    "        # Set storage conditions (inherit from parent or generate new)\n",
    "        if data[\"parent_location_id\"][-1]:\n",
    "            # 80% chance to inherit parent's conditions\n",
    "            if random.random() < 0.8:\n",
    "                parent_index = all_location_ids.index(data[\"parent_location_id\"][-1])\n",
    "                data[\"storage_conditions\"].append(data[\"storage_conditions\"][parent_index])\n",
    "            else:\n",
    "                condition_category = random.choice(list(storage_conditions.keys()))\n",
    "                data[\"storage_conditions\"].append(random.choice(storage_conditions[condition_category]))\n",
    "        else:\n",
    "            condition_category = random.choice(list(storage_conditions.keys()))\n",
    "            data[\"storage_conditions\"].append(random.choice(storage_conditions[condition_category]))\n",
    "        \n",
    "        # Set capacity (smaller for child locations)\n",
    "        if data[\"parent_location_id\"][-1]:\n",
    "            parent_index = all_location_ids.index(data[\"parent_location_id\"][-1])\n",
    "            parent_capacity = data[\"maximum_capacity\"][parent_index]\n",
    "            # Child capacity is a fraction of parent capacity\n",
    "            max_capacity = int(parent_capacity * random.uniform(0.05, 0.2))\n",
    "        else:\n",
    "            max_capacity = random.randint(100, 5000)\n",
    "            \n",
    "        data[\"maximum_capacity\"].append(max_capacity)\n",
    "        \n",
    "        # Set current utilization (as a percentage)\n",
    "        utilization = round(random.uniform(0.2, 0.95) * 100, 1)  # 20-95% utilized\n",
    "        data[\"current_utilization\"].append(utilization)\n",
    "        \n",
    "        # Set status (inherit from parent with some variation)\n",
    "        if data[\"parent_location_id\"][-1]:\n",
    "            parent_index = all_location_ids.index(data[\"parent_location_id\"][-1])\n",
    "            parent_status = data[\"status\"][parent_index]\n",
    "            \n",
    "            if parent_status == \"Inactive\":\n",
    "                # Inactive parents have inactive children\n",
    "                data[\"status\"].append(\"Inactive\")\n",
    "            elif parent_status == \"Maintenance\":\n",
    "                # Maintenance parents likely have maintenance children\n",
    "                statuses = [\"Maintenance\", \"Inactive\", \"Active\"]\n",
    "                data[\"status\"].append(random.choices(statuses, weights=[0.7, 0.2, 0.1])[0])\n",
    "            else:\n",
    "                # Active parents have mostly active children\n",
    "                statuses = [\"Active\", \"Inactive\", \"Maintenance\", \"Full\", \"Reserved\"]\n",
    "                status_weights = [0.75, 0.05, 0.05, 0.1, 0.05]\n",
    "                data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        else:\n",
    "            statuses = [\"Active\", \"Inactive\", \"Maintenance\", \"Full\", \"Reserved\"]\n",
    "            status_weights = [0.8, 0.05, 0.05, 0.05, 0.05]  # Mostly active\n",
    "            data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        \n",
    "        # Add this location to the facility's location list\n",
    "        locations_by_facility[facility_id].append(location_id)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} storage location records across {len(facilities_df)} facilities.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(storage_locations_df, facilities_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated storage locations data\n",
    "    \n",
    "    Parameters:\n",
    "    - storage_locations_df: DataFrame containing storage locations data\n",
    "    - facilities_df: DataFrame containing facilities data (optional)\n",
    "    \"\"\"\n",
    "    if storage_locations_df is None or len(storage_locations_df) == 0:\n",
    "        print(\"No storage locations data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nStorage Locations Statistics:\")\n",
    "    print(f\"Total storage locations: {len(storage_locations_df)}\")\n",
    "    \n",
    "    # Location type distribution (top 10)\n",
    "    print(\"\\nLocation Type Distribution (top 10):\")\n",
    "    type_counts = storage_locations_df['location_type'].value_counts().head(10)\n",
    "    for loc_type, count in type_counts.items():\n",
    "        print(f\"  {loc_type}: {count} ({count/len(storage_locations_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Status distribution\n",
    "    print(\"\\nStatus Distribution:\")\n",
    "    status_counts = storage_locations_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(storage_locations_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Storage conditions distribution (top 10)\n",
    "    print(\"\\nStorage Conditions Distribution (top 10):\")\n",
    "    condition_counts = storage_locations_df['storage_conditions'].value_counts().head(10)\n",
    "    for condition, count in condition_counts.items():\n",
    "        print(f\"  {condition}: {count} ({count/len(storage_locations_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Parent-child relationships\n",
    "    has_parent = storage_locations_df['parent_location_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nLocations with parent: {has_parent} ({has_parent/len(storage_locations_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Distribution by facility\n",
    "    facility_counts = storage_locations_df.groupby('facility_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nStorage Locations by Facility:\")\n",
    "    if facilities_df is not None:\n",
    "        # Create a mapping of facility_id to facility_name\n",
    "        facility_names = dict(zip(facilities_df['facility_id'], facilities_df['facility_name']))\n",
    "        \n",
    "        for facility_id, count in facility_counts.items():\n",
    "            facility_name = facility_names.get(facility_id, facility_id)\n",
    "            print(f\"  {facility_name}: {count} locations ({count/len(storage_locations_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for facility_id, count in facility_counts.items():\n",
    "            print(f\"  {facility_id}: {count} locations ({count/len(storage_locations_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Capacity and utilization statistics\n",
    "    print(\"\\nCapacity Statistics:\")\n",
    "    print(f\"  Total maximum capacity: {storage_locations_df['maximum_capacity'].sum():,}\")\n",
    "    print(f\"  Average maximum capacity: {storage_locations_df['maximum_capacity'].mean():.1f}\")\n",
    "    print(f\"  Average utilization: {storage_locations_df['current_utilization'].mean():.1f}%\")\n",
    "    \n",
    "    # Full or near-full locations\n",
    "    full_locations = storage_locations_df[storage_locations_df['current_utilization'] >= 90]\n",
    "    print(f\"\\nLocations at or above 90% capacity: {len(full_locations)} ({len(full_locations)/len(storage_locations_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Underutilized locations\n",
    "    under_locations = storage_locations_df[storage_locations_df['current_utilization'] <= 30]\n",
    "    print(f\"Locations at or below 30% capacity: {len(under_locations)} ({len(under_locations)/len(storage_locations_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Hierarchy depth analysis\n",
    "    if has_parent > 0:\n",
    "        print(\"\\nLocation Hierarchy Analysis:\")\n",
    "        \n",
    "        # Count locations at each hierarchy level\n",
    "        storage_locations_df['hierarchy_level'] = 1  # Start all at level 1\n",
    "        \n",
    "        # Create a copy to avoid SettingWithCopyWarning\n",
    "        temp_df = storage_locations_df.copy()\n",
    "        \n",
    "        # Create a mapping of location_id to index for faster lookup\n",
    "        location_id_to_index = {loc_id: i for i, loc_id in enumerate(temp_df['location_id'])}\n",
    "        \n",
    "        # Iterate to propagate levels\n",
    "        max_iterations = 10  # Avoid infinite loops\n",
    "        for _ in range(max_iterations):\n",
    "            changes = 0\n",
    "            for i, row in temp_df.iterrows():\n",
    "                if row['parent_location_id'] != \"\":\n",
    "                    parent_idx = location_id_to_index.get(row['parent_location_id'])\n",
    "                    if parent_idx is not None:\n",
    "                        parent_level = temp_df.at[parent_idx, 'hierarchy_level']\n",
    "                        child_level = parent_level + 1\n",
    "                        if temp_df.at[i, 'hierarchy_level'] != child_level:\n",
    "                            temp_df.at[i, 'hierarchy_level'] = child_level\n",
    "                            changes += 1\n",
    "            if changes == 0:\n",
    "                break\n",
    "                \n",
    "        # Count locations at each level\n",
    "        level_counts = temp_df['hierarchy_level'].value_counts().sort_index()\n",
    "        for level, count in level_counts.items():\n",
    "            print(f\"  Level {level}: {count} locations ({count/len(temp_df)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"  Maximum hierarchy depth: {level_counts.index.max()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    facilities_df = load_facilities_data()\n",
    "    \n",
    "    if facilities_df is not None:\n",
    "        # Generate storage locations data\n",
    "        storage_locations_df = generate_storage_locations(\n",
    "            facilities_df,\n",
    "            num_locations=None,  # Auto-calculate based on facilities\n",
    "            output_file=\"data/storage_locations.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if storage_locations_df is not None:\n",
    "            display_statistics(storage_locations_df, facilities_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample storage locations data (first 5 records):\")\n",
    "            print(storage_locations_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158374ab",
   "metadata": {},
   "source": [
    "Shifts Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1442d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Shifts will be generated with synthetic supervisor IDs.\n",
      "Generating synthetic supervisor IDs...\n",
      "Successfully generated 58 shift records across 15 facilities.\n",
      "Data saved to data/shifts.csv\n",
      "\n",
      "Shifts Statistics:\n",
      "Total shifts: 58\n",
      "\n",
      "Shift Duration Statistics:\n",
      "  Average shift duration: 9.8 hours\n",
      "  Minimum shift duration: 8.0 hours\n",
      "  Maximum shift duration: 12.0 hours\n",
      "\n",
      "Shift Distribution by Time of Day:\n",
      "  Morning: 25 shifts (43.1%)\n",
      "  Afternoon: 11 shifts (19.0%)\n",
      "  Night: 11 shifts (19.0%)\n",
      "  Evening: 11 shifts (19.0%)\n",
      "\n",
      "Shifts by Facility:\n",
      "  R&D Center Jones: 5 shifts (8.6%)\n",
      "  North America Distribution Warehouse: 5 shifts (8.6%)\n",
      "  Johnson Distribution Warehouse: 5 shifts (8.6%)\n",
      "  Miller Manufacturing Plant: 5 shifts (8.6%)\n",
      "  Europe Distribution Warehouse: 5 shifts (8.6%)\n",
      "  R&D Center Brownburg: 5 shifts (8.6%)\n",
      "  West Martinfield Distribution Center: 5 shifts (8.6%)\n",
      "  Smithhaven Food Manufacturing Plant: 4 shifts (6.9%)\n",
      "  Latin America Distribution Warehouse: 4 shifts (6.9%)\n",
      "  UK Distribution Center: 3 shifts (5.2%)\n",
      "  Perezburg Distribution Center: 3 shifts (5.2%)\n",
      "  Australia Distribution Center: 3 shifts (5.2%)\n",
      "  Jones Distribution Center: 3 shifts (5.2%)\n",
      "  Oceania Administrative Office: 2 shifts (3.4%)\n",
      "  Wilsontown Administrative Office: 1 shifts (1.7%)\n",
      "\n",
      "Break Period Analysis:\n",
      "  Shifts with breaks: 58 (100.0%)\n",
      "\n",
      "Common Shift Names:\n",
      "  Weekend Day: 9 shifts (15.5%)\n",
      "  Weekend Night: 9 shifts (15.5%)\n",
      "  Morning Shift: 8 shifts (13.8%)\n",
      "  Afternoon Shift: 8 shifts (13.8%)\n",
      "  Overnight Shift: 8 shifts (13.8%)\n",
      "\n",
      "24-Hour Coverage Analysis:\n",
      "  Full 24-hour coverage likely: 11 facilities (73.3%)\n",
      "  Partial coverage (missing night): 2 facilities (13.3%)\n",
      "  Day coverage only: 2 facilities (13.3%)\n",
      "\n",
      "Sample shifts data (first 5 records):\n",
      "         shift_id           shift_name   facility_id start_time end_time  \\\n",
      "0  SHIFT-3F7138E2      Day Shift (R&D)  FAC-13B225D9      08:00    16:00   \n",
      "1  SHIFT-D161988F  Evening Shift (R&D)  FAC-13B225D9      16:00    00:00   \n",
      "2  SHIFT-9767776C    Night Shift (R&D)  FAC-13B225D9      00:00    08:00   \n",
      "3  SHIFT-57F6D954    Weekend Day (R&D)  FAC-13B225D9      07:00    19:00   \n",
      "4  SHIFT-8D2F7BA8  Weekend Night (R&D)  FAC-13B225D9      19:00    07:00   \n",
      "\n",
      "                                       break_periods  supervisor_id  \\\n",
      "0  [{'start': '12:00', 'end': '12:30', 'type': 'L...  PERS-BA32F96B   \n",
      "1  [{'start': '20:00', 'end': '20:30', 'type': 'D...  PERS-31433D70   \n",
      "2  [{'start': '04:00', 'end': '04:30', 'type': 'M...  PERS-F0D4E8A1   \n",
      "3  [{'start': '12:00', 'end': '12:45', 'type': 'L...  PERS-2F906813   \n",
      "4  [{'start': '00:00', 'end': '00:45', 'type': 'M...  PERS-BF70F88C   \n",
      "\n",
      "                                    notes start_time_obj end_time_obj  \\\n",
      "0        Cross-trained personnel required       08:00:00     16:00:00   \n",
      "1                                               16:00:00     00:00:00   \n",
      "2                                               00:00:00     08:00:00   \n",
      "3  Reduced staff, cross-training required       07:00:00     19:00:00   \n",
      "4       No shipping/receiving on weekends       19:00:00     07:00:00   \n",
      "\n",
      "   duration_hours time_category  \n",
      "0             8.0       Morning  \n",
      "1             8.0     Afternoon  \n",
      "2             8.0         Night  \n",
      "3            12.0       Morning  \n",
      "4            12.0       Evening  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta, time\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_facilities_data(facilities_file=\"data/facilities.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated facilities data\n",
    "    \n",
    "    Parameters:\n",
    "    - facilities_file: CSV file containing facilities data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the facilities data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(facilities_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Facilities data file {facilities_file} not found.\")\n",
    "        print(\"Please run the facilities data generation script first.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data if available\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Shifts will be generated with synthetic supervisor IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_shifts_data(facilities_df, personnel_df=None, output_file=\"data/shifts.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the Shifts table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - facilities_df: DataFrame containing facilities data\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated shifts data\n",
    "    \"\"\"\n",
    "    if facilities_df is None or len(facilities_df) == 0:\n",
    "        print(\"Error: No facilities data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Generate supervisor IDs if personnel_df is not provided\n",
    "    if personnel_df is None or len(personnel_df) == 0:\n",
    "        print(\"Generating synthetic supervisor IDs...\")\n",
    "        supervisor_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    else:\n",
    "        # Try to select supervisors or managers if job title info is available\n",
    "        if 'job_title' in personnel_df.columns:\n",
    "            supervisors = personnel_df[personnel_df['job_title'].str.contains('Supervisor|Manager|Lead|Foreman', case=False, na=False)]\n",
    "            if len(supervisors) > 0:\n",
    "                supervisor_ids = supervisors['personnel_id'].tolist()\n",
    "            else:\n",
    "                supervisor_ids = personnel_df['personnel_id'].sample(min(30, len(personnel_df))).tolist()\n",
    "        else:\n",
    "            supervisor_ids = personnel_df['personnel_id'].sample(min(30, len(personnel_df))).tolist()\n",
    "    \n",
    "    # Define common shift patterns\n",
    "    shift_patterns = {\n",
    "        \"Standard\": [\n",
    "            {\"name\": \"Day Shift\", \"start\": \"08:00\", \"end\": \"16:00\", \"breaks\": [{\"start\": \"12:00\", \"end\": \"12:30\", \"type\": \"Lunch\"}]},\n",
    "            {\"name\": \"Evening Shift\", \"start\": \"16:00\", \"end\": \"00:00\", \"breaks\": [{\"start\": \"20:00\", \"end\": \"20:30\", \"type\": \"Dinner\"}]},\n",
    "            {\"name\": \"Night Shift\", \"start\": \"00:00\", \"end\": \"08:00\", \"breaks\": [{\"start\": \"04:00\", \"end\": \"04:30\", \"type\": \"Meal\"}]}\n",
    "        ],\n",
    "        \"Manufacturing\": [\n",
    "            {\"name\": \"First Shift\", \"start\": \"06:00\", \"end\": \"14:00\", \"breaks\": [{\"start\": \"10:00\", \"end\": \"10:15\", \"type\": \"Break\"}, {\"start\": \"12:00\", \"end\": \"12:30\", \"type\": \"Lunch\"}]},\n",
    "            {\"name\": \"Second Shift\", \"start\": \"14:00\", \"end\": \"22:00\", \"breaks\": [{\"start\": \"18:00\", \"end\": \"18:15\", \"type\": \"Break\"}, {\"start\": \"19:00\", \"end\": \"19:30\", \"type\": \"Dinner\"}]},\n",
    "            {\"name\": \"Third Shift\", \"start\": \"22:00\", \"end\": \"06:00\", \"breaks\": [{\"start\": \"02:00\", \"end\": \"02:15\", \"type\": \"Break\"}, {\"start\": \"03:00\", \"end\": \"03:30\", \"type\": \"Meal\"}]}\n",
    "        ],\n",
    "        \"Distribution\": [\n",
    "            {\"name\": \"Morning Shift\", \"start\": \"07:00\", \"end\": \"15:30\", \"breaks\": [{\"start\": \"10:00\", \"end\": \"10:15\", \"type\": \"Break\"}, {\"start\": \"12:00\", \"end\": \"12:30\", \"type\": \"Lunch\"}]},\n",
    "            {\"name\": \"Afternoon Shift\", \"start\": \"15:00\", \"end\": \"23:30\", \"breaks\": [{\"start\": \"18:00\", \"end\": \"18:15\", \"type\": \"Break\"}, {\"start\": \"20:00\", \"end\": \"20:30\", \"type\": \"Dinner\"}]},\n",
    "            {\"name\": \"Overnight Shift\", \"start\": \"23:00\", \"end\": \"07:30\", \"breaks\": [{\"start\": \"02:00\", \"end\": \"02:15\", \"type\": \"Break\"}, {\"start\": \"04:00\", \"end\": \"04:30\", \"type\": \"Meal\"}]}\n",
    "        ],\n",
    "        \"Office\": [\n",
    "            {\"name\": \"Business Hours\", \"start\": \"09:00\", \"end\": \"17:00\", \"breaks\": [{\"start\": \"12:00\", \"end\": \"13:00\", \"type\": \"Lunch\"}]},\n",
    "            {\"name\": \"Extended Hours\", \"start\": \"08:00\", \"end\": \"18:00\", \"breaks\": [{\"start\": \"12:30\", \"end\": \"13:30\", \"type\": \"Lunch\"}]}\n",
    "        ],\n",
    "        \"Continuous\": [\n",
    "            {\"name\": \"A Shift\", \"start\": \"06:00\", \"end\": \"18:00\", \"breaks\": [{\"start\": \"10:00\", \"end\": \"10:15\", \"type\": \"Break\"}, {\"start\": \"14:00\", \"end\": \"14:30\", \"type\": \"Meal\"}]},\n",
    "            {\"name\": \"B Shift\", \"start\": \"18:00\", \"end\": \"06:00\", \"breaks\": [{\"start\": \"22:00\", \"end\": \"22:15\", \"type\": \"Break\"}, {\"start\": \"02:00\", \"end\": \"02:30\", \"type\": \"Meal\"}]}\n",
    "        ],\n",
    "        \"Weekend\": [\n",
    "            {\"name\": \"Weekend Day\", \"start\": \"07:00\", \"end\": \"19:00\", \"breaks\": [{\"start\": \"12:00\", \"end\": \"12:45\", \"type\": \"Lunch\"}]},\n",
    "            {\"name\": \"Weekend Night\", \"start\": \"19:00\", \"end\": \"07:00\", \"breaks\": [{\"start\": \"00:00\", \"end\": \"00:45\", \"type\": \"Meal\"}]}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"shift_id\": [],\n",
    "        \"shift_name\": [],\n",
    "        \"facility_id\": [],\n",
    "        \"start_time\": [],\n",
    "        \"end_time\": [],\n",
    "        \"break_periods\": [],\n",
    "        \"supervisor_id\": [],\n",
    "        \"notes\": []\n",
    "    }\n",
    "    \n",
    "    # Determine which shift patterns to use for each facility\n",
    "    facility_shift_patterns = {}\n",
    "    \n",
    "    for _, facility in facilities_df.iterrows():\n",
    "        facility_id = facility['facility_id']\n",
    "        \n",
    "        # Determine appropriate shift pattern based on facility type\n",
    "        if 'facility_type' in facility:\n",
    "            if facility['facility_type'] == 'Manufacturing Plant':\n",
    "                if 'operating_hours' in facility and '24/7' in str(facility['operating_hours']):\n",
    "                    pattern_type = random.choice([\"Manufacturing\", \"Continuous\"])\n",
    "                else:\n",
    "                    pattern_type = \"Manufacturing\"\n",
    "            elif facility['facility_type'] in ['Warehouse', 'Distribution Center']:\n",
    "                if 'operating_hours' in facility and '24/7' in str(facility['operating_hours']):\n",
    "                    pattern_type = random.choice([\"Distribution\", \"Continuous\"])\n",
    "                else:\n",
    "                    pattern_type = \"Distribution\"\n",
    "            elif facility['facility_type'] == 'R&D Center':\n",
    "                pattern_type = random.choice([\"Standard\", \"Office\"])\n",
    "            else:  # Administrative Office\n",
    "                pattern_type = \"Office\"\n",
    "        else:\n",
    "            # Default if facility type not available\n",
    "            pattern_type = random.choice(list(shift_patterns.keys()))\n",
    "        \n",
    "        # Store the selected pattern type\n",
    "        facility_shift_patterns[facility_id] = pattern_type\n",
    "        \n",
    "        # Determine number of shifts for this facility\n",
    "        if pattern_type == \"Office\":\n",
    "            num_shifts = random.randint(1, 2)  # Offices usually have 1-2 shifts\n",
    "        elif pattern_type == \"Continuous\":\n",
    "            num_shifts = 2  # Continuous operations usually have 2 shifts\n",
    "        else:\n",
    "            num_shifts = len(shift_patterns[pattern_type])\n",
    "            \n",
    "        # Add weekend shifts?\n",
    "        add_weekend = random.random() < 0.4  # 40% chance of weekend shifts\n",
    "        \n",
    "        # Generate shifts for this facility\n",
    "        for i in range(num_shifts):\n",
    "            # Create unique shift ID\n",
    "            shift_id = f\"SHIFT-{uuid.uuid4().hex[:8].upper()}\"\n",
    "            data[\"shift_id\"].append(shift_id)\n",
    "            \n",
    "            # Set facility ID\n",
    "            data[\"facility_id\"].append(facility_id)\n",
    "            \n",
    "            # Get shift template\n",
    "            shift_template = shift_patterns[pattern_type][i % len(shift_patterns[pattern_type])]\n",
    "            \n",
    "            # Set shift name (append facility name or code for uniqueness)\n",
    "            if 'facility_name' in facility:\n",
    "                facility_code = facility['facility_name'].split()[0]  # Use first word of facility name\n",
    "            else:\n",
    "                facility_code = facility_id[-4:]  # Use last 4 chars of ID\n",
    "                \n",
    "            shift_name = f\"{shift_template['name']} ({facility_code})\"\n",
    "            data[\"shift_name\"].append(shift_name)\n",
    "            \n",
    "            # Set start and end times\n",
    "            data[\"start_time\"].append(shift_template['start'])\n",
    "            data[\"end_time\"].append(shift_template['end'])\n",
    "            \n",
    "            # Set break periods as JSON string\n",
    "            data[\"break_periods\"].append(str(shift_template['breaks']))\n",
    "            \n",
    "            # Assign supervisor\n",
    "            data[\"supervisor_id\"].append(random.choice(supervisor_ids))\n",
    "            \n",
    "            # Set notes\n",
    "            notes_options = [\n",
    "                \"\",  # Empty notes most common\n",
    "                \"Cross-trained personnel required\",\n",
    "                \"Heavy machinery operation certification needed\",\n",
    "                \"Quality inspection responsibilities\",\n",
    "                \"Maintenance activities scheduled during this shift\",\n",
    "                \"Handover procedures are critical\",\n",
    "                \"Specialized training required\",\n",
    "                \"High volume production period\",\n",
    "                \"Security escort needed for sensitive areas\"\n",
    "            ]\n",
    "            notes_weights = [0.7, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]  # 70% empty notes\n",
    "            data[\"notes\"].append(random.choices(notes_options, weights=notes_weights)[0])\n",
    "        \n",
    "        # Add weekend shifts if applicable\n",
    "        if add_weekend:\n",
    "            for i in range(min(2, len(shift_patterns[\"Weekend\"]))):\n",
    "                # Create unique shift ID\n",
    "                shift_id = f\"SHIFT-{uuid.uuid4().hex[:8].upper()}\"\n",
    "                data[\"shift_id\"].append(shift_id)\n",
    "                \n",
    "                # Set facility ID\n",
    "                data[\"facility_id\"].append(facility_id)\n",
    "                \n",
    "                # Get shift template\n",
    "                shift_template = shift_patterns[\"Weekend\"][i]\n",
    "                \n",
    "                # Set shift name\n",
    "                if 'facility_name' in facility:\n",
    "                    facility_code = facility['facility_name'].split()[0]\n",
    "                else:\n",
    "                    facility_code = facility_id[-4:]\n",
    "                    \n",
    "                shift_name = f\"{shift_template['name']} ({facility_code})\"\n",
    "                data[\"shift_name\"].append(shift_name)\n",
    "                \n",
    "                # Set start and end times\n",
    "                data[\"start_time\"].append(shift_template['start'])\n",
    "                data[\"end_time\"].append(shift_template['end'])\n",
    "                \n",
    "                # Set break periods as JSON string\n",
    "                data[\"break_periods\"].append(str(shift_template['breaks']))\n",
    "                \n",
    "                # Assign supervisor\n",
    "                data[\"supervisor_id\"].append(random.choice(supervisor_ids))\n",
    "                \n",
    "                # Set weekend-specific notes\n",
    "                weekend_notes = [\n",
    "                    \"Weekend coverage\",\n",
    "                    \"Reduced staff, cross-training required\",\n",
    "                    \"Limited support services available\",\n",
    "                    \"On-call maintenance only\",\n",
    "                    \"No shipping/receiving on weekends\",\n",
    "                    \"Security measures heightened on weekends\"\n",
    "                ]\n",
    "                data[\"notes\"].append(random.choice(weekend_notes))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} shift records across {len(facilities_df)} facilities.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(shifts_df, facilities_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated shifts data\n",
    "    \n",
    "    Parameters:\n",
    "    - shifts_df: DataFrame containing shifts data\n",
    "    - facilities_df: DataFrame containing facilities data (optional)\n",
    "    \"\"\"\n",
    "    if shifts_df is None or len(shifts_df) == 0:\n",
    "        print(\"No shifts data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nShifts Statistics:\")\n",
    "    print(f\"Total shifts: {len(shifts_df)}\")\n",
    "    \n",
    "    # Convert time strings to datetime.time objects for analysis\n",
    "    shifts_df['start_time_obj'] = shifts_df['start_time'].apply(\n",
    "        lambda x: datetime.strptime(x, '%H:%M').time() if isinstance(x, str) else None\n",
    "    )\n",
    "    shifts_df['end_time_obj'] = shifts_df['end_time'].apply(\n",
    "        lambda x: datetime.strptime(x, '%H:%M').time() if isinstance(x, str) else None\n",
    "    )\n",
    "    \n",
    "    # Calculate shift duration\n",
    "    def calculate_duration(start, end):\n",
    "        if start is None or end is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert to datetime for calculation\n",
    "        start_dt = datetime.combine(datetime.today().date(), start)\n",
    "        end_dt = datetime.combine(datetime.today().date(), end)\n",
    "        \n",
    "        # Adjust for overnight shifts\n",
    "        if end <= start:\n",
    "            end_dt += timedelta(days=1)\n",
    "            \n",
    "        duration = end_dt - start_dt\n",
    "        return duration.total_seconds() / 3600  # Return hours\n",
    "    \n",
    "    shifts_df['duration_hours'] = shifts_df.apply(\n",
    "        lambda row: calculate_duration(row['start_time_obj'], row['end_time_obj']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Shift duration statistics\n",
    "    print(\"\\nShift Duration Statistics:\")\n",
    "    print(f\"  Average shift duration: {shifts_df['duration_hours'].mean():.1f} hours\")\n",
    "    print(f\"  Minimum shift duration: {shifts_df['duration_hours'].min():.1f} hours\")\n",
    "    print(f\"  Maximum shift duration: {shifts_df['duration_hours'].max():.1f} hours\")\n",
    "    \n",
    "    # Shift distribution by time of day\n",
    "    def get_time_category(start_time):\n",
    "        if start_time is None:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        hour = start_time.hour\n",
    "        if 5 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif 12 <= hour < 17:\n",
    "            return \"Afternoon\"\n",
    "        elif 17 <= hour < 22:\n",
    "            return \"Evening\"\n",
    "        else:\n",
    "            return \"Night\"\n",
    "    \n",
    "    shifts_df['time_category'] = shifts_df['start_time_obj'].apply(get_time_category)\n",
    "    \n",
    "    print(\"\\nShift Distribution by Time of Day:\")\n",
    "    time_counts = shifts_df['time_category'].value_counts()\n",
    "    for category, count in time_counts.items():\n",
    "        print(f\"  {category}: {count} shifts ({count/len(shifts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Shift count by facility\n",
    "    facility_counts = shifts_df.groupby('facility_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nShifts by Facility:\")\n",
    "    if facilities_df is not None:\n",
    "        # Create a mapping of facility_id to facility_name\n",
    "        facility_names = dict(zip(facilities_df['facility_id'], facilities_df['facility_name']))\n",
    "        \n",
    "        for facility_id, count in facility_counts.items():\n",
    "            facility_name = facility_names.get(facility_id, facility_id)\n",
    "            print(f\"  {facility_name}: {count} shifts ({count/len(shifts_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for facility_id, count in facility_counts.items():\n",
    "            print(f\"  {facility_id}: {count} shifts ({count/len(shifts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Break period analysis\n",
    "    print(\"\\nBreak Period Analysis:\")\n",
    "    \n",
    "    # Count shifts with break periods\n",
    "    has_breaks = shifts_df['break_periods'].apply(lambda x: x != \"[]\" and x != \"\").sum()\n",
    "    print(f\"  Shifts with breaks: {has_breaks} ({has_breaks/len(shifts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Popular shift names\n",
    "    print(\"\\nCommon Shift Names:\")\n",
    "    name_pattern_counts = shifts_df['shift_name'].apply(lambda x: x.split('(')[0].strip()).value_counts().head(5)\n",
    "    for pattern, count in name_pattern_counts.items():\n",
    "        print(f\"  {pattern}: {count} shifts ({count/len(shifts_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for 24-hour coverage\n",
    "    print(\"\\n24-Hour Coverage Analysis:\")\n",
    "    \n",
    "    # Group by facility and check if shifts cover a full 24 hours\n",
    "    # This is a simplistic approach; a more thorough analysis would check for overlaps and gaps\n",
    "    facility_coverage = {}\n",
    "    \n",
    "    for facility_id in shifts_df['facility_id'].unique():\n",
    "        facility_shifts = shifts_df[shifts_df['facility_id'] == facility_id]\n",
    "        \n",
    "        # Check if there are night shifts (starting after 20:00 or before 06:00)\n",
    "        has_night = facility_shifts['start_time_obj'].apply(\n",
    "            lambda x: x.hour >= 20 or x.hour < 6 if x is not None else False\n",
    "        ).any()\n",
    "        \n",
    "        # Check if there are day shifts (starting between 06:00 and 12:00)\n",
    "        has_day = facility_shifts['start_time_obj'].apply(\n",
    "            lambda x: 6 <= x.hour < 12 if x is not None else False\n",
    "        ).any()\n",
    "        \n",
    "        # Check if there are afternoon/evening shifts (starting between 12:00 and 20:00)\n",
    "        has_evening = facility_shifts['start_time_obj'].apply(\n",
    "            lambda x: 12 <= x.hour < 20 if x is not None else False\n",
    "        ).any()\n",
    "        \n",
    "        # Determine coverage\n",
    "        if has_night and has_day and has_evening:\n",
    "            coverage = \"Full 24-hour coverage likely\"\n",
    "        elif has_night and has_day:\n",
    "            coverage = \"Partial coverage (missing afternoon/evening)\"\n",
    "        elif has_night and has_evening:\n",
    "            coverage = \"Partial coverage (missing morning/day)\"\n",
    "        elif has_day and has_evening:\n",
    "            coverage = \"Partial coverage (missing night)\"\n",
    "        elif has_night:\n",
    "            coverage = \"Night coverage only\"\n",
    "        elif has_day:\n",
    "            coverage = \"Day coverage only\"\n",
    "        elif has_evening:\n",
    "            coverage = \"Afternoon/evening coverage only\"\n",
    "        else:\n",
    "            coverage = \"Unknown coverage\"\n",
    "            \n",
    "        facility_coverage[facility_id] = coverage\n",
    "    \n",
    "    coverage_counts = pd.Series(facility_coverage).value_counts()\n",
    "    for coverage, count in coverage_counts.items():\n",
    "        print(f\"  {coverage}: {count} facilities ({count/len(facility_coverage)*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    facilities_df = load_facilities_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    \n",
    "    if facilities_df is not None:\n",
    "        # Generate shifts data\n",
    "        shifts_df = generate_shifts_data(\n",
    "            facilities_df,\n",
    "            personnel_df,\n",
    "            output_file=\"data/shifts.csv\"\n",
    "        )\n",
    "        \n",
    "        # Display statistics\n",
    "        if shifts_df is not None:\n",
    "            display_statistics(shifts_df, facilities_df)\n",
    "            \n",
    "            # Display sample data\n",
    "            print(\"\\nSample shifts data (first 5 records):\")\n",
    "            print(shifts_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0628bd",
   "metadata": {},
   "source": [
    "Inventory Transactions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c372185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Personnel data file data/personnel.csv not found.\n",
      "Inventory transactions will be generated with synthetic operator IDs.\n",
      "Generating synthetic operator IDs...\n",
      "Successfully generated 1000 inventory transaction records.\n",
      "Data saved to data/inventory_transactions.csv\n",
      "\n",
      "Inventory Transactions Statistics:\n",
      "Total transactions: 1000\n",
      "\n",
      "Transaction Type Distribution:\n",
      "  Issue: 321 (32.1%)\n",
      "  Transfer: 245 (24.5%)\n",
      "  Receipt: 196 (19.6%)\n",
      "  Scrap: 55 (5.5%)\n",
      "  Return: 51 (5.1%)\n",
      "  Quality Hold: 46 (4.6%)\n",
      "  Release: 44 (4.4%)\n",
      "  Adjustment: 42 (4.2%)\n",
      "\n",
      "Transaction Volume Statistics:\n",
      "  Positive transactions: 979 (97.9%)\n",
      "  Average positive quantity: 177.82\n",
      "  Total positive quantity: 174082.17\n",
      "  Negative transactions: 21 (2.1%)\n",
      "  Average negative quantity: -22.83\n",
      "  Total negative quantity: -479.52\n",
      "\n",
      "Transaction Timeline:\n",
      "  First transaction: 2024-07-16 14:35:47\n",
      "  Last transaction: 2025-07-16 04:44:39\n",
      "\n",
      "Transactions by Month (latest 6 months):\n",
      "  2025-02: 61 transactions\n",
      "  2025-03: 85 transactions\n",
      "  2025-04: 87 transactions\n",
      "  2025-05: 90 transactions\n",
      "  2025-06: 84 transactions\n",
      "  2025-07: 46 transactions\n",
      "\n",
      "Top 10 Materials by Transaction Count:\n",
      "  Insert ACS 537: 14 transactions (1.4%)\n",
      "  Glass Ultra 708: 13 transactions (1.3%)\n",
      "  Container USP 517: 12 transactions (1.2%)\n",
      "  Cap Technical 653: 12 transactions (1.2%)\n",
      "  Cap Ultra 166: 12 transactions (1.2%)\n",
      "  Container BP 744: 11 transactions (1.1%)\n",
      "  Box ACS 169: 11 transactions (1.1%)\n",
      "  Film ACS 622: 11 transactions (1.1%)\n",
      "  Coloring BP 741: 11 transactions (1.1%)\n",
      "  Glass Ultra 853: 11 transactions (1.1%)\n",
      "\n",
      "Top 10 Locations by Transaction Activity:\n",
      "  Building 4 - Shelf - Shelf 659: 11.0 transactions\n",
      "  Building C - Bulk Storage - Rack 47 - Sub-Location 19: 10.0 transactions\n",
      "  Building C - Hazardous Material - Sub-Location 73: 10.0 transactions\n",
      "  Area 4 - Cold Storage - Cold Storage Section 05 - Sub-Location 18: 9.0 transactions\n",
      "  Wing 2 - Freezer - Freezer Section 04: 8.0 transactions\n",
      "  Damaged Goods Building 2: 8.0 transactions\n",
      "  Building 2 - Bulk Storage - Zone 23 - Sub-Location 88 - Sub-Location 33: 8.0 transactions\n",
      "  Wing 3 - Staging Area - Sub-Location 93 - Sub-Location 94 - Sub-Location 29: 7.0 transactions\n",
      "  Outbound Queue Area 2: 7.0 transactions\n",
      "  Production Area 1 - Quality Control Hold: 7.0 transactions\n",
      "\n",
      "Top 10 Transaction Reasons:\n",
      "  Move to Production: 74 (7.4%)\n",
      "  Production Issue: 74 (7.4%)\n",
      "  Transfer Out: 65 (6.5%)\n",
      "  R&D Use: 64 (6.4%)\n",
      "  Sample Issue: 63 (6.3%)\n",
      "  Storage Consolidation: 60 (6.0%)\n",
      "  Staging for Shipping: 58 (5.8%)\n",
      "  Transfer In: 55 (5.5%)\n",
      "  QC Testing: 55 (5.5%)\n",
      "  Inventory Optimization: 53 (5.3%)\n",
      "\n",
      "Transactions with work order reference: 292 (29.2%)\n",
      "Transactions with reference document: 787 (78.7%)\n",
      "\n",
      "Cost Statistics:\n",
      "  Transactions with cost data: 289 (28.9%)\n",
      "  Average unit cost: $256.20\n",
      "  Minimum unit cost: $7.32\n",
      "  Maximum unit cost: $499.89\n",
      "  Total transaction value: $31976492.96\n",
      "\n",
      "Sample inventory transactions data (first 5 records):\n",
      "  transaction_id transaction_type   material_id        lot_id  \\\n",
      "0   TRX-912A8019         Transfer  MAT-DF3FB384  LOT-20BEA97C   \n",
      "1   TRX-D38DCF29            Scrap  MAT-2EB4F262  LOT-C3F1C8E1   \n",
      "2   TRX-AF348F08         Transfer  MAT-0A43BC98  LOT-A73A13DF   \n",
      "3   TRX-EC96C805            Issue  MAT-24E08AED  LOT-23D9A1BE   \n",
      "4   TRX-81EFDC25         Transfer  MAT-1A47D45B  LOT-71732351   \n",
      "\n",
      "            timestamp  quantity from_location_id to_location_id work_order_id  \\\n",
      "0 2024-07-16 14:35:47      7.74     LOC-E7376447   LOC-6E23971B                 \n",
      "1 2024-07-16 21:17:58     90.12     LOC-5690D79E                                \n",
      "2 2024-07-16 22:32:56     13.93     LOC-4E885A11   LOC-C8239C0C                 \n",
      "3 2024-07-17 05:05:35     47.95     LOC-D04DF74B                  WO-96E448FA   \n",
      "4 2024-07-17 10:14:16     30.97     LOC-5641CB40   LOC-25888941                 \n",
      "\n",
      "  reference_document    operator_id      transaction_reason unit_cost  \\\n",
      "0                     PERS-94BC4BA1      Move to Production             \n",
      "1                     PERS-94849902                Obsolete             \n",
      "2                     PERS-DE7B38DE  Inventory Optimization             \n",
      "3     WO:WO-534D2191  PERS-9EF23D10                 R&D Use             \n",
      "4                     PERS-9EF23D10   Storage Consolidation             \n",
      "\n",
      "   quantity_numeric    month  unit_cost_numeric  \n",
      "0              7.74  2024-07                NaN  \n",
      "1             90.12  2024-07                NaN  \n",
      "2             13.93  2024-07                NaN  \n",
      "3             47.95  2024-07                NaN  \n",
      "4             30.97  2024-07                NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2065/1821377008.py:598: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cost_transactions['transaction_value'] = cost_transactions['quantity_numeric'] * cost_transactions['unit_cost_numeric']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_materials_data(materials_file=\"data/materials.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated materials data\n",
    "    \n",
    "    Parameters:\n",
    "    - materials_file: CSV file containing materials data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the materials data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(materials_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Materials data file {materials_file} not found.\")\n",
    "        print(\"Inventory transactions will be generated with synthetic material IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_storage_locations_data(storage_locations_file=\"data/storage_locations.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated storage locations data\n",
    "    \n",
    "    Parameters:\n",
    "    - storage_locations_file: CSV file containing storage locations data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the storage locations data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(storage_locations_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Storage locations data file {storage_locations_file} not found.\")\n",
    "        print(\"Inventory transactions will be generated with synthetic location IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_work_orders_data(work_orders_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated work orders data\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_file: CSV file containing work orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the work orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(work_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Work orders data file {work_orders_file} not found.\")\n",
    "        print(\"Inventory transactions will be generated with synthetic work order IDs where needed.\")\n",
    "        return None\n",
    "\n",
    "def load_purchase_orders_data(purchase_orders_file=\"data/purchase_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated purchase orders data\n",
    "    \n",
    "    Parameters:\n",
    "    - purchase_orders_file: CSV file containing purchase orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the purchase orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(purchase_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Purchase orders data file {purchase_orders_file} not found.\")\n",
    "        print(\"Inventory transactions will be generated with synthetic purchase order IDs where needed.\")\n",
    "        return None\n",
    "\n",
    "def load_material_lots_data(material_lots_file=\"data/material_lots.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated material lots data\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_file: CSV file containing material lots data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the material lots data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(material_lots_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Material lots data file {material_lots_file} not found.\")\n",
    "        print(\"Inventory transactions will be generated with synthetic lot IDs.\")\n",
    "        return None\n",
    "\n",
    "def load_personnel_data(personnel_file=\"data/personnel.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated personnel data\n",
    "    \n",
    "    Parameters:\n",
    "    - personnel_file: CSV file containing personnel data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the personnel data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(personnel_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Personnel data file {personnel_file} not found.\")\n",
    "        print(\"Inventory transactions will be generated with synthetic operator IDs.\")\n",
    "        return None\n",
    "\n",
    "def generate_inventory_transactions(materials_df=None, storage_locations_df=None, \n",
    "                                  work_orders_df=None, purchase_orders_df=None, \n",
    "                                  material_lots_df=None, personnel_df=None,\n",
    "                                  num_transactions=1000, start_time=None, end_time=None,\n",
    "                                  output_file=\"data/inventory_transactions.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the InventoryTransactions table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    - storage_locations_df: DataFrame containing storage locations data (optional)\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - purchase_orders_df: DataFrame containing purchase orders data (optional)\n",
    "    - material_lots_df: DataFrame containing material lots data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_transactions: Number of transaction records to generate\n",
    "    - start_time: Start time for transaction dates (defaults to 365 days ago)\n",
    "    - end_time: End time for transaction dates (defaults to current date)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated inventory transactions data\n",
    "    \"\"\"\n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=365)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Generate material IDs if materials_df is not provided\n",
    "    if materials_df is None or len(materials_df) == 0:\n",
    "        print(\"Generating synthetic material IDs...\")\n",
    "        material_ids = [f\"MAT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    else:\n",
    "        material_ids = materials_df['material_id'].tolist()\n",
    "    \n",
    "    # Generate location IDs if storage_locations_df is not provided\n",
    "    if storage_locations_df is None or len(storage_locations_df) == 0:\n",
    "        print(\"Generating synthetic storage location IDs...\")\n",
    "        location_ids = [f\"LOC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    else:\n",
    "        location_ids = storage_locations_df['location_id'].tolist()\n",
    "    \n",
    "    # Generate lot IDs if material_lots_df is not provided\n",
    "    if material_lots_df is None or len(material_lots_df) == 0:\n",
    "        print(\"Generating synthetic lot IDs...\")\n",
    "        lot_ids = [f\"LOT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(100)]\n",
    "        \n",
    "        # Create a mapping of material_id to lot_ids\n",
    "        material_to_lots = {}\n",
    "        for material_id in material_ids:\n",
    "            # Assign 2-5 lots to each material\n",
    "            num_lots = random.randint(2, 5)\n",
    "            material_to_lots[material_id] = random.sample(lot_ids, min(num_lots, len(lot_ids)))\n",
    "    else:\n",
    "        lot_ids = material_lots_df['lot_id'].tolist()\n",
    "        \n",
    "        # Create a mapping of material_id to lot_ids if possible\n",
    "        material_to_lots = {}\n",
    "        if 'material_id' in material_lots_df.columns:\n",
    "            for material_id in material_ids:\n",
    "                material_lots = material_lots_df[material_lots_df['material_id'] == material_id]\n",
    "                if len(material_lots) > 0:\n",
    "                    material_to_lots[material_id] = material_lots['lot_id'].tolist()\n",
    "                else:\n",
    "                    # Assign random lots if no specific lots found for this material\n",
    "                    num_lots = random.randint(1, 3)\n",
    "                    material_to_lots[material_id] = random.sample(lot_ids, min(num_lots, len(lot_ids)))\n",
    "        else:\n",
    "            # If no material_id in material_lots_df, create random assignments\n",
    "            for material_id in material_ids:\n",
    "                num_lots = random.randint(1, 3)\n",
    "                material_to_lots[material_id] = random.sample(lot_ids, min(num_lots, len(lot_ids)))\n",
    "    \n",
    "    # Generate work order IDs if work_orders_df is not provided\n",
    "    if work_orders_df is None or len(work_orders_df) == 0:\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    else:\n",
    "        work_order_ids = work_orders_df['work_order_id'].tolist()\n",
    "    \n",
    "    # Generate purchase order IDs if purchase_orders_df is not provided\n",
    "    if purchase_orders_df is None or len(purchase_orders_df) == 0:\n",
    "        print(\"Generating synthetic purchase order IDs...\")\n",
    "        purchase_order_ids = [f\"PO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    else:\n",
    "        purchase_order_ids = purchase_orders_df['po_id'].tolist()\n",
    "    \n",
    "    # Generate operator IDs if personnel_df is not provided\n",
    "    if personnel_df is None or len(personnel_df) == 0:\n",
    "        print(\"Generating synthetic operator IDs...\")\n",
    "        operator_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    else:\n",
    "        operator_ids = personnel_df['personnel_id'].tolist()\n",
    "    \n",
    "    # Define transaction types and their probabilities\n",
    "    transaction_types = {\n",
    "        \"Receipt\": 0.2,       # Receiving materials from suppliers\n",
    "        \"Issue\": 0.3,         # Issuing materials to production\n",
    "        \"Transfer\": 0.25,     # Moving materials between locations\n",
    "        \"Adjustment\": 0.05,   # Inventory count adjustments\n",
    "        \"Return\": 0.05,       # Returns to inventory\n",
    "        \"Scrap\": 0.05,        # Scrapping materials\n",
    "        \"Quality Hold\": 0.05, # Placing materials on quality hold\n",
    "        \"Release\": 0.05       # Releasing materials from hold\n",
    "    }\n",
    "    \n",
    "    # Define reason codes by transaction type\n",
    "    reason_codes = {\n",
    "        \"Receipt\": [\"Purchase Order Receipt\", \"Production Return\", \"Transfer In\", \"Inventory Correction\"],\n",
    "        \"Issue\": [\"Production Issue\", \"Transfer Out\", \"Sample Issue\", \"QC Testing\", \"R&D Use\"],\n",
    "        \"Transfer\": [\"Inventory Optimization\", \"Storage Consolidation\", \"Move to Production\", \"Staging for Shipping\"],\n",
    "        \"Adjustment\": [\"Cycle Count\", \"Physical Inventory\", \"Damaged in Storage\", \"Expiration\", \"System Reconciliation\"],\n",
    "        \"Return\": [\"Production Excess\", \"QC Rejection\", \"Customer Return\", \"Unused Material\"],\n",
    "        \"Scrap\": [\"Expired\", \"Damaged\", \"Failed QC\", \"Contaminated\", \"Obsolete\"],\n",
    "        \"Quality Hold\": [\"Out of Specification\", \"Pending Test Results\", \"Supplier Investigation\", \"Process Deviation\"],\n",
    "        \"Release\": [\"QC Approval\", \"Investigation Complete\", \"Deviation Approved\", \"Rework Complete\"]\n",
    "    }\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"transaction_id\": [f\"TRX-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_transactions)],\n",
    "        \"transaction_type\": [],\n",
    "        \"material_id\": [],\n",
    "        \"lot_id\": [],\n",
    "        \"timestamp\": [],\n",
    "        \"quantity\": [],\n",
    "        \"from_location_id\": [],\n",
    "        \"to_location_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"reference_document\": [],\n",
    "        \"operator_id\": [],\n",
    "        \"transaction_reason\": [],\n",
    "        \"unit_cost\": []\n",
    "    }\n",
    "    \n",
    "    # Keep track of material-location inventory for realistic transactions\n",
    "    inventory = {}  # (material_id, location_id, lot_id) -> quantity\n",
    "    \n",
    "    # Generate data for each transaction\n",
    "    for i in range(num_transactions):\n",
    "        # Generate timestamp within the specified range\n",
    "        time_range_seconds = int((end_time - start_time).total_seconds())\n",
    "        random_seconds = random.randint(0, time_range_seconds)\n",
    "        timestamp = start_time + timedelta(seconds=random_seconds)\n",
    "        data[\"timestamp\"].append(timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Select transaction type (weighted random)\n",
    "        transaction_type = random.choices(\n",
    "            list(transaction_types.keys()), \n",
    "            weights=list(transaction_types.values())\n",
    "        )[0]\n",
    "        data[\"transaction_type\"].append(transaction_type)\n",
    "        \n",
    "        # Select material\n",
    "        material_id = random.choice(material_ids)\n",
    "        data[\"material_id\"].append(material_id)\n",
    "        \n",
    "        # Select lot based on material\n",
    "        if material_id in material_to_lots and material_to_lots[material_id]:\n",
    "            lot_id = random.choice(material_to_lots[material_id])\n",
    "        else:\n",
    "            lot_id = random.choice(lot_ids)\n",
    "        data[\"lot_id\"].append(lot_id)\n",
    "        \n",
    "        # Handle from_location and to_location based on transaction type\n",
    "        if transaction_type == \"Receipt\":\n",
    "            # Receipts come from outside (no from_location) to a storage location\n",
    "            from_location_id = \"\"\n",
    "            to_location_id = random.choice(location_ids)\n",
    "            \n",
    "            # Reference document is typically a purchase order\n",
    "            if random.random() < 0.8:  # 80% of receipts have a PO reference\n",
    "                reference_document = f\"PO:{random.choice(purchase_order_ids)}\"\n",
    "            else:\n",
    "                reference_document = \"\"\n",
    "                \n",
    "        elif transaction_type == \"Issue\":\n",
    "            # Issues go from a storage location to production (no to_location)\n",
    "            from_location_id = random.choice(location_ids)\n",
    "            to_location_id = \"\"\n",
    "            \n",
    "            # Reference document is typically a work order\n",
    "            if random.random() < 0.8:  # 80% of issues have a WO reference\n",
    "                reference_document = f\"WO:{random.choice(work_order_ids)}\"\n",
    "            else:\n",
    "                reference_document = \"\"\n",
    "                \n",
    "        elif transaction_type == \"Transfer\":\n",
    "            # Transfers go from one storage location to another\n",
    "            available_locations = list(location_ids)\n",
    "            from_location_id = random.choice(available_locations)\n",
    "            \n",
    "            # Ensure to_location is different from from_location\n",
    "            remaining_locations = [loc for loc in available_locations if loc != from_location_id]\n",
    "            if remaining_locations:\n",
    "                to_location_id = random.choice(remaining_locations)\n",
    "            else:\n",
    "                # If only one location available, use it for both (not ideal but prevents errors)\n",
    "                to_location_id = from_location_id\n",
    "                \n",
    "            # Reference document could be various things\n",
    "            ref_types = [\"\", f\"WO:{random.choice(work_order_ids)}\", \"Transfer Order:TO-\" + str(random.randint(10000, 99999))]\n",
    "            reference_document = random.choice(ref_types)\n",
    "            \n",
    "        elif transaction_type in [\"Adjustment\", \"Scrap\"]:\n",
    "            # Adjustments and scraps occur at a specific location (from_location)\n",
    "            from_location_id = random.choice(location_ids)\n",
    "            to_location_id = \"\"\n",
    "            \n",
    "            # Reference document could be various things\n",
    "            ref_types = [\"\", \"Count Sheet:CS-\" + str(random.randint(10000, 99999)), \"QC Report:QC-\" + str(random.randint(10000, 99999))]\n",
    "            reference_document = random.choice(ref_types)\n",
    "            \n",
    "        elif transaction_type == \"Return\":\n",
    "            # Returns go from production (no from_location) to a storage location\n",
    "            from_location_id = \"\"\n",
    "            to_location_id = random.choice(location_ids)\n",
    "            \n",
    "            # Reference document is typically a work order\n",
    "            if random.random() < 0.7:  # 70% of returns have a WO reference\n",
    "                reference_document = f\"WO:{random.choice(work_order_ids)}\"\n",
    "            else:\n",
    "                reference_document = \"\"\n",
    "                \n",
    "        elif transaction_type == \"Quality Hold\":\n",
    "            # Quality holds change the status of inventory at a location\n",
    "            from_location_id = random.choice(location_ids)\n",
    "            to_location_id = from_location_id  # Same location, just changing status\n",
    "            \n",
    "            # Reference document is typically a QC document\n",
    "            reference_document = \"QC Hold:\" + str(random.randint(10000, 99999))\n",
    "            \n",
    "        else:  # Release\n",
    "            # Releases change the status of inventory at a location\n",
    "            from_location_id = random.choice(location_ids)\n",
    "            to_location_id = from_location_id  # Same location, just changing status\n",
    "            \n",
    "            # Reference document is typically a QC document\n",
    "            reference_document = \"QC Release:\" + str(random.randint(10000, 99999))\n",
    "        \n",
    "        data[\"from_location_id\"].append(from_location_id)\n",
    "        data[\"to_location_id\"].append(to_location_id)\n",
    "        data[\"reference_document\"].append(reference_document)\n",
    "        \n",
    "        # Determine quantity based on transaction type and maintain inventory\n",
    "        inventory_key = (material_id, from_location_id, lot_id)\n",
    "        \n",
    "        if transaction_type in [\"Receipt\", \"Return\"]:\n",
    "            # Incoming transactions can have any quantity\n",
    "            quantity = round(random.uniform(10, 1000), 2)\n",
    "            \n",
    "            # Update inventory\n",
    "            destination_key = (material_id, to_location_id, lot_id)\n",
    "            if destination_key in inventory:\n",
    "                inventory[destination_key] = inventory[destination_key] + quantity\n",
    "            else:\n",
    "                inventory[destination_key] = quantity\n",
    "                \n",
    "        elif transaction_type in [\"Issue\", \"Transfer\", \"Scrap\"]:\n",
    "            # Outgoing transactions need available inventory\n",
    "            if inventory_key in inventory and inventory[inventory_key] > 0:\n",
    "                # Use up to 80% of available inventory\n",
    "                max_quantity = inventory[inventory_key] * 0.8\n",
    "                quantity = round(random.uniform(1, max_quantity), 2)\n",
    "                \n",
    "                # Update inventory at source\n",
    "                inventory[inventory_key] = inventory[inventory_key] - quantity\n",
    "                \n",
    "                # Update inventory at destination if applicable\n",
    "                if transaction_type == \"Transfer\" and to_location_id:\n",
    "                    destination_key = (material_id, to_location_id, lot_id)\n",
    "                    if destination_key in inventory:\n",
    "                        inventory[destination_key] = inventory[destination_key] + quantity\n",
    "                    else:\n",
    "                        inventory[destination_key] = quantity\n",
    "            else:\n",
    "                # No inventory available, create a small quantity\n",
    "                quantity = round(random.uniform(1, 100), 2)\n",
    "                \n",
    "                # Add to inventory first (anachronistic but ensures future transactions have inventory)\n",
    "                if from_location_id:\n",
    "                    inventory[inventory_key] = quantity\n",
    "        \n",
    "        elif transaction_type == \"Adjustment\":\n",
    "            # Adjustments can be positive or negative\n",
    "            if random.random() < 0.5:  # 50% positive adjustments\n",
    "                quantity = round(random.uniform(1, 100), 2)\n",
    "                \n",
    "                # Update inventory\n",
    "                if inventory_key in inventory:\n",
    "                    inventory[inventory_key] = inventory[inventory_key] + quantity\n",
    "                else:\n",
    "                    inventory[inventory_key] = quantity\n",
    "            else:\n",
    "                # Negative adjustment\n",
    "                if inventory_key in inventory and inventory[inventory_key] > 0:\n",
    "                    # Use up to 30% of available inventory\n",
    "                    max_quantity = inventory[inventory_key] * 0.3\n",
    "                    quantity = -round(random.uniform(1, max_quantity), 2)\n",
    "                    \n",
    "                    # Update inventory\n",
    "                    inventory[inventory_key] = inventory[inventory_key] + quantity  # Adding negative\n",
    "                else:\n",
    "                    # No inventory available, create a small negative quantity\n",
    "                    quantity = -round(random.uniform(1, 50), 2)\n",
    "        \n",
    "        else:  # Quality Hold or Release\n",
    "            # These don't change quantity, just status\n",
    "            if inventory_key in inventory and inventory[inventory_key] > 0:\n",
    "                quantity = inventory[inventory_key]  # Use the full amount in inventory\n",
    "            else:\n",
    "                quantity = round(random.uniform(10, 500), 2)  # Create some quantity if none exists\n",
    "        \n",
    "        data[\"quantity\"].append(quantity)\n",
    "        \n",
    "        # Set work order ID (for certain transaction types)\n",
    "        if transaction_type in [\"Issue\", \"Return\"] and random.random() < 0.8:\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        else:\n",
    "            data[\"work_order_id\"].append(\"\")\n",
    "        \n",
    "        # Set operator\n",
    "        data[\"operator_id\"].append(random.choice(operator_ids))\n",
    "        \n",
    "        # Set transaction reason\n",
    "        if transaction_type in reason_codes:\n",
    "            data[\"transaction_reason\"].append(random.choice(reason_codes[transaction_type]))\n",
    "        else:\n",
    "            data[\"transaction_reason\"].append(\"\")\n",
    "        \n",
    "        # Set unit cost (for financial tracking)\n",
    "        if transaction_type in [\"Receipt\", \"Adjustment\", \"Return\"]:\n",
    "            # These transaction types typically record cost\n",
    "            unit_cost = round(random.uniform(5, 500), 2)\n",
    "        else:\n",
    "            # These use the existing cost basis\n",
    "            unit_cost = \"\"\n",
    "            \n",
    "        data[\"unit_cost\"].append(unit_cost)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Sort by timestamp to create a chronological history\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Reset the index after sorting\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} inventory transaction records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_statistics(inventory_transactions_df, materials_df=None, storage_locations_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated inventory transactions data\n",
    "    \n",
    "    Parameters:\n",
    "    - inventory_transactions_df: DataFrame containing inventory transactions data\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    - storage_locations_df: DataFrame containing storage locations data (optional)\n",
    "    \"\"\"\n",
    "    if inventory_transactions_df is None or len(inventory_transactions_df) == 0:\n",
    "        print(\"No inventory transactions data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nInventory Transactions Statistics:\")\n",
    "    print(f\"Total transactions: {len(inventory_transactions_df)}\")\n",
    "    \n",
    "    # Transaction type distribution\n",
    "    print(\"\\nTransaction Type Distribution:\")\n",
    "    type_counts = inventory_transactions_df['transaction_type'].value_counts()\n",
    "    for trx_type, count in type_counts.items():\n",
    "        print(f\"  {trx_type}: {count} ({count/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Transaction volume statistics\n",
    "    print(\"\\nTransaction Volume Statistics:\")\n",
    "    \n",
    "    # Convert quantity to numeric, handling any non-numeric values\n",
    "    inventory_transactions_df['quantity_numeric'] = pd.to_numeric(\n",
    "        inventory_transactions_df['quantity'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    positive_transactions = inventory_transactions_df[inventory_transactions_df['quantity_numeric'] > 0]\n",
    "    negative_transactions = inventory_transactions_df[inventory_transactions_df['quantity_numeric'] < 0]\n",
    "    \n",
    "    print(f\"  Positive transactions: {len(positive_transactions)} ({len(positive_transactions)/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    if len(positive_transactions) > 0:\n",
    "        print(f\"  Average positive quantity: {positive_transactions['quantity_numeric'].mean():.2f}\")\n",
    "        print(f\"  Total positive quantity: {positive_transactions['quantity_numeric'].sum():.2f}\")\n",
    "        \n",
    "    print(f\"  Negative transactions: {len(negative_transactions)} ({len(negative_transactions)/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    if len(negative_transactions) > 0:\n",
    "        print(f\"  Average negative quantity: {negative_transactions['quantity_numeric'].mean():.2f}\")\n",
    "        print(f\"  Total negative quantity: {negative_transactions['quantity_numeric'].sum():.2f}\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    inventory_transactions_df['timestamp'] = pd.to_datetime(inventory_transactions_df['timestamp'])\n",
    "    \n",
    "    print(\"\\nTransaction Timeline:\")\n",
    "    print(f\"  First transaction: {inventory_transactions_df['timestamp'].min()}\")\n",
    "    print(f\"  Last transaction: {inventory_transactions_df['timestamp'].max()}\")\n",
    "    \n",
    "    # Transactions by month\n",
    "    inventory_transactions_df['month'] = inventory_transactions_df['timestamp'].dt.to_period('M')\n",
    "    monthly_transactions = inventory_transactions_df.groupby('month').size()\n",
    "    \n",
    "    print(\"\\nTransactions by Month (latest 6 months):\")\n",
    "    for month, count in monthly_transactions.tail(6).items():\n",
    "        print(f\"  {month}: {count} transactions\")\n",
    "    \n",
    "    # Material analysis\n",
    "    material_counts = inventory_transactions_df.groupby('material_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Materials by Transaction Count:\")\n",
    "    if materials_df is not None:\n",
    "        # Create a mapping of material_id to material_name\n",
    "        material_names = dict(zip(materials_df['material_id'], materials_df['material_name']))\n",
    "        \n",
    "        for material_id, count in material_counts.head(10).items():\n",
    "            material_name = material_names.get(material_id, material_id)\n",
    "            print(f\"  {material_name}: {count} transactions ({count/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for material_id, count in material_counts.head(10).items():\n",
    "            print(f\"  {material_id}: {count} transactions ({count/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Location analysis\n",
    "    # Combine from and to locations for overall location activity\n",
    "    from_locations = inventory_transactions_df[inventory_transactions_df['from_location_id'] != \"\"]\n",
    "    to_locations = inventory_transactions_df[inventory_transactions_df['to_location_id'] != \"\"]\n",
    "    \n",
    "    from_counts = from_locations.groupby('from_location_id').size()\n",
    "    to_counts = to_locations.groupby('to_location_id').size()\n",
    "    \n",
    "    # Combine the counts\n",
    "    location_activity = pd.Series(0, index=set(from_counts.index) | set(to_counts.index))\n",
    "    location_activity = location_activity.add(from_counts, fill_value=0).add(to_counts, fill_value=0)\n",
    "    location_activity = location_activity.sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Locations by Transaction Activity:\")\n",
    "    if storage_locations_df is not None:\n",
    "        # Create a mapping of location_id to location_name\n",
    "        location_names = dict(zip(storage_locations_df['location_id'], storage_locations_df['location_name']))\n",
    "        \n",
    "        for location_id, count in location_activity.head(10).items():\n",
    "            location_name = location_names.get(location_id, location_id)\n",
    "            print(f\"  {location_name}: {count} transactions\")\n",
    "    else:\n",
    "        for location_id, count in location_activity.head(10).items():\n",
    "            print(f\"  {location_id}: {count} transactions\")\n",
    "    \n",
    "    # Reason code analysis\n",
    "    print(\"\\nTop 10 Transaction Reasons:\")\n",
    "    reason_counts = inventory_transactions_df['transaction_reason'].value_counts().head(10)\n",
    "    for reason, count in reason_counts.items():\n",
    "        print(f\"  {reason}: {count} ({count/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Work order reference analysis\n",
    "    has_wo = inventory_transactions_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nTransactions with work order reference: {has_wo} ({has_wo/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Reference document analysis\n",
    "    has_ref = inventory_transactions_df['reference_document'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"Transactions with reference document: {has_ref} ({has_ref/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Cost analysis\n",
    "    inventory_transactions_df['unit_cost_numeric'] = pd.to_numeric(\n",
    "        inventory_transactions_df['unit_cost'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    cost_transactions = inventory_transactions_df[pd.notna(inventory_transactions_df['unit_cost_numeric'])]\n",
    "    if len(cost_transactions) > 0:\n",
    "        print(\"\\nCost Statistics:\")\n",
    "        print(f\"  Transactions with cost data: {len(cost_transactions)} ({len(cost_transactions)/len(inventory_transactions_df)*100:.1f}%)\")\n",
    "        print(f\"  Average unit cost: ${cost_transactions['unit_cost_numeric'].mean():.2f}\")\n",
    "        print(f\"  Minimum unit cost: ${cost_transactions['unit_cost_numeric'].min():.2f}\")\n",
    "        print(f\"  Maximum unit cost: ${cost_transactions['unit_cost_numeric'].max():.2f}\")\n",
    "        \n",
    "        # Calculate total value movement\n",
    "        cost_transactions['transaction_value'] = cost_transactions['quantity_numeric'] * cost_transactions['unit_cost_numeric']\n",
    "        print(f\"  Total transaction value: ${cost_transactions['transaction_value'].sum():.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load required data\n",
    "    materials_df = load_materials_data()\n",
    "    storage_locations_df = load_storage_locations_data()\n",
    "    work_orders_df = load_work_orders_data()\n",
    "    purchase_orders_df = load_purchase_orders_data()\n",
    "    material_lots_df = load_material_lots_data()\n",
    "    personnel_df = load_personnel_data()\n",
    "    \n",
    "    # Generate inventory transactions data\n",
    "    inventory_transactions_df = generate_inventory_transactions(\n",
    "        materials_df,\n",
    "        storage_locations_df,\n",
    "        work_orders_df,\n",
    "        purchase_orders_df,\n",
    "        material_lots_df,\n",
    "        personnel_df,\n",
    "        num_transactions=1000,  # Generate 1000 transaction records\n",
    "        output_file=\"data/inventory_transactions.csv\"\n",
    "    )\n",
    "    \n",
    "    # Display statistics\n",
    "    if inventory_transactions_df is not None:\n",
    "        display_statistics(inventory_transactions_df, materials_df, storage_locations_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample inventory transactions data (first 5 records):\")\n",
    "        print(inventory_transactions_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb48467",
   "metadata": {},
   "source": [
    "Pending Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0077f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing materials data.\n",
      "Loaded existing products data.\n",
      "Loaded existing facilities data.\n",
      "Loaded existing storage locations data.\n",
      "Loaded existing equipment data.\n",
      "Loaded existing work orders data.\n",
      "Loaded existing batches data.\n",
      "Loaded existing suppliers data.\n",
      "Personnel data not found.\n",
      "\n",
      "1. Generating Material Lots data...\n",
      "Successfully generated 200 material lot records.\n",
      "Data saved to data/material_lots.csv\n",
      "\n",
      "Material Lots Statistics:\n",
      "Total material lots: 200\n",
      "\n",
      "Lot Status Distribution:\n",
      "  Available: 135 (67.5%)\n",
      "  In Use: 22 (11.0%)\n",
      "  Reserved: 19 (9.5%)\n",
      "  Quarantined: 9 (4.5%)\n",
      "  Consumed: 6 (3.0%)\n",
      "  Rejected: 5 (2.5%)\n",
      "  On Hold: 4 (2.0%)\n",
      "\n",
      "Quality Status Distribution:\n",
      "  Released: 137 (68.5%)\n",
      "  Under Test: 19 (9.5%)\n",
      "  Pending Review: 16 (8.0%)\n",
      "  Rejected: 15 (7.5%)\n",
      "  Approved: 13 (6.5%)\n",
      "\n",
      "Quantity Statistics by Unit:\n",
      "  unit:\n",
      "    Count: 21 lots\n",
      "    Total quantity: 20,904.21 unit\n",
      "    Average quantity: 995.44 unit\n",
      "  m³:\n",
      "    Count: 15 lots\n",
      "    Total quantity: 15,965.64 m³\n",
      "    Average quantity: 1,064.38 m³\n",
      "  L:\n",
      "    Count: 33 lots\n",
      "    Total quantity: 16,799.35 L\n",
      "    Average quantity: 509.07 L\n",
      "  batch:\n",
      "    Count: 18 lots\n",
      "    Total quantity: 5,379.37 batch\n",
      "    Average quantity: 298.85 batch\n",
      "  ton:\n",
      "    Count: 24 lots\n",
      "    Total quantity: 22,097.03 ton\n",
      "    Average quantity: 920.71 ton\n",
      "  kg:\n",
      "    Count: 32 lots\n",
      "    Total quantity: 17,056.24 kg\n",
      "    Average quantity: 533.01 kg\n",
      "  roll:\n",
      "    Count: 11 lots\n",
      "    Total quantity: 69,854.91 roll\n",
      "    Average quantity: 6,350.45 roll\n",
      "  piece:\n",
      "    Count: 14 lots\n",
      "    Total quantity: 31,720.17 piece\n",
      "    Average quantity: 2,265.73 piece\n",
      "  sheet:\n",
      "    Count: 13 lots\n",
      "    Total quantity: 69,598.46 sheet\n",
      "    Average quantity: 5,353.73 sheet\n",
      "  g:\n",
      "    Count: 11 lots\n",
      "    Total quantity: 13,704.87 g\n",
      "    Average quantity: 1,245.90 g\n",
      "  box:\n",
      "    Count: 8 lots\n",
      "    Total quantity: 29,941.84 box\n",
      "    Average quantity: 3,742.73 box\n",
      "\n",
      "Top 10 Materials by Lot Count:\n",
      "  Catalyst CP 617: 5 lots (2.5%)\n",
      "  Thickener EP 872: 4 lots (2.0%)\n",
      "  Insert USP 811: 4 lots (2.0%)\n",
      "  Glass Ultra 853: 4 lots (2.0%)\n",
      "  Wood USP 956: 3 lots (1.5%)\n",
      "  Additive BP 550: 3 lots (1.5%)\n",
      "  Coloring BP 741: 3 lots (1.5%)\n",
      "  Base USP 480: 3 lots (1.5%)\n",
      "  Box ACS 169: 3 lots (1.5%)\n",
      "  Paper USP 942: 3 lots (1.5%)\n",
      "\n",
      "Lots with supplier: 137 (68.5%)\n",
      "\n",
      "Top 5 Suppliers by Lot Count:\n",
      "  Eastern Manufacturing: 6 lots (4.4% of supplied lots)\n",
      "  North Supplies: 5 lots (3.6% of supplied lots)\n",
      "  International Supply: 5 lots (3.6% of supplied lots)\n",
      "  Martinez Supplies: 5 lots (3.6% of supplied lots)\n",
      "  Modern Production: 5 lots (3.6% of supplied lots)\n",
      "\n",
      "Top 5 Storage Locations by Lot Count:\n",
      "  Wing 3 - Staging Area - Sub-Location 17 - Sub-Location 84 - Sub-Location 26: 4 lots (2.0%)\n",
      "  Wing A - Pallet Rack - Section 47: 3 lots (1.5%)\n",
      "  Block A - Cold Storage - Cold Storage Section 11: 3 lots (1.5%)\n",
      "  Rejected Material Area A - Sub-Location 25 - Sub-Location 87: 3 lots (1.5%)\n",
      "  Shipping Dock Area 3: 3 lots (1.5%)\n",
      "\n",
      "Lots with parent (split lots): 13 (6.5%)\n",
      "\n",
      "Cost Statistics:\n",
      "  Average cost per unit: $51.96\n",
      "  Total inventory value: $7,185,700.71\n",
      "\n",
      "Time-based Analysis:\n",
      "  Oldest lot created: 2023-07-19 00:00:00\n",
      "  Newest lot created: 2025-07-12 00:00:00\n",
      "  Average shelf life: 1017.5 days (2.8 years)\n",
      "  Minimum shelf life: 39 days\n",
      "  Maximum shelf life: 3619 days (9.9 years)\n",
      "\n",
      "Expired lots: 48 (24.0%)\n",
      "Lots expiring within 90 days: 11 (5.5%)\n",
      "\n",
      "Sample material lots data (first 5 records):\n",
      "         lot_id   material_id  lot_quantity quantity_unit     status  \\\n",
      "0  LOT-AAC3AB58  MAT-07958D8B       5558.06          unit   Reserved   \n",
      "1  LOT-0038CED5  MAT-A2C072A0        883.96            m³     In Use   \n",
      "2  LOT-F47CD7F1  MAT-2443ABE4        970.31             L  Available   \n",
      "3  LOT-E656D16B  MAT-C06E3CCC        346.55         batch   Reserved   \n",
      "4  LOT-C2E4F56E  MAT-CDB4D157       1691.56           ton  Available   \n",
      "\n",
      "  creation_date expiration_date   supplier_id supplier_lot_id receipt_date  \\\n",
      "0    2023-12-20      2026-08-11  SUP-8E73972A    SUPLOT-70053   2024-01-05   \n",
      "1    2024-02-29      2028-11-13  SUP-833BBEDA    SUPLOT-97639   2024-03-04   \n",
      "2    2024-03-26      2028-02-13  SUP-5CF5DECA    SUPLOT-37839   2024-04-01   \n",
      "3    2024-12-14      2025-06-25                                              \n",
      "4    2024-11-15      2027-01-18  SUP-152C51B6    SUPLOT-66637   2024-12-13   \n",
      "\n",
      "  storage_location_id quality_status  cost_per_unit parent_lot_id  \\\n",
      "0        LOC-07C5CE7A       Approved           9.08                 \n",
      "1        LOC-BB49C347     Under Test          90.95                 \n",
      "2        LOC-807BF128       Released           2.48                 \n",
      "3        LOC-DFF22709       Released         104.40                 \n",
      "4        LOC-76956CE0       Released          99.17                 \n",
      "\n",
      "   quantity_numeric  cost_numeric   total_cost  shelf_life_days  \n",
      "0           5558.06          9.08   50467.1848              965  \n",
      "1            883.96         90.95   80396.1620             1719  \n",
      "2            970.31          2.48    2406.3688             1419  \n",
      "3            346.55        104.40   36179.8200              193  \n",
      "4           1691.56         99.17  167752.0052              794  \n",
      "\n",
      "2. Generating Material Consumption data...\n",
      "Generating synthetic operator IDs...\n",
      "Successfully generated 300 material consumption records.\n",
      "Data saved to data/material_consumption.csv\n",
      "\n",
      "Material Consumption Statistics:\n",
      "Total consumption records: 300\n",
      "\n",
      "Quantity Statistics by Unit:\n",
      "  sheet:\n",
      "    Count: 22 records\n",
      "    Total consumed: 29,014.71 sheet\n",
      "    Average consumption: 1,318.85 sheet\n",
      "  m³:\n",
      "    Count: 23 records\n",
      "    Total consumed: 6,395.24 m³\n",
      "    Average consumption: 278.05 m³\n",
      "  L:\n",
      "    Count: 52 records\n",
      "    Total consumed: 5,581.86 L\n",
      "    Average consumption: 107.34 L\n",
      "  piece:\n",
      "    Count: 17 records\n",
      "    Total consumed: 7,917.63 piece\n",
      "    Average consumption: 465.74 piece\n",
      "  kg:\n",
      "    Count: 44 records\n",
      "    Total consumed: 4,798.52 kg\n",
      "    Average consumption: 109.06 kg\n",
      "  box:\n",
      "    Count: 16 records\n",
      "    Total consumed: 9,031.98 box\n",
      "    Average consumption: 564.50 box\n",
      "  g:\n",
      "    Count: 12 records\n",
      "    Total consumed: 5,738.13 g\n",
      "    Average consumption: 478.18 g\n",
      "  unit:\n",
      "    Count: 29 records\n",
      "    Total consumed: 4,170.18 unit\n",
      "    Average consumption: 143.80 unit\n",
      "  ton:\n",
      "    Count: 42 records\n",
      "    Total consumed: 8,903.48 ton\n",
      "    Average consumption: 211.99 ton\n",
      "  batch:\n",
      "    Count: 26 records\n",
      "    Total consumed: 1,739.88 batch\n",
      "    Average consumption: 66.92 batch\n",
      "  roll:\n",
      "    Count: 17 records\n",
      "    Total consumed: 24,023.26 roll\n",
      "    Average consumption: 1,413.13 roll\n",
      "\n",
      "Top 10 Lots by Consumption Frequency:\n",
      "  LOT-6A3DEE21 (Material: MAT-3BBD400D): 5 records (1.7%)\n",
      "  LOT-F47CD7F1 (Material: MAT-2443ABE4): 5 records (1.7%)\n",
      "  LOT-C2E4F56E (Material: MAT-CDB4D157): 5 records (1.7%)\n",
      "  LOT-23B33B34 (Material: MAT-2D5A342C): 5 records (1.7%)\n",
      "  LOT-CB580F1B (Material: MAT-0CFD426F): 5 records (1.7%)\n",
      "  LOT-E35C8ABC (Material: MAT-A1BA4C3F): 5 records (1.7%)\n",
      "  LOT-72C43863 (Material: MAT-B2A7853E): 4 records (1.3%)\n",
      "  LOT-16FD3A46 (Material: MAT-3A039F85): 4 records (1.3%)\n",
      "  LOT-6AC08CC7 (Material: MAT-09B3ED7E): 4 records (1.3%)\n",
      "  LOT-00DA7465 (Material: MAT-BC4C1BBA): 4 records (1.3%)\n",
      "\n",
      "Top 10 Batches by Consumption Records:\n",
      "  BATCH-1CF25CD7 (Product: PROD-45B80EA7): 9 records (3.0%)\n",
      "  BATCH-B4D9A4A4 (Product: PROD-0C079A77): 7 records (2.3%)\n",
      "  BATCH-3431162B (Product: PROD-BCB4AE01): 7 records (2.3%)\n",
      "  BATCH-AF7F8C1E (Product: PROD-169AE337): 6 records (2.0%)\n",
      "  BATCH-CB8DC106 (Product: PROD-9925BDF2): 6 records (2.0%)\n",
      "  BATCH-448CE3CE (Product: PROD-0C079A77): 6 records (2.0%)\n",
      "  BATCH-B648A498 (Product: PROD-039409EB): 6 records (2.0%)\n",
      "  BATCH-E6E98BA1 (Product: PROD-BCB4AE01): 6 records (2.0%)\n",
      "  BATCH-AD5DB274 (Product: PROD-039409EB): 6 records (2.0%)\n",
      "  BATCH-354C5CB5 (Product: PROD-6F202F2D): 6 records (2.0%)\n",
      "\n",
      "Top 5 Equipment by Consumption:\n",
      "  Spiral 806: 6 records (2.0%)\n",
      "  Batch Crystallizer 916: 6 records (2.0%)\n",
      "  Radiation Sterilizer 827: 5 records (1.7%)\n",
      "  Rotary Screw Compressor 589: 5 records (1.7%)\n",
      "  Radiation Sterilizer 303: 5 records (1.7%)\n",
      "\n",
      "Records with planned consumption: 226 (75.3%)\n",
      "\n",
      "Consumption Variance Statistics:\n",
      "  Average variance: -2.09\n",
      "  Average variance percentage: -0.0%\n",
      "  Over-consumption records: 119 (52.7% of planned records)\n",
      "  Under-consumption records: 107 (47.3% of planned records)\n",
      "\n",
      "Consumption Timeline:\n",
      "  First consumption: 2024-07-16 13:55:52\n",
      "  Last consumption: 2025-07-13 11:01:52\n",
      "\n",
      "Consumption by Day of Week:\n",
      "  Monday: 55 records (18.3%)\n",
      "  Tuesday: 48 records (16.0%)\n",
      "  Wednesday: 38 records (12.7%)\n",
      "  Thursday: 47 records (15.7%)\n",
      "  Friday: 31 records (10.3%)\n",
      "  Saturday: 46 records (15.3%)\n",
      "  Sunday: 35 records (11.7%)\n",
      "\n",
      "Consumption by Hour of Day (Top 5):\n",
      "  19:00 - 19:59: 18 records (6.0%)\n",
      "  12:00 - 12:59: 16 records (5.3%)\n",
      "  17:00 - 17:59: 16 records (5.3%)\n",
      "  6:00 - 6:59: 16 records (5.3%)\n",
      "  8:00 - 8:59: 15 records (5.0%)\n",
      "\n",
      "Sample material consumption data (first 5 records):\n",
      "  consumption_id        lot_id        batch_id work_order_id  \\\n",
      "0  CONS-043A2AA1  LOT-64B247C9  BATCH-2FE784F1   WO-CC889B70   \n",
      "1  CONS-C39F982F  LOT-0A629CD1  BATCH-F25A0620   WO-66C83FBA   \n",
      "2  CONS-81A15D78  LOT-CAABEDC2  BATCH-5FBB8E66   WO-EB5EF242   \n",
      "3  CONS-C2D0A536  LOT-96600CC4  BATCH-F1B4C641   WO-9880D96E   \n",
      "4  CONS-373714AF  LOT-4FE54703  BATCH-E6E98BA1   WO-46BACB88   \n",
      "\n",
      "            timestamp  quantity   unit equipment_id        step_id  \\\n",
      "0 2025-06-12 08:17:52    182.95  sheet  EQ-ADA504C4  STEP-AFCF5BCD   \n",
      "1 2025-05-16 05:02:52    179.17     m³  EQ-422154D2  STEP-DAE1DD88   \n",
      "2 2024-10-05 22:39:52     59.74      L  EQ-7958ED31  STEP-0A120C5B   \n",
      "3 2024-10-12 01:18:52    240.28     m³  EQ-7068B50A  STEP-AFCF5BCD   \n",
      "4 2025-02-17 20:10:52   1857.52  piece  EQ-FFD6E73D  STEP-22074C45   \n",
      "\n",
      "     operator_id planned_consumption consumption_variance  quantity_numeric  \\\n",
      "0  PERS-75A2406E              187.04                 -4.1            182.95   \n",
      "1  PERS-9758420C                                                     179.17   \n",
      "2  PERS-9758420C               66.79                -7.05             59.74   \n",
      "3  PERS-0BB934F5               286.5               -46.22            240.28   \n",
      "4  PERS-FB17FB0C             1755.57               101.95           1857.52   \n",
      "\n",
      "   planned_numeric  variance_numeric  variance_percent day_of_week  hour  \n",
      "0           187.04             -4.10         -2.192044    Thursday     8  \n",
      "1              NaN               NaN               NaN      Friday     5  \n",
      "2            66.79             -7.05        -10.555472    Saturday    22  \n",
      "3           286.50            -46.22        -16.132635    Saturday     1  \n",
      "4          1755.57            101.95          5.807231      Monday    20  \n",
      "\n",
      "3. Generating Costs data...\n",
      "Successfully generated 500 cost records.\n",
      "Data saved to data/costs.csv\n",
      "\n",
      "Costs Statistics:\n",
      "Total cost records: 500\n",
      "\n",
      "Cost Type Distribution:\n",
      "  Labor: 158 (31.6%)\n",
      "  Material: 136 (27.2%)\n",
      "  Overhead: 80 (16.0%)\n",
      "  Maintenance: 43 (8.6%)\n",
      "  Quality: 29 (5.8%)\n",
      "  Energy: 28 (5.6%)\n",
      "  Setup: 26 (5.2%)\n",
      "\n",
      "Cost Amount Statistics:\n",
      "  Total cost amount: $2,085,468.79\n",
      "  Average cost amount: $4,170.94\n",
      "  Minimum cost amount: $69.21\n",
      "  Maximum cost amount: $19,734.03\n",
      "\n",
      "Cost Category Distribution (top 10):\n",
      "  Engineering: 41 (8.2%)\n",
      "  Indirect Labor: 39 (7.8%)\n",
      "  Spare Parts: 34 (6.8%)\n",
      "  Raw Material: 32 (6.4%)\n",
      "  Direct Labor: 28 (5.6%)\n",
      "  Supervision: 28 (5.6%)\n",
      "  Chemicals: 27 (5.4%)\n",
      "  Packaging: 22 (4.4%)\n",
      "  Quality Control: 22 (4.4%)\n",
      "  Consumables: 21 (4.2%)\n",
      "\n",
      "Cost Center Distribution:\n",
      "  Production: 210 (42.0%)\n",
      "  Supply Chain: 61 (12.2%)\n",
      "  Maintenance: 47 (9.4%)\n",
      "  Quality: 47 (9.4%)\n",
      "  Facilities: 39 (7.8%)\n",
      "  Utilities: 29 (5.8%)\n",
      "  Engineering: 29 (5.8%)\n",
      "  R&D: 24 (4.8%)\n",
      "  Administration: 14 (2.8%)\n",
      "\n",
      "Currency Distribution:\n",
      "  USD: 319 (63.8%)\n",
      "  EUR: 73 (14.6%)\n",
      "  GBP: 51 (10.2%)\n",
      "  CAD: 21 (4.2%)\n",
      "  JPY: 19 (3.8%)\n",
      "  AUD: 15 (3.0%)\n",
      "  CNY: 2 (0.4%)\n",
      "\n",
      "Association Statistics:\n",
      "  Costs with work order: 406 (81.2%)\n",
      "  Costs with product: 305 (61.0%)\n",
      "  Costs with equipment: 258 (51.6%)\n",
      "  Costs with batch: 285 (57.0%)\n",
      "\n",
      "Costs with planned values: 346 (69.2%)\n",
      "\n",
      "Variance Statistics:\n",
      "  Average variance: $-92.54\n",
      "  Average variance percentage: -0.5%\n",
      "  Costs over budget: 169 (48.8% of planned costs)\n",
      "  Costs under budget: 177 (51.2% of planned costs)\n",
      "\n",
      "Cost Timeline:\n",
      "  First cost recorded: 2024-07-17 14:22:15\n",
      "  Last cost recorded: 2025-07-16 01:56:54\n",
      "\n",
      "Costs by Month (latest 6 months):\n",
      "  2025-02: $144,018.85\n",
      "  2025-03: $160,778.35\n",
      "  2025-04: $127,035.92\n",
      "  2025-05: $195,624.41\n",
      "  2025-06: $195,690.15\n",
      "  2025-07: $112,141.00\n",
      "\n",
      "Top 5 Work Orders by Cost:\n",
      "  WO-0FD67ADD: $34,751.42\n",
      "  WO-F08E2C7D: $33,269.50\n",
      "  WO-4AD4E54B: $32,504.07\n",
      "  WO-D4034BB3: $31,800.49\n",
      "\n",
      "Top 5 Products by Cost:\n",
      "  Consumer Premium 918: $57,041.34\n",
      "  Braking Advanced 673: $56,515.39\n",
      "  Electrical Advanced 720: $37,957.21\n",
      "  Cooling Premium 382: $37,515.37\n",
      "\n",
      "Top 5 Equipment by Cost:\n",
      "  Continuous Distillation 723: $33,728.17\n",
      "  Rotary Screw Compressor 426: $25,798.29\n",
      "  Disc Centrifuge 885: $25,376.71\n",
      "  SCARA Robot 598: $25,190.03\n",
      "\n",
      "Sample costs data (first 5 records):\n",
      "         cost_id cost_type work_order_id     product_id equipment_id  \\\n",
      "0  COST-4F2B3AC5    Energy   WO-5555E671                 EQ-D476EC68   \n",
      "1  COST-11F710CF    Energy                PROD-7C8BD908  EQ-ADA504C4   \n",
      "2  COST-EC137A3B  Overhead   WO-85DABE00                               \n",
      "3  COST-308AC4D3     Labor   WO-9C3600AD                               \n",
      "4  COST-EAEEB657  Material   WO-EA903B07  PROD-5B8E29A6  EQ-5C402C11   \n",
      "\n",
      "         batch_id           timestamp    amount currency    cost_category  \\\n",
      "0                 2024-10-30 19:21:42    941.75      EUR      Electricity   \n",
      "1                 2024-08-20 12:03:49    224.32      USD              Gas   \n",
      "2  BATCH-0B07240B 2025-05-20 22:13:38  19734.03      AUD               IT   \n",
      "3                 2025-05-13 16:24:24   3738.07      USD  Quality Control   \n",
      "4                 2024-09-02 15:23:27   1543.78      GBP     Raw Material   \n",
      "\n",
      "  cost_center planned_cost variance  amount_numeric  planned_cost_numeric  \\\n",
      "0  Production      1020.34   -78.59          941.75               1020.34   \n",
      "1   Utilities       289.95   -65.63          224.32                289.95   \n",
      "2  Production     20428.71  -694.68        19734.03              20428.71   \n",
      "3  Production                               3738.07                   NaN   \n",
      "4  Production      1303.76   240.02         1543.78               1303.76   \n",
      "\n",
      "   variance_numeric  variance_percent    month  \n",
      "0            -78.59         -7.702335  2024-10  \n",
      "1            -65.63        -22.634937  2024-08  \n",
      "2           -694.68         -3.400508  2025-05  \n",
      "3               NaN               NaN  2025-05  \n",
      "4            240.02         18.409830  2024-09  \n",
      "\n",
      "All ISA-95 Level 4 data generation complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_work_orders_data(work_orders_file=\"data/work_orders.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated work orders data\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_file: CSV file containing work orders data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the work orders data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(work_orders_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Work orders data file {work_orders_file} not found.\")\n",
    "        print(\"Costs will be generated with synthetic work order IDs where needed.\")\n",
    "        return None\n",
    "\n",
    "def load_products_data(products_file=\"data/products.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated products data\n",
    "    \n",
    "    Parameters:\n",
    "    - products_file: CSV file containing products data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the products data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(products_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Products data file {products_file} not found.\")\n",
    "        print(\"Costs will be generated with synthetic product IDs where needed.\")\n",
    "        return None\n",
    "\n",
    "def load_equipment_data(equipment_file=\"data/equipment.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated equipment data\n",
    "    \n",
    "    Parameters:\n",
    "    - equipment_file: CSV file containing equipment data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the equipment data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(equipment_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Equipment data file {equipment_file} not found.\")\n",
    "        print(\"Costs will be generated with synthetic equipment IDs where needed.\")\n",
    "        return None\n",
    "\n",
    "def load_batches_data(batches_file=\"data/batches.csv\"):\n",
    "    \"\"\"\n",
    "    Load the previously generated batches data\n",
    "    \n",
    "    Parameters:\n",
    "    - batches_file: CSV file containing batches data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the batches data or None if not available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(batches_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Note: Batches data file {batches_file} not found.\")\n",
    "        print(\"Costs will be generated with synthetic batch IDs where needed.\")\n",
    "        return None\n",
    "\n",
    "def generate_costs_data(work_orders_df=None, products_df=None, equipment_df=None, \n",
    "                      batches_df=None, num_costs=500, start_time=None, end_time=None,\n",
    "                      output_file=\"data/costs.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the Costs table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    - batches_df: DataFrame containing batches data (optional)\n",
    "    - num_costs: Number of cost records to generate\n",
    "    - start_time: Start time for cost dates (defaults to 365 days ago)\n",
    "    - end_time: End time for cost dates (defaults to current date)\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated costs data\n",
    "    \"\"\"\n",
    "    # Set default time range if not provided\n",
    "    if start_time is None:\n",
    "        start_time = datetime.now() - timedelta(days=365)\n",
    "    if end_time is None:\n",
    "        end_time = datetime.now()\n",
    "    \n",
    "    # Generate work order IDs if work_orders_df is not provided\n",
    "    if work_orders_df is None or len(work_orders_df) == 0:\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    else:\n",
    "        work_order_ids = work_orders_df['work_order_id'].tolist()\n",
    "    \n",
    "    # Generate product IDs if products_df is not provided\n",
    "    if products_df is None or len(products_df) == 0:\n",
    "        print(\"Generating synthetic product IDs...\")\n",
    "        product_ids = [f\"PROD-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    else:\n",
    "        product_ids = products_df['product_id'].tolist()\n",
    "    \n",
    "    # Generate equipment IDs if equipment_df is not provided\n",
    "    if equipment_df is None or len(equipment_df) == 0:\n",
    "        print(\"Generating synthetic equipment IDs...\")\n",
    "        equipment_ids = [f\"EQ-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    else:\n",
    "        equipment_ids = equipment_df['equipment_id'].tolist()\n",
    "    \n",
    "    # Generate batch IDs if batches_df is not provided\n",
    "    if batches_df is None or len(batches_df) == 0:\n",
    "        print(\"Generating synthetic batch IDs...\")\n",
    "        batch_ids = [f\"BATCH-{uuid.uuid4().hex[:8].upper()}\" for _ in range(40)]\n",
    "    else:\n",
    "        batch_ids = batches_df['batch_id'].tolist()\n",
    "    \n",
    "    # Define cost types and their probabilities\n",
    "    cost_types = {\n",
    "        \"Labor\": 0.3,\n",
    "        \"Material\": 0.3,\n",
    "        \"Overhead\": 0.15,\n",
    "        \"Energy\": 0.05,\n",
    "        \"Maintenance\": 0.1,\n",
    "        \"Quality\": 0.05,\n",
    "        \"Setup\": 0.05\n",
    "    }\n",
    "    \n",
    "    # Define cost categories\n",
    "    cost_categories = {\n",
    "        \"Labor\": [\"Direct Labor\", \"Indirect Labor\", \"Supervision\", \"Quality Control\", \"Engineering\"],\n",
    "        \"Material\": [\"Raw Material\", \"Packaging\", \"Consumables\", \"Spare Parts\", \"Chemicals\"],\n",
    "        \"Overhead\": [\"Facility\", \"Depreciation\", \"Insurance\", \"Utilities\", \"IT\"],\n",
    "        \"Energy\": [\"Electricity\", \"Gas\", \"Water\", \"Steam\", \"Compressed Air\"],\n",
    "        \"Maintenance\": [\"Preventive\", \"Corrective\", \"Calibration\", \"Cleaning\", \"Inspection\"],\n",
    "        \"Quality\": [\"Testing\", \"Inspection\", \"Rework\", \"Documentation\", \"Validation\"],\n",
    "        \"Setup\": [\"Machine Setup\", \"Changeover\", \"Tooling\", \"Programming\", \"Validation\"]\n",
    "    }\n",
    "    \n",
    "    # Define cost centers\n",
    "    cost_centers = [\"Production\", \"Maintenance\", \"Quality\", \"Engineering\", \"Facilities\", \"Supply Chain\", \n",
    "                   \"Utilities\", \"R&D\", \"Administration\"]\n",
    "    \n",
    "    # Define currencies\n",
    "    currencies = [\"USD\", \"EUR\", \"GBP\", \"JPY\", \"CAD\", \"AUD\", \"CNY\"]\n",
    "    currency_weights = [0.6, 0.15, 0.1, 0.05, 0.05, 0.03, 0.02]  # Mostly USD\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"cost_id\": [f\"COST-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_costs)],\n",
    "        \"cost_type\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"product_id\": [],\n",
    "        \"equipment_id\": [],\n",
    "        \"batch_id\": [],\n",
    "        \"timestamp\": [],\n",
    "        \"amount\": [],\n",
    "        \"currency\": [],\n",
    "        \"cost_category\": [],\n",
    "        \"cost_center\": [],\n",
    "        \"planned_cost\": [],\n",
    "        \"variance\": []\n",
    "    }\n",
    "    \n",
    "    # Generate data for each cost record\n",
    "    for i in range(num_costs):\n",
    "        # Select cost type (weighted random)\n",
    "        cost_type = random.choices(\n",
    "            list(cost_types.keys()), \n",
    "            weights=list(cost_types.values())\n",
    "        )[0]\n",
    "        data[\"cost_type\"].append(cost_type)\n",
    "        \n",
    "        # Generate timestamp within the specified range\n",
    "        time_range_seconds = int((end_time - start_time).total_seconds())\n",
    "        random_seconds = random.randint(0, time_range_seconds)\n",
    "        timestamp = start_time + timedelta(seconds=random_seconds)\n",
    "        data[\"timestamp\"].append(timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Assign work order, product, equipment, and batch based on cost type\n",
    "        # Not all costs are associated with all entities\n",
    "        \n",
    "        # Work order association\n",
    "        if cost_type in [\"Labor\", \"Material\", \"Setup\", \"Quality\"] and random.random() < 0.9:\n",
    "            # These cost types are almost always associated with work orders\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        else:\n",
    "            # Other cost types may or may not be associated with work orders\n",
    "            if random.random() < 0.5:\n",
    "                data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "            else:\n",
    "                data[\"work_order_id\"].append(\"\")\n",
    "        \n",
    "        # Product association\n",
    "        if cost_type in [\"Material\", \"Quality\"] and random.random() < 0.9:\n",
    "            # These cost types are almost always associated with products\n",
    "            data[\"product_id\"].append(random.choice(product_ids))\n",
    "        else:\n",
    "            # Other cost types may or may not be associated with products\n",
    "            if random.random() < 0.4:\n",
    "                data[\"product_id\"].append(random.choice(product_ids))\n",
    "            else:\n",
    "                data[\"product_id\"].append(\"\")\n",
    "        \n",
    "        # Equipment association\n",
    "        if cost_type in [\"Maintenance\", \"Energy\", \"Setup\"] and random.random() < 0.9:\n",
    "            # These cost types are almost always associated with equipment\n",
    "            data[\"equipment_id\"].append(random.choice(equipment_ids))\n",
    "        else:\n",
    "            # Other cost types may or may not be associated with equipment\n",
    "            if random.random() < 0.4:\n",
    "                data[\"equipment_id\"].append(random.choice(equipment_ids))\n",
    "            else:\n",
    "                data[\"equipment_id\"].append(\"\")\n",
    "        \n",
    "        # Batch association\n",
    "        if data[\"work_order_id\"][-1] and random.random() < 0.7:\n",
    "            # If associated with a work order, likely associated with a batch\n",
    "            data[\"batch_id\"].append(random.choice(batch_ids))\n",
    "        else:\n",
    "            data[\"batch_id\"].append(\"\")\n",
    "        \n",
    "        # Generate cost amount based on cost type\n",
    "        if cost_type == \"Labor\":\n",
    "            # Labor costs are typically higher\n",
    "            amount = random.uniform(100, 5000)\n",
    "        elif cost_type == \"Material\":\n",
    "            # Material costs vary widely\n",
    "            amount = random.uniform(50, 10000)\n",
    "        elif cost_type == \"Overhead\":\n",
    "            # Overhead costs can be substantial\n",
    "            amount = random.uniform(500, 20000)\n",
    "        elif cost_type == \"Energy\":\n",
    "            # Energy costs are moderate\n",
    "            amount = random.uniform(100, 3000)\n",
    "        elif cost_type == \"Maintenance\":\n",
    "            # Maintenance costs depend on the scope\n",
    "            amount = random.uniform(200, 8000)\n",
    "        elif cost_type == \"Quality\":\n",
    "            # Quality costs are typically lower\n",
    "            amount = random.uniform(50, 2000)\n",
    "        else:  # Setup\n",
    "            # Setup costs are moderate\n",
    "            amount = random.uniform(100, 3000)\n",
    "            \n",
    "        data[\"amount\"].append(round(amount, 2))\n",
    "        \n",
    "        # Set currency (weighted random)\n",
    "        data[\"currency\"].append(random.choices(currencies, weights=currency_weights)[0])\n",
    "        \n",
    "        # Set cost category\n",
    "        if cost_type in cost_categories:\n",
    "            data[\"cost_category\"].append(random.choice(cost_categories[cost_type]))\n",
    "        else:\n",
    "            data[\"cost_category\"].append(\"General\")\n",
    "        \n",
    "        # Set cost center\n",
    "        if cost_type == \"Labor\":\n",
    "            center_weights = [0.6, 0.05, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.0]  # Mostly Production\n",
    "        elif cost_type == \"Material\":\n",
    "            center_weights = [0.5, 0.0, 0.05, 0.05, 0.0, 0.3, 0.0, 0.1, 0.0]  # Production or Supply Chain\n",
    "        elif cost_type == \"Overhead\":\n",
    "            center_weights = [0.2, 0.05, 0.05, 0.05, 0.3, 0.05, 0.1, 0.05, 0.15]  # Varied\n",
    "        elif cost_type == \"Energy\":\n",
    "            center_weights = [0.3, 0.05, 0.05, 0.05, 0.1, 0.0, 0.45, 0.0, 0.0]  # Utilities or Production\n",
    "        elif cost_type == \"Maintenance\":\n",
    "            center_weights = [0.1, 0.7, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0]  # Mostly Maintenance\n",
    "        elif cost_type == \"Quality\":\n",
    "            center_weights = [0.1, 0.0, 0.7, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0]  # Mostly Quality\n",
    "        else:  # Setup\n",
    "            center_weights = [0.7, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0]  # Mostly Production\n",
    "            \n",
    "        data[\"cost_center\"].append(random.choices(cost_centers, weights=center_weights)[0])\n",
    "        \n",
    "        # Set planned cost and variance\n",
    "        # About 70% of costs have a planned amount\n",
    "        if random.random() < 0.7:\n",
    "            # Planned costs are usually close to actual but can vary\n",
    "            variation = random.uniform(0.7, 1.3)  # -30% to +30%\n",
    "            planned_cost = amount / variation\n",
    "            data[\"planned_cost\"].append(round(planned_cost, 2))\n",
    "            \n",
    "            # Calculate variance (actual - planned)\n",
    "            variance = amount - planned_cost\n",
    "            data[\"variance\"].append(round(variance, 2))\n",
    "        else:\n",
    "            data[\"planned_cost\"].append(\"\")\n",
    "            data[\"variance\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} cost records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Material Lots Data Generation\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "def generate_material_lots_data(materials_df=None, suppliers_df=None, storage_locations_df=None, \n",
    "                             num_lots=200, output_file=\"data/material_lots.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the MaterialLots table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    - suppliers_df: DataFrame containing suppliers data (optional)\n",
    "    - storage_locations_df: DataFrame containing storage locations data (optional)\n",
    "    - num_lots: Number of material lot records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated material lots data\n",
    "    \"\"\"\n",
    "    # Generate material IDs if materials_df is not provided\n",
    "    if materials_df is None or len(materials_df) == 0:\n",
    "        print(\"Generating synthetic material IDs...\")\n",
    "        material_ids = [f\"MAT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "        \n",
    "        # Create synthetic material types for lot assignment\n",
    "        material_types = {}\n",
    "        material_units = {}\n",
    "        for mat_id in material_ids:\n",
    "            material_types[mat_id] = random.choice([\"Raw Material\", \"Packaging\", \"WIP\", \"Intermediate\", \"Consumable\"])\n",
    "            material_units[mat_id] = random.choice([\"kg\", \"L\", \"g\", \"m\", \"piece\", \"unit\", \"box\", \"roll\"])\n",
    "    else:\n",
    "        material_ids = materials_df['material_id'].tolist()\n",
    "        \n",
    "        # Extract material types and units if available\n",
    "        if 'material_type' in materials_df.columns:\n",
    "            material_types = dict(zip(materials_df['material_id'], materials_df['material_type']))\n",
    "        else:\n",
    "            material_types = {mat_id: random.choice([\"Raw Material\", \"Packaging\", \"WIP\", \"Intermediate\", \"Consumable\"]) \n",
    "                             for mat_id in material_ids}\n",
    "            \n",
    "        if 'unit_of_measure' in materials_df.columns:\n",
    "            material_units = dict(zip(materials_df['material_id'], materials_df['unit_of_measure']))\n",
    "        else:\n",
    "            material_units = {mat_id: random.choice([\"kg\", \"L\", \"g\", \"m\", \"piece\", \"unit\", \"box\", \"roll\"]) \n",
    "                             for mat_id in material_ids}\n",
    "    \n",
    "    # Generate supplier IDs if suppliers_df is not provided\n",
    "    if suppliers_df is None or len(suppliers_df) == 0:\n",
    "        print(\"Generating synthetic supplier IDs...\")\n",
    "        supplier_ids = [f\"SUP-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    else:\n",
    "        supplier_ids = suppliers_df['supplier_id'].tolist()\n",
    "    \n",
    "    # Generate storage location IDs if storage_locations_df is not provided\n",
    "    if storage_locations_df is None or len(storage_locations_df) == 0:\n",
    "        print(\"Generating synthetic storage location IDs...\")\n",
    "        storage_location_ids = [f\"LOC-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    else:\n",
    "        storage_location_ids = storage_locations_df['location_id'].tolist()\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"lot_id\": [f\"LOT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_lots)],\n",
    "        \"material_id\": [],\n",
    "        \"lot_quantity\": [],\n",
    "        \"quantity_unit\": [],\n",
    "        \"status\": [],\n",
    "        \"creation_date\": [],\n",
    "        \"expiration_date\": [],\n",
    "        \"supplier_id\": [],\n",
    "        \"supplier_lot_id\": [],\n",
    "        \"receipt_date\": [],\n",
    "        \"storage_location_id\": [],\n",
    "        \"quality_status\": [],\n",
    "        \"cost_per_unit\": [],\n",
    "        \"parent_lot_id\": []\n",
    "    }\n",
    "    \n",
    "    # Define status options\n",
    "    statuses = [\"Available\", \"Reserved\", \"In Use\", \"Consumed\", \"On Hold\", \"Quarantined\", \"Rejected\"]\n",
    "    status_weights = [0.6, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05]  # Mostly available\n",
    "    \n",
    "    # Define quality status options\n",
    "    quality_statuses = [\"Released\", \"Under Test\", \"Approved\", \"Rejected\", \"Pending Review\"]\n",
    "    quality_weights = [0.7, 0.1, 0.1, 0.05, 0.05]  # Mostly released\n",
    "    \n",
    "    # Keep track of created lots by material for parent-child relationships\n",
    "    lots_by_material = {material_id: [] for material_id in material_ids}\n",
    "    \n",
    "    # Generate data for each material lot\n",
    "    for i in range(num_lots):\n",
    "        # Select material\n",
    "        material_id = random.choice(material_ids)\n",
    "        data[\"material_id\"].append(material_id)\n",
    "        \n",
    "        # Determine if this is a split lot (child lot)\n",
    "        is_child_lot = False\n",
    "        if i > num_lots * 0.2 and lots_by_material[material_id] and random.random() < 0.2:  # 20% chance for child lots\n",
    "            is_child_lot = True\n",
    "            parent_lot_id = random.choice(lots_by_material[material_id])\n",
    "            data[\"parent_lot_id\"].append(parent_lot_id)\n",
    "        else:\n",
    "            data[\"parent_lot_id\"].append(\"\")\n",
    "        \n",
    "        # Remember this lot for potential future splits\n",
    "        lots_by_material[material_id].append(data[\"lot_id\"][i])\n",
    "        \n",
    "        # Set quantity based on material type\n",
    "        material_type = material_types.get(material_id, \"Raw Material\")\n",
    "        \n",
    "        if material_type == \"Raw Material\":\n",
    "            if is_child_lot:\n",
    "                # Child lots are smaller\n",
    "                quantity = random.uniform(10, 200)\n",
    "            else:\n",
    "                quantity = random.uniform(100, 2000)\n",
    "        elif material_type == \"Packaging\":\n",
    "            if is_child_lot:\n",
    "                quantity = random.uniform(50, 500)\n",
    "            else:\n",
    "                quantity = random.uniform(500, 10000)\n",
    "        elif material_type in [\"WIP\", \"Intermediate\"]:\n",
    "            if is_child_lot:\n",
    "                quantity = random.uniform(5, 50)\n",
    "            else:\n",
    "                quantity = random.uniform(50, 500)\n",
    "        else:  # Consumable\n",
    "            if is_child_lot:\n",
    "                quantity = random.uniform(1, 20)\n",
    "            else:\n",
    "                quantity = random.uniform(10, 200)\n",
    "            \n",
    "        data[\"lot_quantity\"].append(round(quantity, 2))\n",
    "        \n",
    "        # Set unit\n",
    "        unit = material_units.get(material_id, \"kg\")\n",
    "        data[\"quantity_unit\"].append(unit)\n",
    "        \n",
    "        # Set status\n",
    "        if is_child_lot:\n",
    "            # Child lots typically inherit status from parent, but we don't track that here\n",
    "            # so just make them mostly available\n",
    "            data[\"status\"].append(random.choices(statuses, weights=[0.8, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0])[0])\n",
    "        else:\n",
    "            data[\"status\"].append(random.choices(statuses, weights=status_weights)[0])\n",
    "        \n",
    "        # Generate creation date (within last 2 years)\n",
    "        days_ago = random.randint(1, 730)\n",
    "        creation_date = datetime.now() - timedelta(days=days_ago)\n",
    "        data[\"creation_date\"].append(creation_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Set expiration date based on material type\n",
    "        if material_type == \"Raw Material\":\n",
    "            # Raw materials typically have longer shelf life\n",
    "            shelf_life_days = random.randint(365, 1825)  # 1-5 years\n",
    "        elif material_type == \"Packaging\":\n",
    "            # Packaging materials have very long shelf life\n",
    "            shelf_life_days = random.randint(730, 3650)  # 2-10 years\n",
    "        elif material_type in [\"WIP\", \"Intermediate\"]:\n",
    "            # Intermediate products have shorter shelf life\n",
    "            shelf_life_days = random.randint(30, 365)  # 1 month to 1 year\n",
    "        else:  # Consumable\n",
    "            # Consumables vary widely\n",
    "            shelf_life_days = random.randint(90, 1095)  # 3 months to 3 years\n",
    "            \n",
    "        expiration_date = creation_date + timedelta(days=shelf_life_days)\n",
    "        data[\"expiration_date\"].append(expiration_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        # Set supplier info\n",
    "        if material_type in [\"Raw Material\", \"Packaging\", \"Consumable\"] and not is_child_lot:\n",
    "            # External materials have supplier info\n",
    "            data[\"supplier_id\"].append(random.choice(supplier_ids))\n",
    "            data[\"supplier_lot_id\"].append(f\"SUPLOT-{random.randint(10000, 99999)}\")\n",
    "            \n",
    "            # Receipt date is between creation date and today\n",
    "            max_receipt_days = min((datetime.now() - creation_date).days, 30)  # Within 30 days of creation\n",
    "            if max_receipt_days > 0:\n",
    "                receipt_days = random.randint(0, max_receipt_days)\n",
    "            else:\n",
    "                receipt_days = 0\n",
    "            receipt_date = creation_date + timedelta(days=receipt_days)\n",
    "            data[\"receipt_date\"].append(receipt_date.strftime(\"%Y-%m-%d\"))\n",
    "        else:\n",
    "            # Internally produced materials don't have supplier info\n",
    "            data[\"supplier_id\"].append(\"\")\n",
    "            data[\"supplier_lot_id\"].append(\"\")\n",
    "            data[\"receipt_date\"].append(\"\")\n",
    "        \n",
    "        # Set storage location\n",
    "        data[\"storage_location_id\"].append(random.choice(storage_location_ids))\n",
    "        \n",
    "        # Set quality status\n",
    "        if data[\"status\"][-1] in [\"On Hold\", \"Quarantined\", \"Rejected\"]:\n",
    "            # Problematic lots have corresponding quality status\n",
    "            if data[\"status\"][-1] == \"On Hold\":\n",
    "                data[\"quality_status\"].append(\"Under Test\")\n",
    "            elif data[\"status\"][-1] == \"Quarantined\":\n",
    "                data[\"quality_status\"].append(\"Pending Review\")\n",
    "            else:  # Rejected\n",
    "                data[\"quality_status\"].append(\"Rejected\")\n",
    "        else:\n",
    "            # Normal lots have standard quality status\n",
    "            data[\"quality_status\"].append(random.choices(quality_statuses, weights=quality_weights)[0])\n",
    "        \n",
    "        # Set cost per unit\n",
    "        if material_type == \"Raw Material\":\n",
    "            cost = random.uniform(1, 100)\n",
    "        elif material_type == \"Packaging\":\n",
    "            cost = random.uniform(0.1, 10)\n",
    "        elif material_type in [\"WIP\", \"Intermediate\"]:\n",
    "            cost = random.uniform(5, 200)\n",
    "        else:  # Consumable\n",
    "            cost = random.uniform(0.5, 50)\n",
    "            \n",
    "        data[\"cost_per_unit\"].append(round(cost, 2))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} material lot records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Material Consumption Data Generation\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "def generate_material_consumption_data(material_lots_df=None, batches_df=None, work_orders_df=None,\n",
    "                                   equipment_df=None, personnel_df=None, num_records=300,\n",
    "                                   output_file=\"data/material_consumption.csv\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the MaterialConsumption table from ISA-95 Level 4.\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_df: DataFrame containing material lots data (optional)\n",
    "    - batches_df: DataFrame containing batches data (optional)\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    - personnel_df: DataFrame containing personnel data (optional)\n",
    "    - num_records: Number of material consumption records to generate\n",
    "    - output_file: CSV file to save the data\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame containing the generated material consumption data\n",
    "    \"\"\"\n",
    "    # Load material lots or generate synthetic IDs\n",
    "    if material_lots_df is None or len(material_lots_df) == 0:\n",
    "        print(\"Generating synthetic lot IDs...\")\n",
    "        lot_ids = [f\"LOT-{uuid.uuid4().hex[:8].upper()}\" for _ in range(50)]\n",
    "    else:\n",
    "        # Use only available lots\n",
    "        available_lots = material_lots_df[material_lots_df['status'].isin(['Available', 'Reserved', 'In Use'])]\n",
    "        if len(available_lots) > 0:\n",
    "            lot_ids = available_lots['lot_id'].tolist()\n",
    "        else:\n",
    "            lot_ids = material_lots_df['lot_id'].tolist()\n",
    "    \n",
    "    # Load batches or generate synthetic IDs\n",
    "    if batches_df is None or len(batches_df) == 0:\n",
    "        print(\"Generating synthetic batch IDs...\")\n",
    "        batch_ids = [f\"BATCH-{uuid.uuid4().hex[:8].upper()}\" for _ in range(40)]\n",
    "    else:\n",
    "        batch_ids = batches_df['batch_id'].tolist()\n",
    "    \n",
    "    # Load work orders or generate synthetic IDs\n",
    "    if work_orders_df is None or len(work_orders_df) == 0:\n",
    "        print(\"Generating synthetic work order IDs...\")\n",
    "        work_order_ids = [f\"WO-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    else:\n",
    "        work_order_ids = work_orders_df['work_order_id'].tolist()\n",
    "    \n",
    "    # Load equipment or generate synthetic IDs\n",
    "    if equipment_df is None or len(equipment_df) == 0:\n",
    "        print(\"Generating synthetic equipment IDs...\")\n",
    "        equipment_ids = [f\"EQ-{uuid.uuid4().hex[:8].upper()}\" for _ in range(20)]\n",
    "    else:\n",
    "        equipment_ids = equipment_df['equipment_id'].tolist()\n",
    "    \n",
    "    # Load personnel or generate synthetic IDs\n",
    "    if personnel_df is None or len(personnel_df) == 0:\n",
    "        print(\"Generating synthetic operator IDs...\")\n",
    "        operator_ids = [f\"PERS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(15)]\n",
    "    else:\n",
    "        operator_ids = personnel_df['personnel_id'].tolist()\n",
    "    \n",
    "    # Generate batch step IDs (these would normally come from BatchStepExecution table)\n",
    "    step_ids = [f\"STEP-{uuid.uuid4().hex[:8].upper()}\" for _ in range(30)]\n",
    "    \n",
    "    # Generate data structure\n",
    "    data = {\n",
    "        \"consumption_id\": [f\"CONS-{uuid.uuid4().hex[:8].upper()}\" for _ in range(num_records)],\n",
    "        \"lot_id\": [],\n",
    "        \"batch_id\": [],\n",
    "        \"work_order_id\": [],\n",
    "        \"timestamp\": [],\n",
    "        \"quantity\": [],\n",
    "        \"unit\": [],\n",
    "        \"equipment_id\": [],\n",
    "        \"step_id\": [],\n",
    "        \"operator_id\": [],\n",
    "        \"planned_consumption\": [],\n",
    "        \"consumption_variance\": []\n",
    "    }\n",
    "    \n",
    "    # Create a mapping of batches to work orders (normally this would come from Batches table)\n",
    "    batch_to_work_order = {}\n",
    "    for batch_id in batch_ids:\n",
    "        batch_to_work_order[batch_id] = random.choice(work_order_ids)\n",
    "    \n",
    "    # Keep track of lot consumption to avoid over-consumption\n",
    "    lot_consumption = {lot_id: 0 for lot_id in lot_ids}\n",
    "    \n",
    "    # Get lot quantities and units if available\n",
    "    lot_quantities = {}\n",
    "    lot_units = {}\n",
    "    \n",
    "    if material_lots_df is not None and 'lot_quantity' in material_lots_df.columns:\n",
    "        for _, lot in material_lots_df.iterrows():\n",
    "            if 'lot_id' in lot and 'lot_quantity' in lot:\n",
    "                lot_quantities[lot['lot_id']] = lot['lot_quantity']\n",
    "                if 'quantity_unit' in lot:\n",
    "                    lot_units[lot['lot_id']] = lot['quantity_unit']\n",
    "    \n",
    "    # Generate data for each consumption record\n",
    "    for i in range(num_records):\n",
    "        # Select lot\n",
    "        if lot_ids:\n",
    "            # Try to find a lot that hasn't been fully consumed\n",
    "            available_lots = [lot_id for lot_id in lot_ids \n",
    "                             if lot_id not in lot_consumption or \n",
    "                             (lot_id in lot_quantities and lot_consumption[lot_id] < lot_quantities[lot_id])]\n",
    "            \n",
    "            if available_lots:\n",
    "                lot_id = random.choice(available_lots)\n",
    "            else:\n",
    "                # If all lots are consumed, just pick a random one\n",
    "                lot_id = random.choice(lot_ids)\n",
    "        else:\n",
    "            lot_id = f\"LOT-{uuid.uuid4().hex[:8].upper()}\"\n",
    "            \n",
    "        data[\"lot_id\"].append(lot_id)\n",
    "        \n",
    "        # Select batch\n",
    "        batch_id = random.choice(batch_ids)\n",
    "        data[\"batch_id\"].append(batch_id)\n",
    "        \n",
    "        # Set work order based on batch\n",
    "        if batch_id in batch_to_work_order:\n",
    "            data[\"work_order_id\"].append(batch_to_work_order[batch_id])\n",
    "        else:\n",
    "            data[\"work_order_id\"].append(random.choice(work_order_ids))\n",
    "        \n",
    "        # Generate timestamp (within last year)\n",
    "        days_ago = random.randint(1, 365)\n",
    "        hours_ago = random.randint(0, 23)\n",
    "        minutes_ago = random.randint(0, 59)\n",
    "        timestamp = datetime.now() - timedelta(days=days_ago, hours=hours_ago, minutes=minutes_ago)\n",
    "        data[\"timestamp\"].append(timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "        # Set quantity\n",
    "        if lot_id in lot_quantities and lot_quantities[lot_id] > 0:\n",
    "            # Use a portion of the available lot quantity\n",
    "            max_available = lot_quantities[lot_id] - lot_consumption.get(lot_id, 0)\n",
    "            if max_available <= 0:\n",
    "                # This shouldn't happen with our filtering, but just in case\n",
    "                quantity = random.uniform(0.1, 10)\n",
    "            else:\n",
    "                # Use between 10% and 50% of the available quantity\n",
    "                quantity = random.uniform(0.1 * max_available, 0.5 * max_available)\n",
    "        else:\n",
    "            # No quantity info available, generate a reasonable value\n",
    "            quantity = random.uniform(1, 100)\n",
    "            \n",
    "        data[\"quantity\"].append(round(quantity, 2))\n",
    "        \n",
    "        # Update lot consumption tracking\n",
    "        if lot_id in lot_consumption:\n",
    "            lot_consumption[lot_id] += quantity\n",
    "        else:\n",
    "            lot_consumption[lot_id] = quantity\n",
    "        \n",
    "        # Set unit\n",
    "        if lot_id in lot_units:\n",
    "            data[\"unit\"].append(lot_units[lot_id])\n",
    "        else:\n",
    "            data[\"unit\"].append(random.choice([\"kg\", \"L\", \"g\", \"unit\", \"piece\"]))\n",
    "        \n",
    "        # Set equipment\n",
    "        data[\"equipment_id\"].append(random.choice(equipment_ids))\n",
    "        \n",
    "        # Set process step\n",
    "        data[\"step_id\"].append(random.choice(step_ids))\n",
    "        \n",
    "        # Set operator\n",
    "        data[\"operator_id\"].append(random.choice(operator_ids))\n",
    "        \n",
    "        # Set planned consumption and variance\n",
    "        # About 80% of consumption records have planned values\n",
    "        if random.random() < 0.8:\n",
    "            # Planned consumption is usually close to actual but can vary\n",
    "            variation = random.uniform(0.8, 1.2)  # -20% to +20%\n",
    "            planned = quantity / variation\n",
    "            data[\"planned_consumption\"].append(round(planned, 2))\n",
    "            \n",
    "            # Calculate variance (actual - planned)\n",
    "            variance = quantity - planned\n",
    "            data[\"consumption_variance\"].append(round(variance, 2))\n",
    "        else:\n",
    "            data[\"planned_consumption\"].append(\"\")\n",
    "            data[\"consumption_variance\"].append(\"\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated {len(df)} material consumption records.\")\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Display Statistics Functions\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "def display_costs_statistics(costs_df, work_orders_df=None, products_df=None, equipment_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated costs data\n",
    "    \n",
    "    Parameters:\n",
    "    - costs_df: DataFrame containing costs data\n",
    "    - work_orders_df: DataFrame containing work orders data (optional)\n",
    "    - products_df: DataFrame containing products data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    \"\"\"\n",
    "    if costs_df is None or len(costs_df) == 0:\n",
    "        print(\"No costs data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nCosts Statistics:\")\n",
    "    print(f\"Total cost records: {len(costs_df)}\")\n",
    "    \n",
    "    # Cost type distribution\n",
    "    print(\"\\nCost Type Distribution:\")\n",
    "    type_counts = costs_df['cost_type'].value_counts()\n",
    "    for cost_type, count in type_counts.items():\n",
    "        print(f\"  {cost_type}: {count} ({count/len(costs_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Convert amount to numeric\n",
    "    costs_df['amount_numeric'] = pd.to_numeric(costs_df['amount'], errors='coerce')\n",
    "    \n",
    "    # Amount statistics\n",
    "    print(\"\\nCost Amount Statistics:\")\n",
    "    print(f\"  Total cost amount: ${costs_df['amount_numeric'].sum():,.2f}\")\n",
    "    print(f\"  Average cost amount: ${costs_df['amount_numeric'].mean():,.2f}\")\n",
    "    print(f\"  Minimum cost amount: ${costs_df['amount_numeric'].min():,.2f}\")\n",
    "    print(f\"  Maximum cost amount: ${costs_df['amount_numeric'].max():,.2f}\")\n",
    "    \n",
    "    # Cost category distribution (top 10)\n",
    "    print(\"\\nCost Category Distribution (top 10):\")\n",
    "    category_counts = costs_df['cost_category'].value_counts().head(10)\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"  {category}: {count} ({count/len(costs_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Cost center distribution\n",
    "    print(\"\\nCost Center Distribution:\")\n",
    "    center_counts = costs_df['cost_center'].value_counts()\n",
    "    for center, count in center_counts.items():\n",
    "        print(f\"  {center}: {count} ({count/len(costs_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Currency distribution\n",
    "    print(\"\\nCurrency Distribution:\")\n",
    "    currency_counts = costs_df['currency'].value_counts()\n",
    "    for currency, count in currency_counts.items():\n",
    "        print(f\"  {currency}: {count} ({count/len(costs_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Association statistics\n",
    "    has_wo = costs_df['work_order_id'].apply(lambda x: x != \"\").sum()\n",
    "    has_product = costs_df['product_id'].apply(lambda x: x != \"\").sum()\n",
    "    has_equipment = costs_df['equipment_id'].apply(lambda x: x != \"\").sum()\n",
    "    has_batch = costs_df['batch_id'].apply(lambda x: x != \"\").sum()\n",
    "    \n",
    "    print(\"\\nAssociation Statistics:\")\n",
    "    print(f\"  Costs with work order: {has_wo} ({has_wo/len(costs_df)*100:.1f}%)\")\n",
    "    print(f\"  Costs with product: {has_product} ({has_product/len(costs_df)*100:.1f}%)\")\n",
    "    print(f\"  Costs with equipment: {has_equipment} ({has_equipment/len(costs_df)*100:.1f}%)\")\n",
    "    print(f\"  Costs with batch: {has_batch} ({has_batch/len(costs_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Variance analysis\n",
    "    costs_df['planned_cost_numeric'] = pd.to_numeric(costs_df['planned_cost'], errors='coerce')\n",
    "    costs_df['variance_numeric'] = pd.to_numeric(costs_df['variance'], errors='coerce')\n",
    "    \n",
    "    has_planned = costs_df['planned_cost'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nCosts with planned values: {has_planned} ({has_planned/len(costs_df)*100:.1f}%)\")\n",
    "    \n",
    "    if has_planned > 0:\n",
    "        # Calculate average variance percentage\n",
    "        costs_df['variance_percent'] = costs_df.apply(\n",
    "            lambda row: row['variance_numeric'] / row['planned_cost_numeric'] * 100 if pd.notna(row['variance_numeric']) and pd.notna(row['planned_cost_numeric']) and row['planned_cost_numeric'] != 0 else None,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nVariance Statistics:\")\n",
    "        print(f\"  Average variance: ${costs_df['variance_numeric'].mean():,.2f}\")\n",
    "        print(f\"  Average variance percentage: {costs_df['variance_percent'].mean():.1f}%\")\n",
    "        \n",
    "        # Count over/under budget\n",
    "        over_budget = costs_df[costs_df['variance_numeric'] > 0]\n",
    "        under_budget = costs_df[costs_df['variance_numeric'] < 0]\n",
    "        \n",
    "        print(f\"  Costs over budget: {len(over_budget)} ({len(over_budget)/has_planned*100:.1f}% of planned costs)\")\n",
    "        print(f\"  Costs under budget: {len(under_budget)} ({len(under_budget)/has_planned*100:.1f}% of planned costs)\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    costs_df['timestamp'] = pd.to_datetime(costs_df['timestamp'])\n",
    "    \n",
    "    print(\"\\nCost Timeline:\")\n",
    "    print(f\"  First cost recorded: {costs_df['timestamp'].min()}\")\n",
    "    print(f\"  Last cost recorded: {costs_df['timestamp'].max()}\")\n",
    "    \n",
    "    # Costs by month\n",
    "    costs_df['month'] = costs_df['timestamp'].dt.to_period('M')\n",
    "    monthly_costs = costs_df.groupby('month')['amount_numeric'].sum()\n",
    "    \n",
    "    print(\"\\nCosts by Month (latest 6 months):\")\n",
    "    for month, amount in monthly_costs.tail(6).items():\n",
    "        print(f\"  {month}: ${amount:,.2f}\")\n",
    "    \n",
    "    # Cost breakdown by association\n",
    "    if has_wo > 0 and work_orders_df is not None:\n",
    "        print(\"\\nTop 5 Work Orders by Cost:\")\n",
    "        wo_costs = costs_df.groupby('work_order_id')['amount_numeric'].sum().sort_values(ascending=False)\n",
    "        for wo_id, amount in wo_costs.head(5).items():\n",
    "            if wo_id != \"\":\n",
    "                print(f\"  {wo_id}: ${amount:,.2f}\")\n",
    "    \n",
    "    if has_product > 0 and products_df is not None:\n",
    "        print(\"\\nTop 5 Products by Cost:\")\n",
    "        # Create a mapping of product_id to product_name\n",
    "        product_names = dict(zip(products_df['product_id'], products_df['product_name']))\n",
    "        \n",
    "        product_costs = costs_df.groupby('product_id')['amount_numeric'].sum().sort_values(ascending=False)\n",
    "        for product_id, amount in product_costs.head(5).items():\n",
    "            if product_id != \"\":\n",
    "                product_name = product_names.get(product_id, product_id)\n",
    "                print(f\"  {product_name}: ${amount:,.2f}\")\n",
    "    \n",
    "    if has_equipment > 0 and equipment_df is not None:\n",
    "        print(\"\\nTop 5 Equipment by Cost:\")\n",
    "        # Create a mapping of equipment_id to equipment_name\n",
    "        equipment_names = dict(zip(equipment_df['equipment_id'], equipment_df['equipment_name']))\n",
    "        \n",
    "        equipment_costs = costs_df.groupby('equipment_id')['amount_numeric'].sum().sort_values(ascending=False)\n",
    "        for equipment_id, amount in equipment_costs.head(5).items():\n",
    "            if equipment_id != \"\":\n",
    "                equipment_name = equipment_names.get(equipment_id, equipment_id)\n",
    "                print(f\"  {equipment_name}: ${amount:,.2f}\")\n",
    "\n",
    "def display_material_lots_statistics(material_lots_df, materials_df=None, suppliers_df=None, storage_locations_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated material lots data\n",
    "    \n",
    "    Parameters:\n",
    "    - material_lots_df: DataFrame containing material lots data\n",
    "    - materials_df: DataFrame containing materials data (optional)\n",
    "    - suppliers_df: DataFrame containing suppliers data (optional)\n",
    "    - storage_locations_df: DataFrame containing storage locations data (optional)\n",
    "    \"\"\"\n",
    "    if material_lots_df is None or len(material_lots_df) == 0:\n",
    "        print(\"No material lots data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nMaterial Lots Statistics:\")\n",
    "    print(f\"Total material lots: {len(material_lots_df)}\")\n",
    "    \n",
    "    # Status distribution\n",
    "    print(\"\\nLot Status Distribution:\")\n",
    "    status_counts = material_lots_df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Quality status distribution\n",
    "    print(\"\\nQuality Status Distribution:\")\n",
    "    quality_counts = material_lots_df['quality_status'].value_counts()\n",
    "    for status, count in quality_counts.items():\n",
    "        print(f\"  {status}: {count} ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Convert quantity to numeric\n",
    "    material_lots_df['quantity_numeric'] = pd.to_numeric(material_lots_df['lot_quantity'], errors='coerce')\n",
    "    \n",
    "    # Quantity statistics by unit\n",
    "    print(\"\\nQuantity Statistics by Unit:\")\n",
    "    for unit in material_lots_df['quantity_unit'].unique():\n",
    "        unit_lots = material_lots_df[material_lots_df['quantity_unit'] == unit]\n",
    "        print(f\"  {unit}:\")\n",
    "        print(f\"    Count: {len(unit_lots)} lots\")\n",
    "        print(f\"    Total quantity: {unit_lots['quantity_numeric'].sum():,.2f} {unit}\")\n",
    "        print(f\"    Average quantity: {unit_lots['quantity_numeric'].mean():,.2f} {unit}\")\n",
    "    \n",
    "    # Material distribution\n",
    "    material_counts = material_lots_df.groupby('material_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Materials by Lot Count:\")\n",
    "    if materials_df is not None:\n",
    "        # Create a mapping of material_id to material_name\n",
    "        material_names = dict(zip(materials_df['material_id'], materials_df['material_name']))\n",
    "        \n",
    "        for material_id, count in material_counts.head(10).items():\n",
    "            material_name = material_names.get(material_id, material_id)\n",
    "            print(f\"  {material_name}: {count} lots ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for material_id, count in material_counts.head(10).items():\n",
    "            print(f\"  {material_id}: {count} lots ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Supplier analysis\n",
    "    has_supplier = material_lots_df['supplier_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nLots with supplier: {has_supplier} ({has_supplier/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    if has_supplier > 0 and suppliers_df is not None:\n",
    "        # Create a mapping of supplier_id to supplier_name\n",
    "        supplier_names = dict(zip(suppliers_df['supplier_id'], suppliers_df['supplier_name']))\n",
    "        \n",
    "        supplier_counts = material_lots_df[material_lots_df['supplier_id'] != \"\"].groupby('supplier_id').size().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 5 Suppliers by Lot Count:\")\n",
    "        for supplier_id, count in supplier_counts.head(5).items():\n",
    "            supplier_name = supplier_names.get(supplier_id, supplier_id)\n",
    "            print(f\"  {supplier_name}: {count} lots ({count/has_supplier*100:.1f}% of supplied lots)\")\n",
    "    \n",
    "    # Storage location analysis\n",
    "    location_counts = material_lots_df.groupby('storage_location_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Storage Locations by Lot Count:\")\n",
    "    if storage_locations_df is not None:\n",
    "        # Create a mapping of location_id to location_name\n",
    "        location_names = dict(zip(storage_locations_df['location_id'], storage_locations_df['location_name']))\n",
    "        \n",
    "        for location_id, count in location_counts.head(5).items():\n",
    "            location_name = location_names.get(location_id, location_id)\n",
    "            print(f\"  {location_name}: {count} lots ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for location_id, count in location_counts.head(5).items():\n",
    "            print(f\"  {location_id}: {count} lots ({count/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Parent-child relationship analysis\n",
    "    has_parent = material_lots_df['parent_lot_id'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nLots with parent (split lots): {has_parent} ({has_parent/len(material_lots_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Cost analysis\n",
    "    material_lots_df['cost_numeric'] = pd.to_numeric(material_lots_df['cost_per_unit'], errors='coerce')\n",
    "    material_lots_df['total_cost'] = material_lots_df['quantity_numeric'] * material_lots_df['cost_numeric']\n",
    "    \n",
    "    print(\"\\nCost Statistics:\")\n",
    "    print(f\"  Average cost per unit: ${material_lots_df['cost_numeric'].mean():,.2f}\")\n",
    "    print(f\"  Total inventory value: ${material_lots_df['total_cost'].sum():,.2f}\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    material_lots_df['creation_date'] = pd.to_datetime(material_lots_df['creation_date'])\n",
    "    material_lots_df['expiration_date'] = pd.to_datetime(material_lots_df['expiration_date'])\n",
    "    \n",
    "    print(\"\\nTime-based Analysis:\")\n",
    "    print(f\"  Oldest lot created: {material_lots_df['creation_date'].min()}\")\n",
    "    print(f\"  Newest lot created: {material_lots_df['creation_date'].max()}\")\n",
    "    \n",
    "    # Calculate shelf life\n",
    "    material_lots_df['shelf_life_days'] = (material_lots_df['expiration_date'] - material_lots_df['creation_date']).dt.days\n",
    "    \n",
    "    print(f\"  Average shelf life: {material_lots_df['shelf_life_days'].mean():.1f} days ({material_lots_df['shelf_life_days'].mean()/365:.1f} years)\")\n",
    "    print(f\"  Minimum shelf life: {material_lots_df['shelf_life_days'].min()} days\")\n",
    "    print(f\"  Maximum shelf life: {material_lots_df['shelf_life_days'].max()} days ({material_lots_df['shelf_life_days'].max()/365:.1f} years)\")\n",
    "    \n",
    "    # Expiration analysis\n",
    "    current_date = datetime.now().date()\n",
    "    expired_lots = material_lots_df[material_lots_df['expiration_date'].dt.date < current_date]\n",
    "    expiring_soon = material_lots_df[(material_lots_df['expiration_date'].dt.date >= current_date) & \n",
    "                                   (material_lots_df['expiration_date'].dt.date <= current_date + timedelta(days=90))]\n",
    "    \n",
    "    print(f\"\\nExpired lots: {len(expired_lots)} ({len(expired_lots)/len(material_lots_df)*100:.1f}%)\")\n",
    "    print(f\"Lots expiring within 90 days: {len(expiring_soon)} ({len(expiring_soon)/len(material_lots_df)*100:.1f}%)\")\n",
    "\n",
    "def display_material_consumption_statistics(consumption_df, material_lots_df=None, batches_df=None, equipment_df=None):\n",
    "    \"\"\"\n",
    "    Display basic statistics about the generated material consumption data\n",
    "    \n",
    "    Parameters:\n",
    "    - consumption_df: DataFrame containing material consumption data\n",
    "    - material_lots_df: DataFrame containing material lots data (optional)\n",
    "    - batches_df: DataFrame containing batches data (optional)\n",
    "    - equipment_df: DataFrame containing equipment data (optional)\n",
    "    \"\"\"\n",
    "    if consumption_df is None or len(consumption_df) == 0:\n",
    "        print(\"No material consumption data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nMaterial Consumption Statistics:\")\n",
    "    print(f\"Total consumption records: {len(consumption_df)}\")\n",
    "    \n",
    "    # Convert quantity to numeric\n",
    "    consumption_df['quantity_numeric'] = pd.to_numeric(consumption_df['quantity'], errors='coerce')\n",
    "    \n",
    "    # Quantity statistics by unit\n",
    "    print(\"\\nQuantity Statistics by Unit:\")\n",
    "    for unit in consumption_df['unit'].unique():\n",
    "        unit_consumptions = consumption_df[consumption_df['unit'] == unit]\n",
    "        print(f\"  {unit}:\")\n",
    "        print(f\"    Count: {len(unit_consumptions)} records\")\n",
    "        print(f\"    Total consumed: {unit_consumptions['quantity_numeric'].sum():,.2f} {unit}\")\n",
    "        print(f\"    Average consumption: {unit_consumptions['quantity_numeric'].mean():,.2f} {unit}\")\n",
    "    \n",
    "    # Lot distribution\n",
    "    lot_counts = consumption_df.groupby('lot_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Lots by Consumption Frequency:\")\n",
    "    if material_lots_df is not None:\n",
    "        # Create a mapping of lot_id to material_id\n",
    "        lot_to_material = dict(zip(material_lots_df['lot_id'], material_lots_df['material_id']))\n",
    "        \n",
    "        for lot_id, count in lot_counts.head(10).items():\n",
    "            material_id = lot_to_material.get(lot_id, \"Unknown\")\n",
    "            print(f\"  {lot_id} (Material: {material_id}): {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for lot_id, count in lot_counts.head(10).items():\n",
    "            print(f\"  {lot_id}: {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Batch distribution\n",
    "    batch_counts = consumption_df.groupby('batch_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Batches by Consumption Records:\")\n",
    "    if batches_df is not None:\n",
    "        # Use batch_id to look up product_id if available\n",
    "        batch_to_product = {}\n",
    "        if 'batch_id' in batches_df.columns and 'product_id' in batches_df.columns:\n",
    "            batch_to_product = dict(zip(batches_df['batch_id'], batches_df['product_id']))\n",
    "        \n",
    "        for batch_id, count in batch_counts.head(10).items():\n",
    "            product_id = batch_to_product.get(batch_id, \"\")\n",
    "            if product_id:\n",
    "                print(f\"  {batch_id} (Product: {product_id}): {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  {batch_id}: {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for batch_id, count in batch_counts.head(10).items():\n",
    "            print(f\"  {batch_id}: {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Equipment distribution\n",
    "    equipment_counts = consumption_df.groupby('equipment_id').size().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Equipment by Consumption:\")\n",
    "    if equipment_df is not None:\n",
    "        # Create a mapping of equipment_id to equipment_name\n",
    "        equipment_names = dict(zip(equipment_df['equipment_id'], equipment_df['equipment_name']))\n",
    "        \n",
    "        for equipment_id, count in equipment_counts.head(5).items():\n",
    "            equipment_name = equipment_names.get(equipment_id, equipment_id)\n",
    "            print(f\"  {equipment_name}: {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        for equipment_id, count in equipment_counts.head(5).items():\n",
    "            print(f\"  {equipment_id}: {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Planned vs. actual analysis\n",
    "    consumption_df['planned_numeric'] = pd.to_numeric(consumption_df['planned_consumption'], errors='coerce')\n",
    "    consumption_df['variance_numeric'] = pd.to_numeric(consumption_df['consumption_variance'], errors='coerce')\n",
    "    \n",
    "    has_planned = consumption_df['planned_consumption'].apply(lambda x: x != \"\").sum()\n",
    "    print(f\"\\nRecords with planned consumption: {has_planned} ({has_planned/len(consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    if has_planned > 0:\n",
    "        # Calculate average variance percentage\n",
    "        consumption_df['variance_percent'] = consumption_df.apply(\n",
    "            lambda row: row['variance_numeric'] / row['planned_numeric'] * 100 if pd.notna(row['variance_numeric']) and pd.notna(row['planned_numeric']) and row['planned_numeric'] != 0 else None,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nConsumption Variance Statistics:\")\n",
    "        print(f\"  Average variance: {consumption_df['variance_numeric'].mean():,.2f}\")\n",
    "        print(f\"  Average variance percentage: {consumption_df['variance_percent'].mean():.1f}%\")\n",
    "        \n",
    "        # Count over/under consumption\n",
    "        over_consumption = consumption_df[consumption_df['variance_numeric'] > 0]\n",
    "        under_consumption = consumption_df[consumption_df['variance_numeric'] < 0]\n",
    "        \n",
    "        print(f\"  Over-consumption records: {len(over_consumption)} ({len(over_consumption)/has_planned*100:.1f}% of planned records)\")\n",
    "        print(f\"  Under-consumption records: {len(under_consumption)} ({len(under_consumption)/has_planned*100:.1f}% of planned records)\")\n",
    "    \n",
    "    # Time-based analysis\n",
    "    consumption_df['timestamp'] = pd.to_datetime(consumption_df['timestamp'])\n",
    "    \n",
    "    print(\"\\nConsumption Timeline:\")\n",
    "    print(f\"  First consumption: {consumption_df['timestamp'].min()}\")\n",
    "    print(f\"  Last consumption: {consumption_df['timestamp'].max()}\")\n",
    "    \n",
    "    # Consumption by day of week\n",
    "    consumption_df['day_of_week'] = consumption_df['timestamp'].dt.day_name()\n",
    "    day_counts = consumption_df.groupby('day_of_week').size()\n",
    "    \n",
    "    # Ensure proper day order\n",
    "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_counts = day_counts.reindex(days_order)\n",
    "    \n",
    "    print(\"\\nConsumption by Day of Week:\")\n",
    "    for day, count in day_counts.items():\n",
    "        print(f\"  {day}: {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Consumption by hour of day\n",
    "    consumption_df['hour'] = consumption_df['timestamp'].dt.hour\n",
    "    hour_counts = consumption_df.groupby('hour').size()\n",
    "    \n",
    "    print(\"\\nConsumption by Hour of Day (Top 5):\")\n",
    "    for hour, count in hour_counts.sort_values(ascending=False).head(5).items():\n",
    "        print(f\"  {hour}:00 - {hour}:59: {count} records ({count/len(consumption_df)*100:.1f}%)\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Main Function to Run All Level 4 Data Generation\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "def generate_all_level4_data():\n",
    "    \"\"\"\n",
    "    Generate all remaining ISA-95 Level 4 tables in a single function\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load existing data if available\n",
    "    try:\n",
    "        materials_df = pd.read_csv(\"data/materials.csv\")\n",
    "        print(\"Loaded existing materials data.\")\n",
    "    except FileNotFoundError:\n",
    "        materials_df = None\n",
    "        print(\"Materials data not found.\")\n",
    "    \n",
    "    try:\n",
    "        products_df = pd.read_csv(\"data/products.csv\")\n",
    "        print(\"Loaded existing products data.\")\n",
    "    except FileNotFoundError:\n",
    "        products_df = None\n",
    "        print(\"Products data not found.\")\n",
    "        \n",
    "    try:\n",
    "        facilities_df = pd.read_csv(\"data/facilities.csv\")\n",
    "        print(\"Loaded existing facilities data.\")\n",
    "    except FileNotFoundError:\n",
    "        facilities_df = None\n",
    "        print(\"Facilities data not found.\")\n",
    "    \n",
    "    try:\n",
    "        storage_locations_df = pd.read_csv(\"data/storage_locations.csv\")\n",
    "        print(\"Loaded existing storage locations data.\")\n",
    "    except FileNotFoundError:\n",
    "        storage_locations_df = None\n",
    "        print(\"Storage locations data not found.\")\n",
    "    \n",
    "    try:\n",
    "        equipment_df = pd.read_csv(\"data/equipment.csv\")\n",
    "        print(\"Loaded existing equipment data.\")\n",
    "    except FileNotFoundError:\n",
    "        equipment_df = None\n",
    "        print(\"Equipment data not found.\")\n",
    "    \n",
    "    try:\n",
    "        work_orders_df = pd.read_csv(\"data/work_orders.csv\")\n",
    "        print(\"Loaded existing work orders data.\")\n",
    "    except FileNotFoundError:\n",
    "        work_orders_df = None\n",
    "        print(\"Work orders data not found.\")\n",
    "    \n",
    "    try:\n",
    "        batches_df = pd.read_csv(\"data/batches.csv\")\n",
    "        print(\"Loaded existing batches data.\")\n",
    "    except FileNotFoundError:\n",
    "        batches_df = None\n",
    "        print(\"Batches data not found.\")\n",
    "        \n",
    "    try:\n",
    "        suppliers_df = pd.read_csv(\"data/suppliers.csv\")\n",
    "        print(\"Loaded existing suppliers data.\")\n",
    "    except FileNotFoundError:\n",
    "        suppliers_df = None\n",
    "        print(\"Suppliers data not found.\")\n",
    "        \n",
    "    try:\n",
    "        personnel_df = pd.read_csv(\"data/personnel.csv\")\n",
    "        print(\"Loaded existing personnel data.\")\n",
    "    except FileNotFoundError:\n",
    "        personnel_df = None\n",
    "        print(\"Personnel data not found.\")\n",
    "    \n",
    "    # 1. Generate Material Lots data\n",
    "    print(\"\\n1. Generating Material Lots data...\")\n",
    "    material_lots_df = generate_material_lots_data(\n",
    "        materials_df=materials_df,\n",
    "        suppliers_df=suppliers_df,\n",
    "        storage_locations_df=storage_locations_df,\n",
    "        num_lots=200,\n",
    "        output_file=\"data/material_lots.csv\"\n",
    "    )\n",
    "    \n",
    "    # Display statistics\n",
    "    if material_lots_df is not None:\n",
    "        display_material_lots_statistics(material_lots_df, materials_df, suppliers_df, storage_locations_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample material lots data (first 5 records):\")\n",
    "        print(material_lots_df.head(5))\n",
    "    \n",
    "    # 2. Generate Material Consumption data\n",
    "    print(\"\\n2. Generating Material Consumption data...\")\n",
    "    material_consumption_df = generate_material_consumption_data(\n",
    "        material_lots_df=material_lots_df,\n",
    "        batches_df=batches_df,\n",
    "        work_orders_df=work_orders_df,\n",
    "        equipment_df=equipment_df,\n",
    "        personnel_df=personnel_df,\n",
    "        num_records=300,\n",
    "        output_file=\"data/material_consumption.csv\"\n",
    "    )\n",
    "    \n",
    "    # Display statistics\n",
    "    if material_consumption_df is not None:\n",
    "        display_material_consumption_statistics(material_consumption_df, material_lots_df, batches_df, equipment_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample material consumption data (first 5 records):\")\n",
    "        print(material_consumption_df.head(5))\n",
    "    \n",
    "    # 3. Generate Costs data\n",
    "    print(\"\\n3. Generating Costs data...\")\n",
    "    costs_df = generate_costs_data(\n",
    "        work_orders_df=work_orders_df,\n",
    "        products_df=products_df,\n",
    "        equipment_df=equipment_df,\n",
    "        batches_df=batches_df,\n",
    "        num_costs=500,\n",
    "        output_file=\"data/costs.csv\"\n",
    "    )\n",
    "    \n",
    "    # Display statistics\n",
    "    if costs_df is not None:\n",
    "        display_costs_statistics(costs_df, work_orders_df, products_df, equipment_df)\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample costs data (first 5 records):\")\n",
    "        print(costs_df.head(5))\n",
    "    \n",
    "    print(\"\\nAll ISA-95 Level 4 data generation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_all_level4_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cf4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
